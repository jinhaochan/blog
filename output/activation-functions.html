
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/font-awesome.min.css">







<meta name="author" content="Chan Jin Hao" />
<meta name="description" content="The structure of a deep learning model consists mainly of nodes, and connections between them. Most of the time, every single node is connected to every other node in the next layer, which we call a Dense layer. Nodes in a fully connected neural network Within each node is a …" />
<meta name="keywords" content="Activation Functions">

<meta property="og:site_name" content="glob"/>
<meta property="og:title" content="Activation Functions"/>
<meta property="og:description" content="The structure of a deep learning model consists mainly of nodes, and connections between them. Most of the time, every single node is connected to every other node in the next layer, which we call a Dense layer. Nodes in a fully connected neural network Within each node is a …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/activation-functions.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-04-14 18:07:00+08:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="/author/chan-jin-hao.html">
<meta property="article:section" content="Data Science"/>
<meta property="article:tag" content="Activation Functions"/>
<meta property="og:image" content="">

  <title>glob &ndash; Activation Functions</title>

</head>
<body>
  <aside>
    <div>
      <a href="">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>


      <nav>
        <ul class="list">

          <li><a href="/">All</a></li>
          <li><a href="/category/data-science.html">Data Science</a></li>
          <li><a href="/category/security.html">Cyber Security</a></li>
          <li><a href="/category/software-engineering.html">Software Engineering</a></li>
          <li><a href="/category/review.html">Book Reviews</a></li>
          <li><a href="/category/ramblings.html">Ramblings</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/jinhao-hao-chan-162630120/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-github" href="https://www.github.com/jinhaochan" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>

		<p>What the osfork? Just like the pythonic way of spawning off different processes by invoking <p style="font-family:courier;">os.fork</p>, this website is meant to have different processes of content.</p>
		<p>From technical stuff, to books I've read, and ramblings about life, all my thought processes are here.</p>

    </div>


  </aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="activation-functions">Activation Functions</h1>
    <p>
          Posted on Sun 14 April 2019 in <a href="/category/data-science.html">Data Science</a>


    </p>
  </header>


  <div>
    <p>The structure of a deep learning model consists mainly of nodes, and connections between them. Most of the time, every single node is connected to every other node in the next layer, which we call a Dense layer.</p>
<!-- wp:image {"id":260,"align":"center"} -->

<p><img alt="placeholder" class="wp-image-260" src="/media/2019/01/2.png"><br>
<figcaption>
Nodes in a fully connected neural network
</figcaption></p>
<p>Within each node is a mathematical equation, decides, based on the input values and their weights, what values to output to the next layer. These mathematical equations are called Activation Functions.</p>
<!-- wp:image {"id":261,"align":"center"} -->

<p><img alt="placeholder" class="wp-image-261" src="/media/2019/01/3.png"><br>
<figcaption>
The activation function is in the middle box, which performs an operation on the inputs, z, based on their weights and bias value.
</figcaption></p>
<!-- wp:heading {"level":3} -->

<h3>Different Activation Functions</h3>
<hr>
<p>There are several kinds of Activation Functions, or in other words, different kinds of mathematical operations that a node can take. They are:</p>
<!-- wp:list {"ordered":true} -->

<ol>
<li>Sigmoid Function</li>
<li>Tanh Function</li>
<li>ReLU Function</li>
<li>Leaky ReLU Function</li>
</ol>
<!-- wp:image {"id":262,"align":"center"} -->

<p><img alt="placeholder" class="wp-image-262" src="/media/2019/01/4.png"><br>
<figcaption>
Different Activation Functions and their mathematical equations
</figcaption></p>
<p>These activation functions take in the inputs <code>z</code> from the previous layer, and feed it into their equations to produce an output <code>a</code>.</p>
<!-- wp:heading {"level":3} -->

<h3>Sigmoid vs TanH</h3>
<hr>
<p>The TanH function is almost strictly superior to the Sigmoid function, because the TanH function has it's mean centered at <code>0</code>. This feature will result in a higher value of derivative, and a faster learning rate. Also, having a <code>0</code> value mean will avoid having bias in the gradients.</p>
<!-- wp:heading {"level":3} -->

<h3>ReLU vs (Sigmoid + TanH)</h3>
<hr>
<p>The drawback of both Sigmoid and TanH, given that they have a curved graph, is that if the value of <code>z</code> is either extremely large or small, the gradient on the curve will be extremely small as well. This small gradient will have an adverse effect on the learning rate when performing Gradient Descent.</p>
<p>The solution to this is ReLU (Rectified Linear Unit), which has a constant gradient regardless of the value of <code>z</code>. But for ReLU, having a negative value of <code>z</code> will result in a <code>0</code> value activation. The solution for that is a Leaky ReLU, which allows for a small value of <code>a</code> for negative values of <code>z</code>.</p>
<!-- wp:heading {"level":3} -->

<h3>Must they Always be Non-Linear?</h3>
<hr>
<p>Yes, Activation Function must always be non-linear. Having multiple linear activation functions can be condensed together, effectively negating the need for any hidden layers or hidden nodes.</p>
<!-- wp:heading {"level":3} -->

<h3>Conclusion</h3>
<hr>
<p>In this post, we talked very briefly about the different kinds of Activation Functions, and compared their pro and cons.</p>
<p>A recommendation for building a neural network model is to have the hidden nodes all be either TanH or ReLU, and never having Sigmoid.</p>
<p>The only time you can have a Sigmoid is at your output layer, if your problem is a binary classification problem.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/activation-functions.html">Activation Functions</a>
    </p>
  </div>





  <section id="comments" class="body">
	  <h2>Comments:</h2>
	  Contact me for any comments, and I'll paste them here! Why not Disqus or other comment services? That's because I want to have control of my site, comments included!

	  PS: I won't put your real name and email if you don't want me to.
	  
  </section>

</article>

    <footer>
<p>&copy;  </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " glob ",
  "url" : "",
  "image": "",
  "description": ""
}
</script>

</body>
</html>