<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>LSTM</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="A Pelican Blog Atom Feed" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">A Pelican Blog </a></h1>
                <nav><ul>
                    <li><a href="/category/book-review.html">Book Review</a></li>
                    <li class="active"><a href="/category/data-science.html">Data Science</a></li>
                    <li><a href="/category/misc.html">misc</a></li>
                    <li><a href="/category/ramblings.html">Ramblings</a></li>
                    <li><a href="/category/review.html">Review</a></li>
                    <li><a href="/category/security.html">Security</a></li>
                    <li><a href="/category/software-engineering.html">Software Engineering</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/lstm.html" rel="bookmark"
           title="Permalink to LSTM">LSTM</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-06-30T15:33:00+00:00">
                Published: Sun 30 June 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/exploding-gradients.html">Exploding Gradients</a> <a href="/tag/lstm.html">LSTM</a> <a href="/tag/vanishing-gradients.html">Vanishing Gradients</a> </p>
</footer><!-- /.post-info -->      <p>In the previous post, we talked about RNN, and how performing Backpropagation through time (BPTT) on an unrolled RNN with many time steps can lead to the problems of vanishing / exploding gradients, and difficulties in learning long term dependencies.</p>
<p>In this post, we're going to look at a the LSTM (Long Short Term Memory) model that is a variant of an RNN, but is designed specifically to combat the 2 issues.</p>
<!-- wp:heading {"level":3} -->

<h3>LSTM Structure</h3>
<hr>
<p>Lets visually inspect the difference between a normal RNN cell and an LSTM cell.</p>
<!-- wp:image {"id":362} -->

<p><img alt="placeholder" class="wp-image-362" src="/media/2019/03/lstm3-simplernn.png">  </p>
<!-- wp:image {"id":356} -->

<p><img alt="placeholder" src="/media/2019/03/lstm3-chain.png"> </p>
<p>An unrolled LSTM cell</p>
<p>The A on each of the cells represent the Activation in the cell. In the RNN, this can either be a Sigmoid, tanh, ReLU, or other activation functions. In the LSTM however, its a combination of 3 Sigmoids and a tanh function.</p>
<p>The biggest difference, aside from the more complex internal structure of the LSTM, is that it has two connecting data pipelines from cell to cell.</p>
<!-- wp:heading {"level":3} -->

<h3>Cell State</h3>
<hr>
<p>The top line of an LSTM cell represents the cell state</p>
<!-- wp:image {"id":357,"align":"center"} -->

<p><img alt="placeholder" class="wp-image-357" src="/media/2019/03/lstm3-c-line.png"></p>
<p>The LSTM has the ability to modify the cell state by removing information (through the multiplicative forget gate), or adding information (through the additive input gate)</p>
<p>This data flow from cell to cell is modified by two operators: The multiplication operator, and the addition operator denoted by the two pink nodes</p>
<!-- wp:heading {"level":3} -->

<h3>LSTM Gates</h3>
<hr>
<p>The LSTM has 3 gates in the cell:</p>
<!-- wp:list {"ordered":true} -->

<ol>
<li>Forget Gate</li>
<li>Input gate</li>
<li>Output Gate</li>
</ol>
<p>These gates modify the data that is flowing in and out of the LSTM cell</p>
<!-- wp:heading {"level":4} -->

<h4>The Common Sigmoid Layer</h4>
<p>In all the 3 gates, there exists the common Sigmoid layer. This layer outputs a value from 0 to 1 for each state in the cell state.</p>
<!-- wp:image {"id":365,"align":"center","width":87,"height":106} -->

<p><img alt="placeholder" class="wp-image-365" height="106" src="/media/2019/03/lstm3-gate.png" width="87"><br>
<figcaption>
The common Sigmoid layer in all 3 gates
</figcaption></p>
<p>The Sigmoid layer outputs a value from 0 to 1. This value corresponds to a value in the cell state, and this would mean different things for different gates.</p>
<p>In the Forget gate, 0 would mean forget the value in the cell state, and 1 would mean remember the value entirely.</p>
<p>In the Input gate, 0 would mean do not update this value at all, and 1 would mean update the value entirely.</p>
<p>In the Output gate, 0 would mean do not output this cell state value, and 1 would mean output this value entirely.</p>
<!-- wp:heading {"level":4} -->

<h4>Forget Gate</h4>
<p>The first gate is the forget gate. This gate decides what information to discard from the cell state.</p>
<p>This gate has the Sigmoid activation function. It takes in the previous time step's output, and the current time step input.</p>
<!-- wp:image {"id":359} -->

<p><img alt="placeholder" class="wp-image-359" src="/media/2019/03/lstm3-focus-f.png"></p>
<p>The two inputs are concatenated, and passed through the Sigmoid layer.</p>
<p>Recall that a Sigmoid activation function outputs a value from 0 to 1. A value of 0 means completely forget this input value, while 1 means to remember the value entirely.</p>
<!-- wp:heading {"level":4} -->

<h4>Input Gate</h4>
<p>The next gate is the input gate, which is a combination of both the Sigmoid and tanh activation function. This gate decides what new information to add to the cell state.</p>
<!-- wp:image {"id":360} -->

<p><img alt="placeholder" class="wp-image-360" src="/media/2019/03/lstm3-focus-i.png"></p>
<p>There are two steps in the input gate phase</p>
<p>In the first step, the output of the previous time step and the input of the current time step are concatenated together, and passed into a Sigmoid. This layer decides which cell state values to update. (0 means do not update, 1 means update entirely)</p>
<p>The inputs are also passed into a tanh activation function, which tells the model what to update the cells states with.</p>
<p>The multiplicative combination of these two outputs tells us which cell state to update (from the sigmoid layer), and what to update it with (tanh layer)</p>
<!-- wp:heading {"level":4} -->

<h4>Output Gate</h4>
<p>The last gate, output gate, decides what value the cell would output. The output is derived from multiplying the outputs from the Sigmoid layer and tanh layer</p>
<!-- wp:image {"id":361} -->

<p><img alt="placeholder" class="wp-image-361" src="/media/2019/03/lstm3-focus-o.png"></p>
<p>The Sigmoid layer takes in the previous and current time step values, and outputs values 0 to 1 for each value in the cell. A value of 0 means do not output this cell state value at all, and 1 means to output the entire value.</p>
<p>The tanh layer takes in the current cell state, which scales the values to be from -1 to 1. This value is then multiplied by the output of the Sigmoid layer to get the final output value.</p>
<!-- wp:heading {"level":3} -->

<h3>Modification of the Cell State</h3>
<hr>
<p>The cell states in each LSTM cell are modified either by the forget gate, or the input gate.</p>
<!-- wp:image {"id":364} -->

<p><img alt="placeholder" class="wp-image-364" src="/media/2019/03/lstm3-focus-c-2.png"></p>
<p>The forget gate outputs values 0-1, and is multiplied by the cell state. Cell states multiplied by 0 will be completely forgotten, while those multiplied by value &gt; 0 will be remembered by varying degrees.</p>
<p>The input gate then updates each value in the cell state by a candidate amount (from the tanh layer), scaled by a factor (decided from the Sigmoid layer)</p>
<!-- wp:heading {"level":3} -->

<h3>Conclusion</h3>
<hr>
<p>In each LSTM cell, there contains a cell state.</p>
<p>Forget gate decides what to drop from the cell state</p>
<p>Input gate creates candidate values to update the cell state</p>
<p>Output gate decides what values to output from the cell state</p>
<h3>Final Notes</h3>
<hr>
<p>I usually end with the conclusion, but all the information above was all the technical aspects of an LSTM. Here are some further questions relating to LSTM.</p>
<p>Q: What are you actually training when you do your Backpropagation through time on an LSTM?</p>
<p>A: Recall that the gates have to control what to forget, input and output from each LSTM cell. What exactly to forget, input and output are the variables being trained.</p>
<p>Q: So... how does it combat vanishing/exploding gradients?</p>
<p>Gradients explode when their values are greater than 1, and vanish when their values are lesser than 1, and are backpropagated for too large a time step.</p>
<p>The key to LSTM preventing the vanexplgrad (I just made that up) is cell state updating step. Below shows the formula for updating the cell</p>
<p><img alt="placeholder" class="wp-image-369" src="/media/2019/03/untitled-1.png"><br>
<figcaption>
Calculating the current cell state using values from the previous cell state
</figcaption></p>
<ul>
<li><code>c(t)</code> is the current cell state to compute</li>
<li><code>i</code> is the input gate that decides which cell state to update, <code>g</code> is the actual value changes to be made to the cell state. These two are multiplied together to get final cell state changes.</li>
<li><code>f</code> is the forget gate, and <code>c(t-1)</code> is the previous cell state. These two are multiplied to drop values from the previous cell state.</li>
</ul>
<p>When performing backpropagation, we find the derivative w.r.t the error. This gives us the formula</p>
<p><img alt="placeholder" class="wp-image-368" src="/media/2019/03/2.png"><br>
<figcaption>
Derivative of w.r.t the error
</figcaption></p>
<p>This formula does not have any multiplicative element in it, and so when BPTT occurs, a <em>linear carousel</em> occurs, thus preventing vanexplgrad.</p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>