
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/font-awesome.min.css">







<meta name="author" content="Chan Jin Hao" />
<meta name="description" content="I&#39;ve been doing Kaggle competitions for awhile (although with not much success), and I&#39;ve learning quite a few things along the way. One of which is how to properly approach the problem, and iterate through it to climb the LB (leader board). Setting the baseline The first thing I would …" />
<meta name="keywords" content="Kaggle">

<meta property="og:site_name" content="glob"/>
<meta property="og:title" content="Tips for Kaggling"/>
<meta property="og:description" content="I&#39;ve been doing Kaggle competitions for awhile (although with not much success), and I&#39;ve learning quite a few things along the way. One of which is how to properly approach the problem, and iterate through it to climb the LB (leader board). Setting the baseline The first thing I would …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/tips-for-kaggling.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-05-12 11:31:00+08:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="/author/chan-jin-hao.html">
<meta property="article:section" content="Data Science"/>
<meta property="article:tag" content="Kaggle"/>
<meta property="og:image" content="">

  <title>glob &ndash; Tips for Kaggling</title>

</head>
<body>
  <aside>
    <div>
      <a href="">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>


      <nav>
        <ul class="list">

          <li><a href="/">All</a></li>
          <li><a href="/category/data-science.html">Data Science</a></li>
          <li><a href="/category/security.html">Cyber Security</a></li>
          <li><a href="/category/software-engineering.html">Software Engineering</a></li>
          <li><a href="/category/review.html">Book Reviews</a></li>
          <li><a href="/category/ramblings.html">Ramblings</a></li>
        </ul>
      </nav>

		<p>What the osfork? Just like the pythonic way of spawning off different processes by invoking <p style="font-family:courier;">os.fork</p> this website is meant to have different processes of content.</p>
		<p>From technical stuff, to books I've read, and ramblings about life, all my thought processes are here.</p>
      <br/>
      <br/>
      <ul class="social">
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/jinhao-hao-chan-162630120/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-github" href="https://www.github.com/jinhaochan" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>

    </div>


  </aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="tips-for-kaggling">Tips for Kaggling</h1>
    <p>
          Posted on Sun 12 May 2019 in <a href="/category/data-science.html">Data Science</a>


    </p>
  </header>


  <div>
    <p>I've been doing Kaggle competitions for awhile (although with not much success), and I've learning quite a few things along the way. One of which is how to properly approach the problem, and iterate through it to climb the LB (leader board).</p>
<!-- wp:heading {"level":3} -->

<h3>Setting the baseline</h3>
<hr>
<p>The first thing I would do is to use some very simple features, and build a quick model that has a relatively low bias.</p>
<p>I'll then use cross validation to ensure that I have low variance between train-test splits. This ensures that i'm not overfitting to any segment of the training data.</p>
<p>After getting a satisfactory bias and variance value, we cross our fingers, and submit our prediction to see how well it does on the LB!</p>
<p>There are now 3 things you need to take note of:</p>
<p>Cross Validation Score (CV Score)</p>
<p>Leader board Score (LB Score)</p>
<p>Difference between CV and LB Score (Your variance on the testing set)</p>
<p>If the difference between your CV and LB is high, you're overfitting the training data. Try to tune your model so that the difference isn't too high.</p>
<!-- wp:heading {"level":3} -->

<h3>Feature Generation</h3>
<hr>
<p>Now that you got your baseline, <strong>Do Not Modify Your Model's Parameters!</strong>..... yet</p>
<p>Using parameters for the baseline model, you want to generate more features to increase your CV score. Then you make submissions to the LB to checkout your LB score. If your CV score increases, but your LB score stays the same or decreases, you're overfitting.</p>
<p>Here's a snippet of my comments I used to keep track of my CV/LB climb:</p>
<!-- wp:code -->

<div class="highlight"><pre><span></span><code><span class="n">xgb_params</span> <span class="o">=</span> <span class="err">{</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>  <span class="o">#</span> <span class="n">the</span> <span class="n">maximum</span> <span class="n">depth</span> <span class="k">of</span> <span class="k">each</span> <span class="n">tree</span>
    <span class="s1">&#39;eta&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">01</span><span class="p">,</span>  <span class="o">#</span> <span class="n">the</span> <span class="n">training</span> <span class="n">step</span> <span class="k">for</span> <span class="k">each</span> <span class="n">iteration</span>
    <span class="s1">&#39;silent&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>  <span class="o">#</span> <span class="n">logging</span> <span class="k">mode</span> <span class="o">-</span> <span class="n">quiet</span>
    <span class="s1">&#39;objective&#39;</span><span class="p">:</span> <span class="s1">&#39;multi:softprob&#39;</span><span class="p">,</span>  <span class="o">#</span> <span class="n">error</span> <span class="n">evaluation</span> <span class="k">for</span> <span class="n">multiclass</span> <span class="n">training</span>
    <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">9</span><span class="p">,</span>
    <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">3</span><span class="p">,</span>
    <span class="s1">&#39;colsample_bytree&#39;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">09</span><span class="p">,</span>
    <span class="s1">&#39;subsample&#39;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">09</span><span class="p">,</span>
    <span class="s1">&#39;num_class&#39;</span><span class="p">:</span> <span class="mi">9</span><span class="err">}</span>  <span class="o">#</span> <span class="n">the</span> <span class="nb">number</span> <span class="k">of</span> <span class="n">classes</span> <span class="n">that</span> <span class="n">exist</span> <span class="k">in</span> <span class="n">this</span> <span class="n">datset</span>

<span class="o">##</span> <span class="n">Original</span>
<span class="n">nfolds</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">CV</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">7299</span>
<span class="n">std</span><span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">0112</span>
<span class="n">LB</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">67291</span>

<span class="n">Difference</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">05699</span>

<span class="o">##</span> <span class="n">changed</span> <span class="n">rolling</span> <span class="n">window</span> <span class="k">size</span>
<span class="n">nfolds</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">CV</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">7458</span>
<span class="n">std</span><span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">0207</span>
<span class="n">LB</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">67995</span>

<span class="n">Difference</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">06585</span>

<span class="o">##</span> <span class="n">added</span> <span class="n">quantile</span> <span class="n">features</span>
<span class="n">nfolds</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">CV</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">7564</span>
<span class="n">std</span><span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">0167</span>
<span class="n">LB</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">68347</span>

<span class="n">Difference</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">07293</span>

<span class="o">##</span> <span class="n">dropping</span> <span class="n">mean</span>
<span class="k">using</span> <span class="n">smaller</span> <span class="n">feature</span> <span class="k">set</span> <span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">nfolds</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">CV</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">7670</span>
<span class="n">std</span><span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">0243</span>
<span class="n">LB</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">69284</span>

<span class="n">Difference</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">07416</span>

<span class="o">##</span> <span class="n">adding</span> <span class="n">iqr</span> <span class="k">and</span> <span class="n">trimming</span> <span class="n">mean</span>
<span class="k">using</span> <span class="n">smaller</span> <span class="n">feature</span> <span class="k">set</span> <span class="p">(</span><span class="mi">70</span><span class="p">)</span>
<span class="n">nfolds</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">CV</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">7740</span>
<span class="n">std</span><span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">0257</span>
<span class="n">LB</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">69988</span>

<span class="n">Difference</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">07412</span>
</code></pre></div>


<!-- /wp:code -->

<p>As you can see, my parameters are the same, and I'm only adding or removing features to push up my CV and LB</p>
<!-- wp:heading {"level":3} -->

<h3>Feature Selection</h3>
<hr>
<p>Feature generation was the process of adding in new features, but using all the features (if you have a lot of them), is usually too noisy.</p>
<p>Most models like XGBoost and LightGBM have functions to tell you which features have the most impact on your prediction. You would want to trim your selected features to only include those high impact ones.</p>
<p>By doing this, I was able to push my score up just a little bit more!</p>
<!-- wp:heading {"level":3} -->

<h3>Parameter Tuning</h3>
<hr>
<p>Once you're done with Feature Generation and Feature Selection, then we come to parameter tuning phase.</p>
<p>In this phase, we want to tune the parameters to reduce bias, variance, and CV-LB difference. You can use functions like GridSearch to efficiently search across a range of parameters</p>
<!-- wp:heading {"level":3} -->

<h3>Conclusion</h3>
<hr>
<p>Baseline -&gt; Feature Generation -&gt; Feature Selection -&gt; Parameter Tuning!</p>
<p>This isn't a silver bullet to all competitions, but its the strategy that I use regularly.</p>
<p>In data science and Kaggle, there are plenty of moving parts, and it's easy to get lost in the myriad of factors affecting your prediction. It's always good to have a system to isolate and tackle the problem!</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/kaggle.html">Kaggle</a>
    </p>
  </div>





  <section id="comments" class="body">
	  <h2>Comments:</h2>
	  Contact me for any comments, and I'll paste them here! Why not Disqus or other comment services? That's because I want to have control of my site, comments included!

	  PS: I won't put your real name and email if you don't want me to.
	  
  </section>

</article>

    <footer>
<p>&copy;  </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " glob ",
  "url" : "",
  "image": "",
  "description": ""
}
</script>

</body>
</html>