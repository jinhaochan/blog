
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/font-awesome.min.css">







<meta name="author" content="Chan Jin Hao" />
<meta name="description" content="One of the major problems in training a model in machine learning is overfitting. Especially when your model gets more and more complex, it starts to memorize the patterns in the training data. This makes it perform poorly on unseen data, which has new patterns. Overfitting is the result of …" />
<meta name="keywords" content="drop out, regularization">

<meta property="og:site_name" content="glob"/>
<meta property="og:title" content="Regularization"/>
<meta property="og:description" content="One of the major problems in training a model in machine learning is overfitting. Especially when your model gets more and more complex, it starts to memorize the patterns in the training data. This makes it perform poorly on unseen data, which has new patterns. Overfitting is the result of …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/regularization.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-04-07 09:49:00+08:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="/author/chan-jin-hao.html">
<meta property="article:section" content="Data Science"/>
<meta property="article:tag" content="drop out"/>
<meta property="article:tag" content="regularization"/>
<meta property="og:image" content="">

  <title>glob &ndash; Regularization</title>

</head>
<body>
  <aside>
    <div>
      <a href="">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>


      <nav>
        <ul class="list">

          <li><a href="/">All</a></li>
          <li><a href="/category/data-science.html">Data Science</a></li>
          <li><a href="/category/security.html">Cyber Security</a></li>
          <li><a href="/category/software-engineering.html">Software Engineering</a></li>
          <li><a href="/category/review.html">Book Reviews</a></li>
          <li><a href="/category/ramblings.html">Ramblings</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/jinhao-hao-chan-162630120/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-github" href="https://www.github.com/jinhaochan" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>

      <ul>
	<li>
		<p>What the osfork? Just like the pythonic way of spawning off different processes by invoking <p style="font-family:courier;">os.fork</p>, this website is meant to have different processes of content.</p>
		<p>From technical stuff, to books I've read, and ramblings about life, all my thought processes are here.</p>
	</li>

      </ul>
      

    </div>


  </aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="regularization">Regularization</h1>
    <p>
          Posted on Sun 07 April 2019 in <a href="/category/data-science.html">Data Science</a>


    </p>
  </header>


  <div>
    <p>One of the major problems in training a model in machine learning is overfitting. Especially when your model gets more and more complex, it starts to memorize the patterns in the training data. This makes it perform poorly on unseen data, which has new patterns.</p>
<p>Overfitting is the result of low-bias and high-variance, where it performs well for a single data set, but given new data, the error fluctuates. That means that the model is learning too much for each data set.</p>
<p>One of the ways to overcome overfitting is Regularization</p>
<!-- wp:heading {"level":3} -->

<h3>What is Regularization</h3>
<hr>
<p>The mathematical definition of Regularization is the process of adding information in order to solve ill-posed problems, or to prevent overfitting. Ill-posed meaning that the solution is highly sensitive to the changes in the data.</p>
<!-- wp:image {"id":248,"align":"center","width":238,"height":228} -->

<p><img alt="placeholder" class="wp-image-248" height="228" src="/media/2019/01/1280px-regularization.png" width="238"><br>
<figcaption>
The blue line shows the model before regularization, while the green line shows the model after regularization.  </p>
<p>Regularization makes the model less complex.<br>
</figcaption></p>
<p>By introducing regularization, we reduce the complexity of the learned model. This means that we're reducing the accuracy of the model for a given data set, but in doing so we're making it generalize across data sets. This action reduces variance, while not changing your bias too much, and bring us to the idea situation of low-bias low-variance.</p>
<!-- wp:heading {"level":3} -->

<h3>Regularization in Machine Learning</h3>
<hr>
<p>To put this into a machine learning context, for each model we use, we have a loss function we wish to minimize. We'll use the RSS (Residual Sum Squares) loss function in this example.</p>
<!-- wp:image {"id":249} -->

<p><img alt="placeholder" class="wp-image-249" src="/media/2019/01/rss.png">  </p>
<figcaption>
RSS loss function we want to minimize

</figcaption>

<p>This will calculate how much to adjust your parameters based on your training data. But if your training data has noise, then your parameters will be adjusted to pick up the noise, and your model will be optimized towards the noise in the data. That's overfitting.</p>
<p>To combat this, we add in a regularization factor, which will shrink the estimated value to adjust your parameters. This way, your parameters won't move too much towards learning the noise in the data.</p>
<!-- wp:heading {"level":3} -->

<h3>Ridge Regression (L2)</h3>
<hr>
<!-- wp:image {"id":250} -->

<p><img alt="placeholder" class="wp-image-250" src="/media/2019/01/ridge.png"></p>
<p>Ridge Regression adds a shrinkage quantity to the original loss function RSS. This works by preventing the change in parameters from being too high in value.</p>
<p>When <em>λ = 0</em> , the penalty term is essentially taken out. Your estimated value to modify the parameters will then simply be RSS</p>
<p>When <strong><em>λ→∞</em></strong>, the penalty term, the penalty term grows large, and your estimated value to modify the parameters will approach 0. (But never being 0). Because it never reaches 0, the impact of those noisy features will only be minimized, but never removed.</p>
<!-- wp:heading {"level":3} -->

<h3>Lasso Regression (L1)</h3>
<hr>
<!-- wp:image {"id":251} -->

<p><img alt="placeholder" class="wp-image-251" src="/media/2019/01/lasso.png"></p>
<p>Lasso Regression also adds a shrinkage quantity, but the difference is that it only penalizes high valued coefficients.</p>
<p>The penalty term uses <strong><em>|β1|</em></strong> instead of <strong><em>β1²</em></strong> , hence it is named L1, while<br>
Ridge regularization is named L2.</p>
<p>Lasso also differs from from Ridge in that it can set coefficients to 0, making them not relevant at all. In the end, because the coefficients are 0, you may end up with lesser features, which is an advantage!</p>
<!-- wp:heading {"level":3} -->

<h3>Regularization in Deep Learning - Drop Out</h3>
<hr>
<p>Regularization in deep learning is slightly different from shallow learning.</p>
<p>In deep learning, we have neurons that are for the most times fully connected. That's to say, every single neuron is connected to every other neuron in the next layer.</p>
<p>This may cause some problems like overfitting again, because the neurons may develop false co-dependencies among each other (which may be due to noise).</p>
<p>Regularization in deep learning works by occasionally ignoring a fraction of the neurons during the training phase.</p>
<!-- wp:image {"id":252} -->

<p><img alt="placeholder" class="wp-image-252" src="/media/2019/01/dropout.png"></p>
<p>By using dropout, you're forcing the model to learn more robust features, as opposed to random combinations of neurons. Also, it roughly doubles the number of iterations required to converge.</p>
<!-- wp:heading {"level":3} -->

<h3>Conclusion</h3>
<hr>
<p>To conclude, we've talked about methods in shallow learning and deep learning to combat overfitting by regularization.</p>
<p>Regularization is the process of adding new information to reduce the value to modify the parameters. This prevents it from learning any noise that is specific to the data set, and reduce the chances of overfitting</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/drop-out.html">drop out</a>
      <a href="/tag/regularization.html">regularization</a>
    </p>
  </div>





  <section id="comments" class="body">
	  <h2>Comments:</h2>
	  Contact me for any comments, and I'll paste them here! Why not Disqus or other comment services? That's because I want to have control of my site, comments included!

	  PS: I won't put your real name and email if you don't want me to.
	  
  </section>

</article>

    <footer>
<p>&copy;  </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " glob ",
  "url" : "",
  "image": "",
  "description": ""
}
</script>

</body>
</html>