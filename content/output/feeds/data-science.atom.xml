<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>A Pelican Blog - Data Science</title><link href="/" rel="alternate"></link><link href="/feeds/data-science.atom.xml" rel="self"></link><id>/</id><updated>2019-06-30T15:33:00+00:00</updated><entry><title>LSTM</title><link href="/lstm.html" rel="alternate"></link><published>2019-06-30T15:33:00+00:00</published><updated>2019-06-30T15:33:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-06-30:/lstm.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In the previous post, we talked about RNN, and how performing Backpropagation through time (BPTT) on an unrolled RNN with many time steps can lead to the problems of vanishing / exploding gradients, and difficulties in learning long term dependencies.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this post, we're going to look at a the LSTM …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In the previous post, we talked about RNN, and how performing Backpropagation through time (BPTT) on an unrolled RNN with many time steps can lead to the problems of vanishing / exploding gradients, and difficulties in learning long term dependencies.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this post, we're going to look at a the LSTM (Long Short Term Memory) model that is a variant of an RNN, but is designed specifically to combat the 2 issues.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;LSTM Structure&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Lets visually inspect the difference between a normal RNN cell and an LSTM cell.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":362} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/lstm3-simplernn.png){.wp-image-362}

&lt;figcaption&gt;
An unrolled RNN cell

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:image {"id":356} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/lstm3-chain.png){.wp-image-356}

&lt;figcaption&gt;
An unrolled LSTM cell

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The A on each of the cells represent the Activation in the cell. In the RNN, this can either be a Sigmoid, tanh, ReLU, or other activation functions. In the LSTM however, its a combination of 3 Sigmoids and a tanh function.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The biggest difference, aside from the more complex internal structure of the LSTM, is that it has two connecting data pipelines from cell to cell.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Cell State&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The top line of an LSTM cell represents the cell state&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":357,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/lstm3-c-line.png){.wp-image-357}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The LSTM has the ability to modify the cell state by removing information (through the multiplicative forget gate), or adding information (through the additive input gate)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This data flow from cell to cell is modified by two operators: The multiplication operator, and the addition operator denoted by the two pink nodes&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;LSTM Gates&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The LSTM has 3 gates in the cell:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Forget Gate&lt;/li&gt;
&lt;li&gt;Input gate&lt;/li&gt;
&lt;li&gt;Output Gate&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;These gates modify the data that is flowing in and out of the LSTM cell&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;The Common Sigmoid Layer&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In all the 3 gates, there exists the common Sigmoid layer. This layer outputs a value from 0 to 1 for each state in the cell state.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":365,"align":"center","width":87,"height":106} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/lstm3-gate.png){.wp-image-365 width="87" height="106"}  
&lt;figcaption&gt;
The common Sigmoid layer in all 3 gates
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The Sigmoid layer outputs a value from 0 to 1. This value corresponds to a value in the cell state, and this would mean different things for different gates.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In the Forget gate, 0 would mean forget the value in the cell state, and 1 would mean remember the value entirely.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In the Input gate, 0 would mean do not update this value at all, and 1 would mean update the value entirely.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In the Output gate, 0 would mean do not output this cell state value, and 1 would mean output this value entirely.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;Forget Gate&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The first gate is the forget gate. This gate decides what information to discard from the cell state.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This gate has the Sigmoid activation function. It takes in the previous time step's output, and the current time step input.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":359} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/lstm3-focus-f.png){.wp-image-359}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The two inputs are concatenated, and passed through the Sigmoid layer.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Recall that a Sigmoid activation function outputs a value from 0 to 1. A value of 0 means completely forget this input value, while 1 means to remember the value entirely.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;Input Gate&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The next gate is the input gate, which is a combination of both the Sigmoid and tanh activation function. This gate decides what new information to add to the cell state.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":360} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/lstm3-focus-i.png){.wp-image-360}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;There are two steps in the input gate phase&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In the first step, the output of the previous time step and the input of the current time step are concatenated together, and passed into a Sigmoid. This layer decides which cell state values to update. (0 means do not update, 1 means update entirely)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The inputs are also passed into a tanh activation function, which tells the model what to update the cells states with.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The multiplicative combination of these two outputs tells us which cell state to update (from the sigmoid layer), and what to update it with (tanh layer)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;Output Gate&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The last gate, output gate, decides what value the cell would output. The output is derived from multiplying the outputs from the Sigmoid layer and tanh layer&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":361} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/lstm3-focus-o.png){.wp-image-361}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The Sigmoid layer takes in the previous and current time step values, and outputs values 0 to 1 for each value in the cell. A value of 0 means do not output this cell state value at all, and 1 means to output the entire value.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The tanh layer takes in the current cell state, which scales the values to be from -1 to 1. This value is then multiplied by the output of the Sigmoid layer to get the final output value.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Modification of the Cell State&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The cell states in each LSTM cell are modified either by the forget gate, or the input gate.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":364} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/lstm3-focus-c-2.png){.wp-image-364}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The forget gate outputs values 0-1, and is multiplied by the cell state. Cell states multiplied by 0 will be completely forgotten, while those multiplied by value &amp;gt; 0 will be remembered by varying degrees.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The input gate then updates each value in the cell state by a candidate amount (from the tanh layer), scaled by a factor (decided from the Sigmoid layer)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In each LSTM cell, there contains a cell state.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Forget gate decides what to drop from the cell state&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Input gate creates candidate values to update the cell state&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Output gate decides what values to output from the cell state&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Final Notes&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;I usually end with the conclusion, but all the information above was all the technical aspects of an LSTM. Here are some further questions relating to LSTM.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Q: What are you actually training when you do your Backpropagation through time on an LSTM?&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A: Recall that the gates have to control what to forget, input and output from each LSTM cell. What exactly to forget, input and output are the variables being trained.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Q: So... how does it combat vanishing/exploding gradients?&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Gradients explode when their values are greater than 1, and vanish when their values are lesser than 1, and are backpropagated for too large a time step.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The key to LSTM preventing the vanexplgrad (I just made that up) is cell state updating step. Below shows the formula for updating the cell&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":369,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/untitled-1.png){.wp-image-369}  
&lt;figcaption&gt;
Calculating the current cell state using values from the previous cell state
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;c(t)&lt;/code&gt; is the current cell state to compute&lt;/li&gt;
&lt;li&gt;&lt;code&gt;i&lt;/code&gt; is the input gate that decides which cell state to update, &lt;code&gt;g&lt;/code&gt; is the actual value changes to be made to the cell state. These two are multiplied together to get final cell state changes.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;f&lt;/code&gt; is the forget gate, and &lt;code&gt;c(t-1)&lt;/code&gt; is the previous cell state. These two are multiplied to drop values from the previous cell state.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;When performing backpropagation, we find the derivative w.r.t the error. This gives us the formula&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":368,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/2.png){.wp-image-368}  
&lt;figcaption&gt;
Derivative of w.r.t the error
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This formula does not have any multiplicative element in it, and so when BPTT occurs, a &lt;em&gt;linear carousel&lt;/em&gt; occurs, thus preventing vanexplgrad.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;!-- /wp:paragraph --&gt;</content><category term="Exploding Gradients"></category><category term="LSTM"></category><category term="Vanishing Gradients"></category></entry><entry><title>RNN and Vanishing/Exploding Gradients</title><link href="/rnn-and-vanishing-exploding-gradients.html" rel="alternate"></link><published>2019-06-23T16:23:00+00:00</published><updated>2019-06-23T16:23:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-06-23:/rnn-and-vanishing-exploding-gradients.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this post, we're going to be looking at:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Recurrent Neural Networks (RNN)&lt;/li&gt;
&lt;li&gt;Weight updates in an RNN&lt;/li&gt;
&lt;li&gt;Unrolling an RNN&lt;/li&gt;
&lt;li&gt;Vanishing/Exploding Gradient Problem&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Recurrent Neural Networks&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;A Recurrent Neural Network (RNN) is a variant of neural networks, where in each neuron, the outputs cycle back to themselves, hence …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this post, we're going to be looking at:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Recurrent Neural Networks (RNN)&lt;/li&gt;
&lt;li&gt;Weight updates in an RNN&lt;/li&gt;
&lt;li&gt;Unrolling an RNN&lt;/li&gt;
&lt;li&gt;Vanishing/Exploding Gradient Problem&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Recurrent Neural Networks&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;A Recurrent Neural Network (RNN) is a variant of neural networks, where in each neuron, the outputs cycle back to themselves, hence being recurrent.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":345} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/0_mrhhgabskajpbt21.png){.wp-image-345}

&lt;figcaption&gt;
Each neuron's output cycle back to themselves, as compared to a feed-forward neural network

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This means that each neuron in an RNN has two sources of inputs:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;The present data (Which can be one or more inputs)&lt;/li&gt;
&lt;li&gt;The recent past data (A single output based on the previous set of inputs)&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Intuitively, this means that the network can learn whats happening now, and what happened before.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The RNN has a Short-Term memory, as the recurrent input is only derived from it's most recent past. Anything that happened way before is "forgotten".&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For example, if we feed in the word "neuron" letter by letter, in a feed-forward NN, when we reach the letter "r", the model would have forgotten "n", "e", "u".&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In an RNN, the model would remember the immediate past, that previously we have seen the letter "u".&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Like a normal feed-forward NN, the RNN also has a weight matrix, but with one additional weight to include the recurrent input. When doing backpropagation, this recurrent weights is also subjected to tweaking.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Weight Updates in an RNN&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;This weight updating phase for an RNN is called Backpropagation Though Time. Lets examine first how a feed-forward NN does forward and backward propagation for weight correction&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In a feed-forward NN, forward propagation is done to get the predicted output. An error estimate is gotten from the predicted output and the true label.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Using the error estimate, we do backpropgation to find the partial derivatives of the error with respect to the weights of the network.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;These derivatives are then used by Gradient Descent to tweak the weights of the model, and ultimately try to minimize the error estimate, so that the predicted output is close to the true output.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":346,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/0_fbugysciqjnfi3n6.png){.wp-image-346}  
&lt;figcaption&gt;
Forward propagation to get the outputs, error estimate calculation, and backpropgation to get the gradients of the error w.r.t. the weights, and apply gradient descent.  
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In an RNN, there is an additional component of the recurrent input in each neuron. This input also has its corresponding weight that needs to be tweaked. To understand how that happens, we need to be able to visualize "unrolling" an RNN&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Unrolling an RNN&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;As mentioned earlier, each neuron will 2 sources of inputs: The current input, and the most recent previous input.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":348,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/0_ynlojw7yvjarwmd4-copy.png){.wp-image-348}  
&lt;figcaption&gt;
The output of the RNN cell is fed back.
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In the next time step, it will take the current input plus the previous output. We can visualize this by "unrolling" the RNN, so we can see what happens at each time step.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":349,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/0_ynlojw7yvjarwmd4.png){.wp-image-349}  
&lt;figcaption&gt;
An unolled RNN to visualize what happens to the cell at each time step
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The image above shows what happens when you unroll one recurrent neuron. In a network with 100s of neurons, some layers recurrent, things can get really messy.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":350,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/dpln_0423.png){.wp-image-350}  
&lt;figcaption&gt;
Hidden layers 1 and 2 are recurrent. Here we unroll them for 3 time steps
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Vanishing/Exploding Gradient Problem&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;When we combine the two concepts of applying Backpropagation on an unrolled RNN, we get Backpropagation through time (BPTT).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Recall that we also need to learn the weights of the recurrent input, and BPTT is done to get the gradient by finding the partial derivatives of the error with respect to the recurrent inputs. (Just like how in a normal feed-forward NN, backpropagation is done to get the partial derivatives of the error with respect to the weights). Using the gradients, Gradient Descent is then applied.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In BPTT, the error is backpropagated from the last time step all the way to the first time step to update the weights of the recurrent input.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The problem comes when there are too many time steps, and BPTT has to propagate error back too many times, which will result in the gradients exploding, or vanishing.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;At each time step, the gradients are multiplied by each other via matrix multiplication because of chain rule. If the gradient is greater than 1.0, a large number of time steps will cause the gradient to "explode", or become too large.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Likewise, when the gradient is less than 1.0, multiplying it too many times by itself will cause the gradient to "vanish", or become close to zero.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Both exploding and vanishing gradients are problematic, because then Gradient Descent will performing poorly on overly large values, or overly small values.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Summary&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;To recap on a feed-forward NN:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Forward propagation is done get the output prediction&lt;/li&gt;
&lt;li&gt;An error estimate is calculated from the model output to the the true values&lt;/li&gt;
&lt;li&gt;Backpropagation is done using the error, to get partial derivative of the error w.r.t. the weights&lt;/li&gt;
&lt;li&gt;Gradient Descent is applied using the gradients to minimize the error&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;And for an RNN:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Forward propagation is done get the output prediction&lt;/li&gt;
&lt;li&gt;An error estimate is calculated from the model output to the the true values&lt;/li&gt;
&lt;li&gt;The RNN is unrolled by the total number of time steps&lt;/li&gt;
&lt;li&gt;BPTT is done to get partial derivative of the error w.r.t. the weights&lt;/li&gt;
&lt;li&gt;Gradient Descent is applied using the gradients to minimize the error&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The problem comes when there are too many times steps, and performing BPTT causes the gradients to explode or vanish. This affects the final step of applying Gradient Descent.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;!-- /wp:paragraph --&gt;</content><category term="RNN"></category><category term="Vanishing Gradients"></category></entry><entry><title>K-Means Clustering</title><link href="/k-means-clustering.html" rel="alternate"></link><published>2019-06-16T21:18:00+00:00</published><updated>2019-06-16T21:18:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-06-16:/k-means-clustering.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;K-Means Clustering is an unsupervised learning algorithm. It works by grouping similar data points together to try to find underlying patterns.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The number of groups are pre-defined by the user as K.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;How the Algorithm works&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Before the iterative update starts, a random selection of centroid locations are picked on …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;K-Means Clustering is an unsupervised learning algorithm. It works by grouping similar data points together to try to find underlying patterns.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The number of groups are pre-defined by the user as K.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;How the Algorithm works&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Before the iterative update starts, a random selection of centroid locations are picked on the graph. These centroids act as the beginning points for each cluster. (if K = 5, there will be 5 random centroids)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Data Assignment Step&lt;ul&gt;
&lt;li&gt;Each data point is assigned to its nearest centroid, based on the squared Euclidean distance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Centroid Update&lt;ul&gt;
&lt;li&gt;Given the new data points, re-calculate the centroid value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repeat until centroid no longer changes, or until a stopping criteria.&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Choosing K&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;How do we choose K? Well, iteratively of cause. We define K to be a range of values, and run K-mean clustering through those values.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":341} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/introduction-to-k-means-clustering-elbow-point-example-1.jpg){.wp-image-341}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conlcusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;This was a pretty short post, but it acts as a summary of how K-means clustering works!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="K means clustering"></category></entry><entry><title>Random Forests</title><link href="/random-forests.html" rel="alternate"></link><published>2019-06-09T20:43:00+00:00</published><updated>2019-06-09T20:43:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-06-09:/random-forests.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A random forest is an ensemble approach of combining multiple decision trees. Ensembling and Decision Trees, we first need to explain what these two things are&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Decision Trees&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Decision Trees try to encode and separate the data into if-else rules. It breaks the data down into smaller and smaller subsets …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A random forest is an ensemble approach of combining multiple decision trees. Ensembling and Decision Trees, we first need to explain what these two things are&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Decision Trees&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Decision Trees try to encode and separate the data into if-else rules. It breaks the data down into smaller and smaller subsets. Each node poses the question, and each branch represents the decision.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":331,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/1_jaey3kp7tu2q6hn6lasmrw.png){.wp-image-331}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Given the example above, how do we know which question to ask first at the root node? Do we first split by Age, Pizza consumption, or Exercise? This decision is made by calculating the &lt;strong&gt;Entropy Loss&lt;/strong&gt;, or &lt;strong&gt;Information Gain&lt;/strong&gt;. Information gain is calculated using Entropy loss, so the two variables are closely linked.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Intuitively, we want to reduce entropy of the data, so that we can separate them nicely. A 0 entropy data means that all the samples are the same, and an entropy of 1 means that the samples are split evenly between the classes.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":332,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/0_klhgarh43lgdoksn.png){.wp-image-332}  
&lt;figcaption&gt;
When there are 0 samples of class P, Entropy is 0.  
When 0.5 of the samples are class P, Entropy is 1.  
When all the samples are class P, Entropy is 0.
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We want to make splits that &lt;strong&gt;Maximizes Information Gain&lt;/strong&gt; (or making the resulting data sets more homogeneous). The following steps are involved:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;For each target feature (Age, Pizza consumption, Exercise), calculate the current Entropy value.&lt;/li&gt;
&lt;li&gt;Split the data on each target feature, and calculate the resulting entropy after splitting.&lt;/li&gt;
&lt;li&gt;Choose the feature that results in the largest information gain, or entropy loss.&lt;/li&gt;
&lt;li&gt;Repeat.&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;That was a gist of a decision tree, now lets look at ensembling and Random Forest&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Ensembling and Random Forest&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Ensembling revolves around the idea of putting together several weak learners to form a strong learner. In Random Forest, the weak learners are Decision Trees&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":333,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/03/skitch.jpg){.wp-image-333}  
&lt;figcaption&gt;
Blue dots represent the data points.  
Grey lines represent the weak learners.  
The red line represents the single strong learner.
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Here's the process of a Random Forest:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;From the full data set, create several subsets by random sampling with replacement.&lt;/li&gt;
&lt;li&gt;Using these subsets, we create Decision Trees from them.&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Now that we created the Random Forest, when we get a new input to predict, we pass the input to all the Decision Trees in the Random Forest. This gives up multiple outputs, one output for each tree. The final result of the Random Forest would then be an average of the trees (if it's a regression problem), or voting by majority (if it's a classification problem).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Downside of Random Forest are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Random Forest that are used for regression cannot predict beyond the range in the training data they are fed with&lt;/li&gt;
&lt;li&gt;Random Forests may overfit noisy data sets&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;This post was a fairly simple and straightforwad one, cover basic, but essential topics.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We have seen how a Decision Tree works, and the how the Random Forest makes use of multiple Decision Trees.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="Decision Tree"></category><category term="Random Forest"></category></entry><entry><title>GAN?</title><link href="/gan.html" rel="alternate"></link><published>2019-05-26T13:13:00+00:00</published><updated>2019-05-26T13:13:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-05-26:/gan.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A Generative Adversarial Network (GAN) is a collection of two neural network models: A Discriminator, and a Generator. The goals of the two models are opposing to each other&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Discriminator: Given a set of features, we try to predict the label&lt;/li&gt;
&lt;li&gt;Generator: Given a label, we try to predict the …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A Generative Adversarial Network (GAN) is a collection of two neural network models: A Discriminator, and a Generator. The goals of the two models are opposing to each other&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Discriminator: Given a set of features, we try to predict the label&lt;/li&gt;
&lt;li&gt;Generator: Given a label, we try to predict the features that lead to the label&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For example, a Discriminator in a spam email detector identifies if an email is spam, given certain keywords. A Generator on the other hand, given a spam label, tries to come up with keywords that results in the label spam.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The goal of a GAN is to train a generative model that can produce outputs that are believable enough for the discriminator to classify it as a positive label. At the end, the generative model will be able to produce outputs that are close to what the true distribution produces. Examples of this are image generations, text generation and&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;How It Works&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Generator:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;The generator takes in a random numbers from a noise generator, and produces a random output&lt;/li&gt;
&lt;li&gt;The outputs of the generator are mixed together with a collection from the actual training data set&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Discriminator:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;The Discriminator takes in data from both the actual data set and the output of the Generator&lt;/li&gt;
&lt;li&gt;The Discriminator makes a prediction of each data and predicts the probability of the label&lt;/li&gt;
&lt;li&gt;It tries to predict if the data is from the training set, or generated from the Generator model (Real vs Fake data)&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The Discriminator and Generator goes back and forth, Generating new data points, and predicting the data points. This goes on until convergence: When the Generator produces data points that are classified as Real by the Discriminator.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The Discriminator gets the feedback for optimization from the ground truth, and the Generator gets feedback from the Discriminator.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":314,"align":"center","width":503,"height":218} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/gans.png){.wp-image-314 width="503" height="218"}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Visualizing the Generator&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;We can visualize how the Generator learns to generate outputs that goes closer to the distribution of the real distribution&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":315} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/iterations-1.gif){.wp-image-315}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We can see that initially, the distribution by the Generator was random and scattered all over. Over several iterations, the Generator starts producing outputs that have a distribution getting closer to the actual distribution.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Math for GAN&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;We have a joint loss function, with the two models (Generative &lt;code&gt;G&lt;/code&gt; and Discriminative &lt;code&gt;D&lt;/code&gt;) optimizing for different things.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The Discriminator tries to identify if the data is from the true distribution &lt;code&gt;x&lt;/code&gt;, and outputs a value &lt;code&gt;D(x)&lt;/code&gt;. The Discriminator also tries to recognize if the data comes from the Generator &lt;code&gt;G&lt;/code&gt;, which outputs a value &lt;code&gt;D(G(z))&lt;/code&gt;. (or &lt;code&gt;1 - D(G(z))&lt;/code&gt;, because the inverse of the Generated data is the data from the true distribution) Putting these two together, we get the loss function:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":316,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/1-4xahmaugxeoqnnjhzjq-4q.jpeg){.wp-image-316}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The Discriminator &lt;code&gt;D&lt;/code&gt; wants to maximize this, as it wants to correctly identify true data &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;1 - D(G(z))&lt;/code&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;On the other hand, the Generator &lt;code&gt;G&lt;/code&gt; tries to generates data to fool the Discriminator, and it wants to minimize the second half of the loss function:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":317,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/1-n235xeigxkl3ktl08d-cza.jpeg){.wp-image-317}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;By maximizing &lt;code&gt;D&lt;/code&gt; and minimizing &lt;code&gt;G&lt;/code&gt;, we get the function:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":318} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/1-ihk3whuaz_0uek4sjicyfw.png){.wp-image-318}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We then use alternating gradient descent, one step to Maximize the function by Discriminator &lt;code&gt;D&lt;/code&gt;, and the other step to Minimize the function by Generator &lt;code&gt;G&lt;/code&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We fix the Generator's parameters, and perform one iteration of Gradient Descent on the Discriminator. Then we switch and fix the Discriminator's parameters, and perform one iteration of Gradient Descent on the Generator. We keep alternating these steps of Gradient Descent of both models until convergence.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The Discriminator usually wins early on against the Generator, as initially, it is very easy for the Discriminator to identify Generated data because the Generator has not learnt anything yet. As such, the Generator will get diminished gradient, and learning will be very slow. GAN therefore modifies the loss function slightly to backpropagate the gradient to the Generator&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":319} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/1-6so6q3dwurg8qrmwk1y3jw.jpeg){.wp-image-319}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;As the gradient backpropagated to the Generator approaches 0, the GAN changes the function to another one to ensure the gradient to the Generator does not vanish.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Tips for training a GAN&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;When training the Generator, hold the values of the Discriminator constant.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;When training the Discriminator, hold the values of the Generator constant.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;You may train one network that is stronger than the other, giving adverse results: If the Generator is too strong, it will always successfully deceive the Discriminator, leading to a lot of false negatives. If the Discriminator is too strong, it will give outputs that are close to 0 or 1, and the Generator will struggle during learning from gradient descent.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content></entry><entry><title>Visualizing Neural Networks</title><link href="/visualizing-neural-networks.html" rel="alternate"></link><published>2019-05-19T09:31:00+00:00</published><updated>2019-05-19T09:31:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-05-19:/visualizing-neural-networks.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Neural Networks have always been sort of a black box when it comes to it's implementation, and how it produces good results. I came across some material that shows visually, how the neural networks morph the problem space so that they are separable.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Simple Data&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Here's a sample graph that …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Neural Networks have always been sort of a black box when it comes to it's implementation, and how it produces good results. I came across some material that shows visually, how the neural networks morph the problem space so that they are separable.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Simple Data&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Here's a sample graph that is not linearly separable:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":302,"align":"center","width":284,"height":277} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/simple2_data.png){.wp-image-302 width="284" height="277"}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;When we try to use a linear model to discriminate the two data, we get a poorly separated model:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":303,"align":"center","width":278,"height":271} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/simple2_linear.png){.wp-image-303 width="278" height="271"}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Neural Networks, with the interactions of their hidden layers and nodes, are able to learn more complex information about the graph to plot a non-linear separation:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":304,"align":"center","width":270,"height":263} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/simple2_0.png){.wp-image-304 width="270" height="263"}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;What a Neural Networks does is that it warps the space of the problem so that it becomes more separable. The hidden layers in the network transforms the problem space by representing it in a different way&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":305,"align":"center","width":281,"height":274} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/simple2_1.png){.wp-image-305 width="281" height="274"}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;By warping the problem space with the hidden layers, we see that it's able to linearly separate the two distributions. That's pretty cool! So what the neural network is doing is finding the most optimal way to represent the problem that is discriminative.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;So what happens if the data distribution is too complex, or your neural network model is too simple (too shallow) that it can't properly represent the data?&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Spiral Data&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Given a complex data set that resembles a spiral shape, and a neural network model that is too simple, we can see it struggling to find a representation that is separable. This means that there is not enough hidden layers and hidden nodes to transform the data. We need to go deeper!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":308,"align":"center","width":364,"height":355} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/spiral.2.2-2-2-2-2-2-2.gif){.wp-image-308 width="364" height="355"}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Here's the same spiral graph, but with enough hidden layers and nodes to transform the spiral data to a separable space. We can see the model separating the data very clearly&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":307,"align":"center","width":344,"height":336} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/spiral.1-2.2-2-2-2-2-2-1.gif){.wp-image-307 width="344" height="336"}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;More Complex Data&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In the last example, we see a more complex example, and see how a neural network can separate the data.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":309,"align":"center","width":376,"height":283} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/topology_base.png){.wp-image-309 width="376" height="283"}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Given a circular topology data, a shallow neural network will have difficulties trying to separate the data from the inside and outer ring. We see it trying to pull apart the data like how it did with the spiral data, but it fails to do so&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":310,"align":"center","width":351,"height":343} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/topology_2d-2d_train.gif){.wp-image-310 width="351" height="343"}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;By introducing more hidden layers and nodes and going deeper, we see that the data is able to be extracted out into another dimension, making it separable!  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":311} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/02/topology_3d.png){.wp-image-311}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In this post, I wanted to show how neural networks warp the space of the data make them separable, and how a shallow network might fail to perform well.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;By adding more hidden layers and nodes, we are able to morph and warp the space into different dimensions, representing them differently and making them discriminative&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content></entry><entry><title>Tips for Kaggling</title><link href="/tips-for-kaggling.html" rel="alternate"></link><published>2019-05-12T11:31:00+00:00</published><updated>2019-05-12T11:31:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-05-12:/tips-for-kaggling.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;I've been doing Kaggle competitions for awhile (although with not much success), and I've learning quite a few things along the way. One of which is how to properly approach the problem, and iterate through it to climb the LB (leader board).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Setting the baseline&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The first thing I would …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;I've been doing Kaggle competitions for awhile (although with not much success), and I've learning quite a few things along the way. One of which is how to properly approach the problem, and iterate through it to climb the LB (leader board).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Setting the baseline&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The first thing I would do is to use some very simple features, and build a quick model that has a relatively low bias.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;I'll then use cross validation to ensure that I have low variance between train-test splits. This ensures that i'm not overfitting to any segment of the training data.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;After getting a satisfactory bias and variance value, we cross our fingers, and submit our prediction to see how well it does on the LB!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;There are now 3 things you need to take note of:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Cross Validation Score (CV Score)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Leader board Score (LB Score)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Difference between CV and LB Score (Your variance on the testing set)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;If the difference between your CV and LB is high, you're overfitting the training data. Try to tune your model so that the difference isn't too high.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Feature Generation&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Now that you got your baseline, &lt;strong&gt;Do Not Modify Your Model's Parameters!&lt;/strong&gt;..... yet&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Using parameters for the baseline model, you want to generate more features to increase your CV score. Then you make submissions to the LB to checkout your LB score. If your CV score increases, but your LB score stays the same or decreases, you're overfitting.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Here's a snippet of my comments I used to keep track of my CV/LB climb:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;xgb_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; {
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;max_depth&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;: &lt;span class="mi"&gt;4&lt;/span&gt;,  # &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;maximum&lt;/span&gt; &lt;span class="nv"&gt;depth&lt;/span&gt; &lt;span class="nv"&gt;of&lt;/span&gt; &lt;span class="nv"&gt;each&lt;/span&gt; &lt;span class="nv"&gt;tree&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;eta&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;: &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;01&lt;/span&gt;,  # &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;training&lt;/span&gt; &lt;span class="nv"&gt;step&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;each&lt;/span&gt; &lt;span class="nv"&gt;iteration&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;silent&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;: &lt;span class="mi"&gt;1&lt;/span&gt;,  # &lt;span class="nv"&gt;logging&lt;/span&gt; &lt;span class="nv"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nv"&gt;quiet&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;objective&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;: &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;multi:softprob&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;,  # &lt;span class="nv"&gt;error&lt;/span&gt; &lt;span class="nv"&gt;evaluation&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;multiclass&lt;/span&gt; &lt;span class="nv"&gt;training&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;gamma&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;: &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;9&lt;/span&gt;,
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;alpha&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;: &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;3&lt;/span&gt;,
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;colsample_bytree&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;09&lt;/span&gt;,
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;subsample&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;09&lt;/span&gt;,
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;num_class&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;: &lt;span class="mi"&gt;9&lt;/span&gt;}  # &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;number&lt;/span&gt; &lt;span class="nv"&gt;of&lt;/span&gt; &lt;span class="nv"&gt;classes&lt;/span&gt; &lt;span class="nv"&gt;that&lt;/span&gt; &lt;span class="nv"&gt;exist&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;this&lt;/span&gt; &lt;span class="nv"&gt;datset&lt;/span&gt;

## &lt;span class="nv"&gt;Original&lt;/span&gt;
&lt;span class="nv"&gt;nfolds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="nv"&gt;CV&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;7299&lt;/span&gt;
&lt;span class="nv"&gt;std&lt;/span&gt;: &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;0112&lt;/span&gt;
&lt;span class="nv"&gt;LB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;67291&lt;/span&gt;

&lt;span class="nv"&gt;Difference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;05699&lt;/span&gt;

## &lt;span class="nv"&gt;changed&lt;/span&gt; &lt;span class="nv"&gt;rolling&lt;/span&gt; &lt;span class="nv"&gt;window&lt;/span&gt; &lt;span class="nv"&gt;size&lt;/span&gt;
&lt;span class="nv"&gt;nfolds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="nv"&gt;CV&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;7458&lt;/span&gt;
&lt;span class="nv"&gt;std&lt;/span&gt;: &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;0207&lt;/span&gt;
&lt;span class="nv"&gt;LB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;67995&lt;/span&gt;

&lt;span class="nv"&gt;Difference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;06585&lt;/span&gt;

## &lt;span class="nv"&gt;added&lt;/span&gt; &lt;span class="nv"&gt;quantile&lt;/span&gt; &lt;span class="nv"&gt;features&lt;/span&gt;
&lt;span class="nv"&gt;nfolds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="nv"&gt;CV&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;7564&lt;/span&gt;
&lt;span class="nv"&gt;std&lt;/span&gt;: &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;0167&lt;/span&gt;
&lt;span class="nv"&gt;LB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;68347&lt;/span&gt;

&lt;span class="nv"&gt;Difference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;07293&lt;/span&gt;

## &lt;span class="nv"&gt;dropping&lt;/span&gt; &lt;span class="nv"&gt;mean&lt;/span&gt;
&lt;span class="nv"&gt;using&lt;/span&gt; &lt;span class="nv"&gt;smaller&lt;/span&gt; &lt;span class="nv"&gt;feature&lt;/span&gt; &lt;span class="nv"&gt;set&lt;/span&gt; &lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;nfolds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="nv"&gt;CV&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;7670&lt;/span&gt;
&lt;span class="nv"&gt;std&lt;/span&gt;: &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;0243&lt;/span&gt;
&lt;span class="nv"&gt;LB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;69284&lt;/span&gt;

&lt;span class="nv"&gt;Difference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;07416&lt;/span&gt;

## &lt;span class="nv"&gt;adding&lt;/span&gt; &lt;span class="nv"&gt;iqr&lt;/span&gt; &lt;span class="nv"&gt;and&lt;/span&gt; &lt;span class="nv"&gt;trimming&lt;/span&gt; &lt;span class="nv"&gt;mean&lt;/span&gt;
&lt;span class="nv"&gt;using&lt;/span&gt; &lt;span class="nv"&gt;smaller&lt;/span&gt; &lt;span class="nv"&gt;feature&lt;/span&gt; &lt;span class="nv"&gt;set&lt;/span&gt; &lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;70&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;nfolds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="nv"&gt;CV&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;7740&lt;/span&gt;
&lt;span class="nv"&gt;std&lt;/span&gt;: &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;0257&lt;/span&gt;
&lt;span class="nv"&gt;LB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;69988&lt;/span&gt;

&lt;span class="nv"&gt;Difference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;07412&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;As you can see, my parameters are the same, and I'm only adding or removing features to push up my CV and LB&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Feature Selection&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Feature generation was the process of adding in new features, but using all the features (if you have a lot of them), is usually too noisy.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Most models like XGBoost and LightGBM have functions to tell you which features have the most impact on your prediction. You would want to trim your selected features to only include those high impact ones.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;By doing this, I was able to push my score up just a little bit more!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Parameter Tuning&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Once you're done with Feature Generation and Feature Selection, then we come to parameter tuning phase.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this phase, we want to tune the parameters to reduce bias, variance, and CV-LB difference. You can use functions like GridSearch to efficiently search across a range of parameters&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Baseline -&amp;gt; Feature Generation -&amp;gt; Feature Selection -&amp;gt; Parameter Tuning!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This isn't a silver bullet to all competitions, but its the strategy that I use regularly.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In data science and Kaggle, there are plenty of moving parts, and it's easy to get lost in the myriad of factors affecting your prediction. It's always good to have a system to isolate and tackle the problem!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="Kaggle"></category></entry><entry><title>Say NO to Overfitting!</title><link href="/say-no-to-overfitting.html" rel="alternate"></link><published>2019-05-05T13:58:00+00:00</published><updated>2019-05-05T13:58:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-05-05:/say-no-to-overfitting.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Just some experience I've encountered while working on a very small data set of 1703 training samples, and 1705 testing samples.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;One way to combat overfitting is to use cross validation. While doing so, it's important for you not to just look at the final validation score, but also observe …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Just some experience I've encountered while working on a very small data set of 1703 training samples, and 1705 testing samples.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;One way to combat overfitting is to use cross validation. While doing so, it's important for you not to just look at the final validation score, but also observe the training process itself.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;If we just look at the final CV result, we see an astounding &lt;code&gt;~92%&lt;/code&gt; accuracy. This should raise some alarms that shout "Overfitting!". And this is an accurate observation, because when submitted to the Kaggle leader board, it got a measly &lt;code&gt;0.64&lt;/code&gt;, which was below the baseline! And more horribly, the difference between CV and LB score is &lt;code&gt;~30%&lt;/code&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":294} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/xgb2.png){.wp-image-294}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We can also observe the huge disparity between the training error and validation error. A training error of &lt;code&gt;0%&lt;/code&gt;? And validation error of &lt;code&gt;~8%&lt;/code&gt;? Something is really wrong.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Preventing overfitting in XGB&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Since I've used XGBoost, there are several techniques available to combat overfitting, such as regularization, maximum depth of tree and bagging fractions. After applying all of those, I get a final CV score of &lt;code&gt;~74%&lt;/code&gt;, but if we observe the disparity between the training error and validation error, the difference is only &lt;code&gt;~2%&lt;/code&gt;!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":293} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/xgb1.png){.wp-image-293}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;After uploading that to the leader board, I see my score rise up to &lt;code&gt;0.69&lt;/code&gt;. The difference between my CV score and the LB is drop drastically from &lt;code&gt;~30%&lt;/code&gt; to &lt;code&gt;~3%&lt;/code&gt;! Classic example of overfitting.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Aside from feature engineering, a lot of effort should also go into ensuring your model is not too complex for very simple data.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="Overfitting"></category></entry><entry><title>What is Maximum Likelihood Estimation?</title><link href="/what-is-maximum-likelihood-estimation.html" rel="alternate"></link><published>2019-04-28T09:44:00+00:00</published><updated>2019-04-28T09:44:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-04-28:/what-is-maximum-likelihood-estimation.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In machine learning, we often perform what we call &lt;strong&gt;parameter estimation&lt;/strong&gt;, which are the weights that are assigned to each feature of the input data.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For example, in a simple linear model, we use the equation &lt;code&gt;y=mx + c&lt;/code&gt; , and &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; are your parameters to be estimated. For …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In machine learning, we often perform what we call &lt;strong&gt;parameter estimation&lt;/strong&gt;, which are the weights that are assigned to each feature of the input data.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For example, in a simple linear model, we use the equation &lt;code&gt;y=mx + c&lt;/code&gt; , and &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; are your parameters to be estimated. For different values of the parameters, we build different models that produce different estimations of the data&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":286} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/parameters.png){.wp-image-286}

&lt;figcaption&gt;
Given different parameter values, we get different models. In the case of a linear model, each model is a different line on the graph.

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Maximum likelihood is a technique for parameter value estimation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;MLE Parameter Estimation&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Whenever we create a model with certain parameters, the outputs of the model (or the prediction) can be plotted as a probability distribution as well.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;What MLE does it to try to make the distribution of the model close to the distribution of the observed data. Intuitively, this makes the model more accurate, as it becomes more representative of the actual data.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For example, given the following training data distribution points:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":287} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/1-z3jjgvetojmplfvmwiur3q.png){.wp-image-287}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We want to find out which of the graphs below has the highest probability of plotting those points. Each graph has different parameter values, and so they are plotted in different spaces on the graph.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":288} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/1-ulkl0nz1vfg6bmfiqpckzq.png){.wp-image-288}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Just by visual inspection, we can see that the blue line is the graph with the correct parameters that produces those data points. But of course in a machine, there is no visual inspection, only &lt;strong&gt;maths&lt;/strong&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Calculating the MLE&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;We want to calculate what is the total probability of observing all the generated data, or the joint probability of all the data points.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For a single data point for an assume Gaussian distribution, we have the following equation&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":289} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/1-t4zrihvhtlzjzsvcx3jrjg.png){.wp-image-289}

&lt;figcaption&gt;
Probability for observing 1 point

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For 3 data points, we have the following joint probability:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":290} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/1-rfzbq614ir4zewbm3k1v0q.png){.wp-image-290}

&lt;figcaption&gt;
Joint probability for observing 3 points

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This can be extended to &lt;code&gt;n&lt;/code&gt; number of points&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;To calculate the MLE of the parameters, we need to find the values of the parameters in the equation that gives us the maximum value of the probability. To find the maximum, we get the differential of the equation and set it to 0, and solve for the parameters.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Extending the MLE to the least squares method&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;When the distribution is Gaussian, the process of finding the MLE is similar to the least squared method.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For least squares estimation we want to find the line that minimizes the total squared distance between the data points and the regression line.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;When the data distribution is assumed to be Gaussian, the maximum probability is found when the data points get closer to the mean value.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Since the Gaussian distribution is symmetric, this is equivalent to minimizing the distance between the data points and the mean value.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content></entry><entry><title>Activation Functions</title><link href="/activation-functions.html" rel="alternate"></link><published>2019-04-14T18:07:00+00:00</published><updated>2019-04-14T18:07:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-04-14:/activation-functions.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The structure of a deep learning model consists mainly of nodes, and connections between them. Most of the time, every single node is connected to every other node in the next layer, which we call a Dense layer.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":260,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/2.png){.wp-image-260}  
&lt;figcaption&gt;
Nodes in …&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The structure of a deep learning model consists mainly of nodes, and connections between them. Most of the time, every single node is connected to every other node in the next layer, which we call a Dense layer.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":260,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/2.png){.wp-image-260}  
&lt;figcaption&gt;
Nodes in a fully connected neural network
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Within each node is a mathematical equation, decides, based on the input values and their weights, what values to output to the next layer. These mathematical equations are called Activation Functions.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":261,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/3.png){.wp-image-261}  
&lt;figcaption&gt;
The activation function is in the middle box, which performs an operation on the inputs, z, based on their weights and bias value.
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Different Activation Functions&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;There are several kinds of Activation Functions, or in other words, different kinds of mathematical operations that a node can take. They are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Sigmoid Function&lt;/li&gt;
&lt;li&gt;Tanh Function&lt;/li&gt;
&lt;li&gt;ReLU Function&lt;/li&gt;
&lt;li&gt;Leaky ReLU Function&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:image {"id":262,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/4.png){.wp-image-262}  
&lt;figcaption&gt;
Different Activation Functions and their mathematical equations
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;These activation functions take in the inputs &lt;code&gt;z&lt;/code&gt; from the previous layer, and feed it into their equations to produce an output &lt;code&gt;a&lt;/code&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Sigmoid vs TanH&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The TanH function is almost strictly superior to the Sigmoid function, because the TanH function has it's mean centered at &lt;code&gt;0&lt;/code&gt;. This feature will result in a higher value of derivative, and a faster learning rate. Also, having a &lt;code&gt;0&lt;/code&gt; value mean will avoid having bias in the gradients.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;ReLU vs (Sigmoid + TanH)&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The drawback of both Sigmoid and TanH, given that they have a curved graph, is that if the value of &lt;code&gt;z&lt;/code&gt; is either extremely large or small, the gradient on the curve will be extremely small as well. This small gradient will have an adverse effect on the learning rate when performing Gradient Descent.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The solution to this is ReLU (Rectified Linear Unit), which has a constant gradient regardless of the value of &lt;code&gt;z&lt;/code&gt;. But for ReLU, having a negative value of &lt;code&gt;z&lt;/code&gt; will result in a &lt;code&gt;0&lt;/code&gt; value activation. The solution for that is a Leaky ReLU, which allows for a small value of &lt;code&gt;a&lt;/code&gt; for negative values of &lt;code&gt;z&lt;/code&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Must they Always be Non-Linear?&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Yes, Activation Function must always be non-linear. Having multiple linear activation functions can be condensed together, effectively negating the need for any hidden layers or hidden nodes.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In this post, we talked very briefly about the different kinds of Activation Functions, and compared their pro and cons.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A recommendation for building a neural network model is to have the hidden nodes all be either TanH or ReLU, and never having Sigmoid.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The only time you can have a Sigmoid is at your output layer, if your problem is a binary classification problem.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="Activation Functions"></category></entry><entry><title>The Interpretation of ROC and AUC</title><link href="/the-interpretation-of-roc-and-auc.html" rel="alternate"></link><published>2019-04-14T12:52:00+00:00</published><updated>2019-04-14T12:52:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-04-14:/the-interpretation-of-roc-and-auc.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The ROC curve and it's AUC is a common metric for evaluation the performance of a model. In this post, we dig deeper to find out how to interpret the results, and what corrective actions to take to improve it.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;What is it?&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The ROC curve, or Receiver Operating Characteristic …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The ROC curve and it's AUC is a common metric for evaluation the performance of a model. In this post, we dig deeper to find out how to interpret the results, and what corrective actions to take to improve it.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;What is it?&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The ROC curve, or Receiver Operating Characteristic curve works on binary classification problems (True or False). It plots the following values against each other:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;True Positive Rate (TPR): Of those sample that are true, how many did I predict are true? This is also known as Recall (&lt;code&gt;TP/P&lt;/code&gt;, where &lt;code&gt;TP&lt;/code&gt; is how many True samples predicted True, and &lt;code&gt;P&lt;/code&gt; is the total number of True samples)&lt;/li&gt;
&lt;li&gt;False Positive Rate (FPR): Of those samples that are false, how many did I predict are true? This is also known as False Alarms (&lt;code&gt;FP/N&lt;/code&gt;, where &lt;code&gt;FP&lt;/code&gt; is how many False samples predicted True, and &lt;code&gt;N&lt;/code&gt; is the total number of False samples)&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;So... What is it?&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Now that we got the formal definitions out of the way, lets talk about the intuition behind the ROC and AUC.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The TPR is also called &lt;strong&gt;sensitivity&lt;/strong&gt;, which means how many in the &lt;strong&gt;true positives&lt;/strong&gt; have I identified to be &lt;strong&gt;True&lt;/strong&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;TNR (True Negative Rate) is also called &lt;strong&gt;specificity&lt;/strong&gt;, which means how many in the &lt;strong&gt;true negatives&lt;/strong&gt; have I identified to be &lt;strong&gt;False&lt;/strong&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;FPR is (&lt;code&gt;1 - TNR&lt;/code&gt;), which means how many in the &lt;strong&gt;true negatives&lt;/strong&gt; have I identified to be &lt;strong&gt;True&lt;/strong&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The ROC curve plots TPR against FPR.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":257,"align":"center"} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/roc-curves.png){.wp-image-257}  
&lt;figcaption&gt;
Plotting the TPR against the FPR. There are 4 scenarios here
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The top left graph shows the perfect scenario, where the Area Under Curve (AUC) is 1. This means that the model has 100% TPR, regardless of my FPR rate, and correctly classifies all True samples as True.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The bottom right graph shows a random classifier, which is randomly separated.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The question to ask is: what is the lowest possible FPR, that can give me the highest FPR?&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;If we look at the perfect scenario, the highest possible TPR corresponds to an almost 0 FPR. That's perfect! It means the model made no classification errors (Excellent Precision and Recall)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Looking at the random separation, the highest TPR corresponds to the highest FPR, which is horrible. It means that for my model to get a good Recall, I must predict all my False samples to be True as well.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;A good model will thus have a lower FPR that will give a reasonable TPR (Reasonable here depends on the scenario and use case).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Another way to look at it is, to get a good Recall, what is the amount of Precision I have to sacrifice.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;If the model has to sacrifice a lot of Precision to get a good Recall, then it is a bad model. If the model does not have to sacrifice any Precision (In the case of the perfect scenario), then it is a good model.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content></entry><entry><title>Regularization</title><link href="/regularization.html" rel="alternate"></link><published>2019-04-07T09:49:00+00:00</published><updated>2019-04-07T09:49:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-04-07:/regularization.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;One of the major problems in training a model in machine learning is overfitting. Especially when your model gets more and more complex, it starts to memorize the patterns in the training data. This makes it perform poorly on unseen data, which has new patterns.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Overfitting is the result of …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;One of the major problems in training a model in machine learning is overfitting. Especially when your model gets more and more complex, it starts to memorize the patterns in the training data. This makes it perform poorly on unseen data, which has new patterns.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Overfitting is the result of low-bias and high-variance, where it performs well for a single data set, but given new data, the error fluctuates. That means that the model is learning too much for each data set.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;One of the ways to overcome overfitting is Regularization&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;What is Regularization&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The mathematical definition of Regularization is the process of adding information in order to solve ill-posed problems, or to prevent overfitting. Ill-posed meaning that the solution is highly sensitive to the changes in the data.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":248,"align":"center","width":238,"height":228} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/1280px-regularization.png){.wp-image-248 width="238" height="228"}  
&lt;figcaption&gt;
The blue line shows the model before regularization, while the green line shows the model after regularization.  

Regularization makes the model less complex.  
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;By introducing regularization, we reduce the complexity of the learned model. This means that we're reducing the accuracy of the model for a given data set, but in doing so we're making it generalize across data sets. This action reduces variance, while not changing your bias too much, and bring us to the idea situation of low-bias low-variance.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Regularization in Machine Learning&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;To put this into a machine learning context, for each model we use, we have a loss function we wish to minimize. We'll use the RSS (Residual Sum Squares) loss function in this example.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":249} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/rss.png){.wp-image-249}

&lt;figcaption&gt;
RSS loss function we want to minimize

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This will calculate how much to adjust your parameters based on your training data. But if your training data has noise, then your parameters will be adjusted to pick up the noise, and your model will be optimized towards the noise in the data. That's overfitting.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;To combat this, we add in a regularization factor, which will shrink the estimated value to adjust your parameters. This way, your parameters won't move too much towards learning the noise in the data.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Ridge Regression (L2)&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:image {"id":250} --&gt;&lt;/p&gt;
&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/ridge.png){.wp-image-250}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Ridge Regression adds a shrinkage quantity to the original loss function RSS. This works by preventing the change in parameters from being too high in value.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;When &lt;em&gt;λ = 0&lt;/em&gt; , the penalty term is essentially taken out. Your estimated value to modify the parameters will then simply be RSS&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;When &lt;strong&gt;&lt;em&gt;λ→∞&lt;/em&gt;&lt;/strong&gt;, the penalty term, the penalty term grows large, and your estimated value to modify the parameters will approach 0. (But never being 0). Because it never reaches 0, the impact of those noisy features will only be minimized, but never removed.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Lasso Regression (L1)&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:image {"id":251} --&gt;&lt;/p&gt;
&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/lasso.png){.wp-image-251}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Lasso Regression also adds a shrinkage quantity, but the difference is that it only penalizes high valued coefficients.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The penalty term uses &lt;strong&gt;&lt;em&gt;|β1|&lt;/em&gt;&lt;/strong&gt; instead of &lt;strong&gt;&lt;em&gt;β1²&lt;/em&gt;&lt;/strong&gt; , hence it is named L1, while&lt;br&gt;
Ridge regularization is named L2.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Lasso also differs from from Ridge in that it can set coefficients to 0, making them not relevant at all. In the end, because the coefficients are 0, you may end up with lesser features, which is an advantage!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Regularization in Deep Learning - Drop Out&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Regularization in deep learning is slightly different from shallow learning.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In deep learning, we have neurons that are for the most times fully connected. That's to say, every single neuron is connected to every other neuron in the next layer.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This may cause some problems like overfitting again, because the neurons may develop false co-dependencies among each other (which may be due to noise).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Regularization in deep learning works by occasionally ignoring a fraction of the neurons during the training phase.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":252} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/dropout.png){.wp-image-252}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;By using dropout, you're forcing the model to learn more robust features, as opposed to random combinations of neurons. Also, it roughly doubles the number of iterations required to converge.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;To conclude, we've talked about methods in shallow learning and deep learning to combat overfitting by regularization.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Regularization is the process of adding new information to reduce the value to modify the parameters. This prevents it from learning any noise that is specific to the data set, and reduce the chances of overfitting&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="drop out"></category><category term="regularization"></category></entry><entry><title>Microsoft Kaggle Competition</title><link href="/microsoft-kaggle-competition.html" rel="alternate"></link><published>2019-03-31T09:38:00+00:00</published><updated>2019-03-31T09:38:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-03-31:/microsoft-kaggle-competition.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This is the write up for my solution for the Microsoft Malware Prediction&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;a href="https://www.kaggle.com/c/microsoft-malware-prediction"&gt;https://www.kaggle.com/c/microsoft-malware-prediction&lt;/a&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;I got pretty high up the leader board, but it was nothing that I was proud of, because:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;I grossly overfitted my model&lt;/li&gt;
&lt;li&gt;The final result was a blend of another …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This is the write up for my solution for the Microsoft Malware Prediction&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;a href="https://www.kaggle.com/c/microsoft-malware-prediction"&gt;https://www.kaggle.com/c/microsoft-malware-prediction&lt;/a&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;I got pretty high up the leader board, but it was nothing that I was proud of, because:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;I grossly overfitted my model&lt;/li&gt;
&lt;li&gt;The final result was a blend of another kernel&lt;/li&gt;
&lt;li&gt;All my attempts at feature engineering failed&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;And I'm now officially ceasing efforts to improve my score, because:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;I'm not learning anything new, just mindlessly tweaking hyperparameters and hoping one of them works&lt;/li&gt;
&lt;li&gt;I don't think I've got the basics down yet, and this challenge was way over my head. I feel I should start with something simpler&lt;/li&gt;
&lt;li&gt;And most importantly, I'm no longer having fun&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;But wait! Doing such a hard competition really taught me many concepts, such as:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Adversarial Cross Validation&lt;/li&gt;
&lt;li&gt;How to properly use LightGBM&lt;/li&gt;
&lt;li&gt;The real problem of overfitting, which LightGBM does really fast&lt;/li&gt;
&lt;li&gt;Blending and Ensembling is a powerful method for getting good results&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Below shows one of the code variants I've used. They were all mostly the same, with a few hyperparameters being different, and some failed attempts at feature engineering.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The most important features were Version numbers, and there's probably some way to exploit that.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;My single model (which I believe grossly overfitted, because I set &lt;code&gt;num_leaves=8000&lt;/code&gt; got me a score of 0.684. I blended with another blended kernel of 0.688, and my final score was 0.692.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;I think this got me off at the wrong start because I was working and tweaking with those overfitted hyperparameters, when they were wrong in the first place.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Anyway, on to the next competition!  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# This Python 3 environment comes with many helpful analytics libraries installed&lt;/span&gt;
&lt;span class="c1"&gt;# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python&lt;/span&gt;
&lt;span class="c1"&gt;# For example, here&amp;#39;s several helpful packages to load in &lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;gc&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;mpl&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;warnings&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;lightgbm&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;lgb&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;roc_auc_score&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StratifiedKFold&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;category_encoders&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ce&lt;/span&gt;

&lt;span class="n"&gt;le&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LabelEncoder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="c1"&gt;# Any results you write to the current directory are saved as output.&lt;/span&gt;

&lt;span class="n"&gt;dtypes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;#&amp;#39;MachineIdentifier&amp;#39;:                                    &amp;#39;category&amp;#39;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;ProductName&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                          &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;EngineVersion&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                        &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;AppVersion&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                           &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;AvSigVersion&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                         &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;IsBeta&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                               &lt;span class="s1"&gt;&amp;#39;int8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;RtpStateBitfield&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                     &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;IsSxsPassiveMode&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                     &lt;span class="s1"&gt;&amp;#39;int8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;DefaultBrowsersIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                            &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;AVProductStatesIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                            &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;AVProductsInstalled&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                  &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;AVProductsEnabled&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                    &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;HasTpm&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                               &lt;span class="s1"&gt;&amp;#39;int8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;CountryIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                    &lt;span class="s1"&gt;&amp;#39;int32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;CityIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                       &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;OrganizationIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                               &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;GeoNameIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                    &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;LocaleEnglishNameIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                          &lt;span class="s1"&gt;&amp;#39;int32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Platform&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                             &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Processor&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                            &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;OsVer&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                                &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;OsBuild&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                              &lt;span class="s1"&gt;&amp;#39;int16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;OsSuite&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                              &lt;span class="s1"&gt;&amp;#39;int16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;OsPlatformSubRelease&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                 &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;OsBuildLab&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                           &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;SkuEdition&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                           &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;IsProtected&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                          &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;AutoSampleOptIn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                      &lt;span class="s1"&gt;&amp;#39;int8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;PuaMode&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                              &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;SMode&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                                &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;IeVerIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                      &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;SmartScreen&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                          &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Firewall&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                             &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;UacLuaenable&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                         &lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_MDC2FormFactor&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_DeviceFamily&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                  &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OEMNameIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                             &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OEMModelIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                            &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_ProcessorCoreCount&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                            &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_ProcessorManufacturerIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;               &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_ProcessorModelIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                      &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_ProcessorClass&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_PrimaryDiskTotalCapacity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                      &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_PrimaryDiskTypeName&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                           &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_SystemVolumeTotalCapacity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                     &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_HasOpticalDiskDrive&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                           &lt;span class="s1"&gt;&amp;#39;int8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_TotalPhysicalRAM&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                              &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_ChassisTypeName&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                               &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_InternalPrimaryDiagonalDisplaySizeInInches&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;    &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_InternalPrimaryDisplayResolutionHorizontal&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;    &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_InternalPrimaryDisplayResolutionVertical&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;      &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_PowerPlatformRoleName&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                         &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_InternalBatteryType&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                           &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_InternalBatteryNumberOfCharges&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OSVersion&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                     &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OSArchitecture&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OSBranch&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                      &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OSBuildNumber&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                 &lt;span class="s1"&gt;&amp;#39;int32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OSBuildRevision&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                               &lt;span class="s1"&gt;&amp;#39;int32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OSEdition&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                     &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OSSkuName&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                     &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OSInstallTypeName&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                             &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OSInstallLanguageIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                   &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OSUILocaleIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                          &lt;span class="s1"&gt;&amp;#39;int32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_OSWUAutoUpdateOptionsName&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                     &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_IsPortableOperatingSystem&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                     &lt;span class="s1"&gt;&amp;#39;int8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_GenuineStateName&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                              &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_ActivationChannel&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                             &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_IsFlightingInternal&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                           &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_IsFlightsDisabled&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                             &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_FlightRing&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                    &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_ThresholdOptIn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_FirmwareManufacturerIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_FirmwareVersionIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                     &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_IsSecureBootEnabled&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                           &lt;span class="s1"&gt;&amp;#39;int8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_IsWIMBootEnabled&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                              &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_IsVirtualDevice&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                               &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_IsTouchEnabled&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                &lt;span class="s1"&gt;&amp;#39;int8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_IsPenCapable&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                  &lt;span class="s1"&gt;&amp;#39;int8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Census_IsAlwaysOnAlwaysConnectedCapable&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;              &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Wdft_IsGamer&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                         &lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Wdft_RegionIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                &lt;span class="s1"&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;HasDetections&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;                                        &lt;span class="s1"&gt;&amp;#39;int8&amp;#39;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;


&lt;span class="c1"&gt;# read data&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Reading Data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sorted.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nrows&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MachineIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MachineIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Reading&lt;/span&gt; &lt;span class="k"&gt;Data&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;df_train&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

&lt;span class="nv"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; [&lt;span class="nv"&gt;f&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;f&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;df_train&lt;/span&gt;.&lt;span class="nv"&gt;columns&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nv"&gt;f&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;HasDetections&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;]

#&lt;span class="nv"&gt;assign&lt;/span&gt; &lt;span class="nv"&gt;target&lt;/span&gt; &lt;span class="nv"&gt;variable&lt;/span&gt; &lt;span class="nv"&gt;to&lt;/span&gt; &lt;span class="nv"&gt;y_train&lt;/span&gt;
&lt;span class="nv"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;df_train&lt;/span&gt;[&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;HasDetections&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;]
&lt;span class="nv"&gt;Train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;df_train&lt;/span&gt;[&lt;span class="nv"&gt;features&lt;/span&gt;]
&lt;span class="nv"&gt;Test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;df_test&lt;/span&gt;[&lt;span class="nv"&gt;features&lt;/span&gt;]
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:heading --&gt;

&lt;h2&gt;Feature Engineering&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:code --&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;def&lt;/span&gt; &lt;span class="nv"&gt;add_noise&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;series&lt;/span&gt;, &lt;span class="nv"&gt;noise_level&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nv"&gt;series&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nv"&gt;noise_level&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="k"&gt;random&lt;/span&gt;.&lt;span class="nv"&gt;randn&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;series&lt;/span&gt;&lt;span class="ss"&gt;)))&lt;/span&gt;

&lt;span class="nv"&gt;def&lt;/span&gt; &lt;span class="nv"&gt;target_encode&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;trn_series&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;None&lt;/span&gt;, 
                  &lt;span class="nv"&gt;tst_series&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;None&lt;/span&gt;, 
                  &lt;span class="nv"&gt;target&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;None&lt;/span&gt;, 
                  &lt;span class="nv"&gt;min_samples_leaf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;, 
                  &lt;span class="nv"&gt;smoothing&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;,
                  &lt;span class="nv"&gt;noise_level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:
    &lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="nv"&gt;Smoothing&lt;/span&gt; &lt;span class="nv"&gt;is&lt;/span&gt; &lt;span class="nv"&gt;computed&lt;/span&gt; &lt;span class="nv"&gt;like&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;following&lt;/span&gt; &lt;span class="nv"&gt;paper&lt;/span&gt; &lt;span class="nv"&gt;by&lt;/span&gt; &lt;span class="nv"&gt;Daniele&lt;/span&gt; &lt;span class="nv"&gt;Micci&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nv"&gt;Barreca&lt;/span&gt;
    &lt;span class="nv"&gt;https&lt;/span&gt;:&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="nv"&gt;kaggle2&lt;/span&gt;.&lt;span class="nv"&gt;blob&lt;/span&gt;.&lt;span class="nv"&gt;core&lt;/span&gt;.&lt;span class="nv"&gt;windows&lt;/span&gt;.&lt;span class="nv"&gt;net&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nv"&gt;forum&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nv"&gt;message&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nv"&gt;attachments&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;225952&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;7441&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nv"&gt;high&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="nv"&gt;cardinality&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="nv"&gt;categoricals&lt;/span&gt;.&lt;span class="nv"&gt;pdf&lt;/span&gt;
    &lt;span class="nv"&gt;trn_series&lt;/span&gt; : &lt;span class="nv"&gt;training&lt;/span&gt; &lt;span class="nv"&gt;categorical&lt;/span&gt; &lt;span class="nv"&gt;feature&lt;/span&gt; &lt;span class="nv"&gt;as&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt; &lt;span class="nv"&gt;pd&lt;/span&gt;.&lt;span class="nv"&gt;Series&lt;/span&gt;
    &lt;span class="nv"&gt;tst_series&lt;/span&gt; : &lt;span class="nv"&gt;test&lt;/span&gt; &lt;span class="nv"&gt;categorical&lt;/span&gt; &lt;span class="nv"&gt;feature&lt;/span&gt; &lt;span class="nv"&gt;as&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt; &lt;span class="nv"&gt;pd&lt;/span&gt;.&lt;span class="nv"&gt;Series&lt;/span&gt;
    &lt;span class="nv"&gt;target&lt;/span&gt; : &lt;span class="nv"&gt;target&lt;/span&gt; &lt;span class="nv"&gt;data&lt;/span&gt; &lt;span class="nv"&gt;as&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt; &lt;span class="nv"&gt;pd&lt;/span&gt;.&lt;span class="nv"&gt;Series&lt;/span&gt;
    &lt;span class="nv"&gt;min_samples_leaf&lt;/span&gt; &lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;int&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt; : &lt;span class="nv"&gt;minimum&lt;/span&gt; &lt;span class="nv"&gt;samples&lt;/span&gt; &lt;span class="nv"&gt;to&lt;/span&gt; &lt;span class="nv"&gt;take&lt;/span&gt; &lt;span class="nv"&gt;category&lt;/span&gt; &lt;span class="nv"&gt;average&lt;/span&gt; &lt;span class="nv"&gt;into&lt;/span&gt; &lt;span class="nv"&gt;account&lt;/span&gt;
    &lt;span class="nv"&gt;smoothing&lt;/span&gt; &lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;int&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt; : &lt;span class="nv"&gt;smoothing&lt;/span&gt; &lt;span class="nv"&gt;effect&lt;/span&gt; &lt;span class="nv"&gt;to&lt;/span&gt; &lt;span class="nv"&gt;balance&lt;/span&gt; &lt;span class="nv"&gt;categorical&lt;/span&gt; &lt;span class="nv"&gt;average&lt;/span&gt; &lt;span class="nv"&gt;vs&lt;/span&gt; &lt;span class="nv"&gt;prior&lt;/span&gt;  
    &lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;
    &lt;span class="nv"&gt;assert&lt;/span&gt; &lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;trn_series&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;target&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;assert&lt;/span&gt; &lt;span class="nv"&gt;trn_series&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nv"&gt;tst_series&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt;
    &lt;span class="nv"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;pd&lt;/span&gt;.&lt;span class="nv"&gt;concat&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[&lt;span class="nv"&gt;trn_series&lt;/span&gt;, &lt;span class="nv"&gt;target&lt;/span&gt;], &lt;span class="nv"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    # &lt;span class="nv"&gt;Compute&lt;/span&gt; &lt;span class="nv"&gt;target&lt;/span&gt; &lt;span class="nv"&gt;mean&lt;/span&gt; 
    &lt;span class="nv"&gt;averages&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;temp&lt;/span&gt;.&lt;span class="nv"&gt;groupby&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;trn_series&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;[&lt;span class="nv"&gt;target&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt;].&lt;span class="nv"&gt;agg&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;mean&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;count&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;]&lt;span class="ss"&gt;)&lt;/span&gt;
    # &lt;span class="nv"&gt;Compute&lt;/span&gt; &lt;span class="nv"&gt;smoothing&lt;/span&gt;
    &lt;span class="nv"&gt;smoothing&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;exp&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;averages&lt;/span&gt;[&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;count&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;] &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nv"&gt;min_samples_leaf&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nv"&gt;smoothing&lt;/span&gt;&lt;span class="ss"&gt;))&lt;/span&gt;
    # &lt;span class="nv"&gt;Apply&lt;/span&gt; &lt;span class="nv"&gt;average&lt;/span&gt; &lt;span class="nv"&gt;function&lt;/span&gt; &lt;span class="nv"&gt;to&lt;/span&gt; &lt;span class="nv"&gt;all&lt;/span&gt; &lt;span class="nv"&gt;target&lt;/span&gt; &lt;span class="nv"&gt;data&lt;/span&gt;
    &lt;span class="nv"&gt;prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;target&lt;/span&gt;.&lt;span class="nv"&gt;mean&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;
    # &lt;span class="nv"&gt;The&lt;/span&gt; &lt;span class="nv"&gt;bigger&lt;/span&gt; &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;count&lt;/span&gt; &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;less&lt;/span&gt; &lt;span class="nv"&gt;full_avg&lt;/span&gt; &lt;span class="nv"&gt;is&lt;/span&gt; &lt;span class="nv"&gt;taken&lt;/span&gt; &lt;span class="nv"&gt;into&lt;/span&gt; &lt;span class="nv"&gt;account&lt;/span&gt;
    &lt;span class="nv"&gt;averages&lt;/span&gt;[&lt;span class="nv"&gt;target&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt;] &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;prior&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nv"&gt;smoothing&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nv"&gt;averages&lt;/span&gt;[&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;mean&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;] &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nv"&gt;smoothing&lt;/span&gt;
    &lt;span class="nv"&gt;averages&lt;/span&gt;.&lt;span class="nv"&gt;drop&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;mean&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;count&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;], &lt;span class="nv"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;, &lt;span class="nv"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;True&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    # &lt;span class="nv"&gt;Apply&lt;/span&gt; &lt;span class="nv"&gt;averages&lt;/span&gt; &lt;span class="nv"&gt;to&lt;/span&gt; &lt;span class="nv"&gt;trn&lt;/span&gt; &lt;span class="nv"&gt;and&lt;/span&gt; &lt;span class="nv"&gt;tst&lt;/span&gt; &lt;span class="nv"&gt;series&lt;/span&gt;
    &lt;span class="nv"&gt;ft_trn_series&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;pd&lt;/span&gt;.&lt;span class="nv"&gt;merge&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;
        &lt;span class="nv"&gt;trn_series&lt;/span&gt;.&lt;span class="nv"&gt;to_frame&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;trn_series&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;,
        &lt;span class="nv"&gt;averages&lt;/span&gt;.&lt;span class="nv"&gt;reset_index&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;.&lt;span class="nv"&gt;rename&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;{&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;index&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;: &lt;span class="nv"&gt;target&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt;, &lt;span class="nv"&gt;target&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt;: &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;average&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;}&lt;span class="ss"&gt;)&lt;/span&gt;,
        &lt;span class="nv"&gt;on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;trn_series&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt;,
        &lt;span class="nv"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;left&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;[&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;average&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;].&lt;span class="nv"&gt;rename&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;trn_series&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;_mean&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;.&lt;span class="nv"&gt;fillna&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;prior&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    # &lt;span class="nv"&gt;pd&lt;/span&gt;.&lt;span class="nv"&gt;merge&lt;/span&gt; &lt;span class="nv"&gt;does&lt;/span&gt; &lt;span class="nv"&gt;not&lt;/span&gt; &lt;span class="nv"&gt;keep&lt;/span&gt; &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;index&lt;/span&gt; &lt;span class="nv"&gt;so&lt;/span&gt; &lt;span class="nv"&gt;restore&lt;/span&gt; &lt;span class="nv"&gt;it&lt;/span&gt;
    &lt;span class="nv"&gt;ft_trn_series&lt;/span&gt;.&lt;span class="nv"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;trn_series&lt;/span&gt;.&lt;span class="nv"&gt;index&lt;/span&gt; 
    &lt;span class="nv"&gt;ft_tst_series&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;pd&lt;/span&gt;.&lt;span class="nv"&gt;merge&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;
        &lt;span class="nv"&gt;tst_series&lt;/span&gt;.&lt;span class="nv"&gt;to_frame&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;tst_series&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;,
        &lt;span class="nv"&gt;averages&lt;/span&gt;.&lt;span class="nv"&gt;reset_index&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;.&lt;span class="nv"&gt;rename&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;{&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;index&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;: &lt;span class="nv"&gt;target&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt;, &lt;span class="nv"&gt;target&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt;: &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;average&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;}&lt;span class="ss"&gt;)&lt;/span&gt;,
        &lt;span class="nv"&gt;on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;tst_series&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt;,
        &lt;span class="nv"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;left&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;[&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;average&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;].&lt;span class="nv"&gt;rename&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;trn_series&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;_mean&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;.&lt;span class="nv"&gt;fillna&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;prior&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    # &lt;span class="nv"&gt;pd&lt;/span&gt;.&lt;span class="nv"&gt;merge&lt;/span&gt; &lt;span class="nv"&gt;does&lt;/span&gt; &lt;span class="nv"&gt;not&lt;/span&gt; &lt;span class="nv"&gt;keep&lt;/span&gt; &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;index&lt;/span&gt; &lt;span class="nv"&gt;so&lt;/span&gt; &lt;span class="nv"&gt;restore&lt;/span&gt; &lt;span class="nv"&gt;it&lt;/span&gt;
    &lt;span class="nv"&gt;ft_tst_series&lt;/span&gt;.&lt;span class="nv"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;tst_series&lt;/span&gt;.&lt;span class="nv"&gt;index&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nv"&gt;add_noise&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;ft_trn_series&lt;/span&gt;, &lt;span class="nv"&gt;noise_level&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;, &lt;span class="nv"&gt;add_noise&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;ft_tst_series&lt;/span&gt;, &lt;span class="nv"&gt;noise_level&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# &lt;span class="nv"&gt;combine&lt;/span&gt; &lt;span class="nv"&gt;dtrain&lt;/span&gt; &lt;span class="nv"&gt;and&lt;/span&gt; &lt;span class="nv"&gt;dtest&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;preprocessing&lt;/span&gt;
&lt;span class="nv"&gt;alldata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;pd&lt;/span&gt;.&lt;span class="nv"&gt;concat&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[&lt;span class="nv"&gt;Train&lt;/span&gt;,&lt;span class="nv"&gt;Test&lt;/span&gt;], &lt;span class="nv"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

&lt;span class="nv"&gt;trainlen&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;Train&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

# &lt;span class="nv"&gt;convert&lt;/span&gt; &lt;span class="nv"&gt;character&lt;/span&gt; &lt;span class="nv"&gt;columns&lt;/span&gt; &lt;span class="nv"&gt;to&lt;/span&gt; &lt;span class="nv"&gt;integer&lt;/span&gt;
&lt;span class="nv"&gt;print&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;Label Encoding&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;numeric&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;Train&lt;/span&gt;.&lt;span class="nv"&gt;select_dtypes&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="k"&gt;include&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;number&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;.&lt;span class="nv"&gt;columns&lt;/span&gt;.&lt;span class="nv"&gt;tolist&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;
&lt;span class="nv"&gt;categorical&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; [&lt;span class="nv"&gt;f&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;f&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;Train&lt;/span&gt;.&lt;span class="nv"&gt;columns&lt;/span&gt;.&lt;span class="nv"&gt;tolist&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nv"&gt;f&lt;/span&gt; &lt;span class="nv"&gt;not&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;numeric&lt;/span&gt;]
&lt;span class="nv"&gt;categorical&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;cat&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;categorical&lt;/span&gt;:
    &lt;span class="nv"&gt;alldata&lt;/span&gt;[&lt;span class="nv"&gt;cat&lt;/span&gt;] &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;le&lt;/span&gt;.&lt;span class="nv"&gt;fit_transform&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;alldata&lt;/span&gt;[&lt;span class="nv"&gt;cat&lt;/span&gt;].&lt;span class="nv"&gt;fillna&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;alldata&lt;/span&gt;[&lt;span class="nv"&gt;cat&lt;/span&gt;].&lt;span class="nv"&gt;mode&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;.&lt;span class="nv"&gt;iloc&lt;/span&gt;[&lt;span class="mi"&gt;0&lt;/span&gt;]&lt;span class="ss"&gt;))&lt;/span&gt;

# &lt;span class="nv"&gt;split&lt;/span&gt; &lt;span class="nv"&gt;back&lt;/span&gt; &lt;span class="nv"&gt;into&lt;/span&gt; &lt;span class="nv"&gt;dtrain&lt;/span&gt; &lt;span class="nv"&gt;and&lt;/span&gt; &lt;span class="nv"&gt;dtest&lt;/span&gt;
&lt;span class="nv"&gt;Train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;alldata&lt;/span&gt;[:&lt;span class="nv"&gt;trainlen&lt;/span&gt;]
&lt;span class="nv"&gt;Test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;alldata&lt;/span&gt;[&lt;span class="nv"&gt;trainlen&lt;/span&gt;:]
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:heading --&gt;

&lt;h2&gt;Training&lt;/h2&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:code --&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; {
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;boosting_type&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;gbdt&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;, 
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;objective&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;binary&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;,
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;metric&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;binary_logloss&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;, 
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;nthread&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="mi"&gt;16&lt;/span&gt;, 
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;learning_rate&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;02&lt;/span&gt;,
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;max_depth&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="mi"&gt;12&lt;/span&gt;,
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;num_leaves&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="mi"&gt;100&lt;/span&gt;,
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;sub_feature&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;7&lt;/span&gt;, 
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;sub_row&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;7&lt;/span&gt;, 
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;bagging_freq&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="mi"&gt;1&lt;/span&gt;,
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;bagging_fraction&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;8&lt;/span&gt;,
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;lambda_l1&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;: &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;1&lt;/span&gt;, 
    &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;lambda_l2&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; : &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;2&lt;/span&gt;
}

&lt;span class="nv"&gt;def&lt;/span&gt; &lt;span class="nv"&gt;modeling_cross_validation&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;params&lt;/span&gt;, &lt;span class="nv"&gt;X&lt;/span&gt;, &lt;span class="nv"&gt;y&lt;/span&gt;, &lt;span class="nv"&gt;folds&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:
    &lt;span class="nv"&gt;clfs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; []
    &lt;span class="nv"&gt;allPreds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;zeros&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;X&lt;/span&gt;.&lt;span class="nv"&gt;shape&lt;/span&gt;[&lt;span class="mi"&gt;0&lt;/span&gt;]&lt;span class="ss"&gt;)&lt;/span&gt;

    &lt;span class="nv"&gt;evals_result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; {}

    &lt;span class="nv"&gt;X_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;int&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;X&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

    &lt;span class="nv"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;X&lt;/span&gt;[&lt;span class="nv"&gt;X_len&lt;/span&gt;:]
    &lt;span class="nv"&gt;X_valid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;X&lt;/span&gt;[:&lt;span class="nv"&gt;X_len&lt;/span&gt;]
    &lt;span class="nv"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;y&lt;/span&gt;[&lt;span class="nv"&gt;X_len&lt;/span&gt;:]
    &lt;span class="nv"&gt;y_valid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;y&lt;/span&gt;[:&lt;span class="nv"&gt;X_len&lt;/span&gt;] 

    &lt;span class="nv"&gt;lgb_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;lgb&lt;/span&gt;.&lt;span class="nv"&gt;Dataset&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;X_train&lt;/span&gt;, &lt;span class="nv"&gt;y_train&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;lgb_valid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;lgb&lt;/span&gt;.&lt;span class="nv"&gt;Dataset&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;X_valid&lt;/span&gt;, &lt;span class="nv"&gt;y_valid&lt;/span&gt;, &lt;span class="nv"&gt;reference&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;lgb_train&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

    &lt;span class="nv"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;lgb&lt;/span&gt;.&lt;span class="nv"&gt;train&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;params&lt;/span&gt;,
                &lt;span class="nv"&gt;lgb_train&lt;/span&gt;,
                &lt;span class="nv"&gt;num_boost_round&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;,
                &lt;span class="nv"&gt;valid_sets&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;[&lt;span class="nv"&gt;lgb_train&lt;/span&gt;, &lt;span class="nv"&gt;lgb_valid&lt;/span&gt;],
                &lt;span class="nv"&gt;evals_result&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;evals_result&lt;/span&gt;,
                &lt;span class="nv"&gt;verbose_eval&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;,
                &lt;span class="nv"&gt;early_stopping_rounds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

    &lt;span class="nv"&gt;print&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;Plot metrics during training...&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;lgb&lt;/span&gt;.&lt;span class="nv"&gt;plot_metric&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;evals_result&lt;/span&gt;, &lt;span class="nv"&gt;metric&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;binary_logloss&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;plt&lt;/span&gt;.&lt;span class="k"&gt;show&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;

    &lt;span class="nv"&gt;print&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;Plot feature importances...&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;lgb&lt;/span&gt;.&lt;span class="nv"&gt;plot_importance&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;model&lt;/span&gt;, &lt;span class="nv"&gt;max_num_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;plt&lt;/span&gt;.&lt;span class="k"&gt;show&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;

    &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="nv"&gt;allPreds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;model&lt;/span&gt;.&lt;span class="nv"&gt;predict&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;X_test&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

    &lt;span class="nv"&gt;score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;roc_auc_score&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;y_test&lt;/span&gt;, &lt;span class="nv"&gt;allPreds&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;print&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;score&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="nv"&gt;clfs&lt;/span&gt;.&lt;span class="nv"&gt;append&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;model&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nv"&gt;clfs&lt;/span&gt;


&lt;span class="nv"&gt;def&lt;/span&gt; &lt;span class="nv"&gt;predict_cross_validation&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;test&lt;/span&gt;, &lt;span class="nv"&gt;clfs&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:

    &lt;span class="nv"&gt;sub_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;zeros&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;test&lt;/span&gt;.&lt;span class="nv"&gt;shape&lt;/span&gt;[&lt;span class="mi"&gt;0&lt;/span&gt;]&lt;span class="ss"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;i&lt;/span&gt;, &lt;span class="nv"&gt;model&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;enumerate&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;clfs&lt;/span&gt;, &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:    
        &lt;span class="nv"&gt;test_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;model&lt;/span&gt;.&lt;span class="nv"&gt;predict_proba&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;test&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
        &lt;span class="nv"&gt;sub_preds&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="nv"&gt;test_preds&lt;/span&gt;[:,&lt;span class="mi"&gt;1&lt;/span&gt;]

    # &lt;span class="nv"&gt;Averaging&lt;/span&gt; &lt;span class="nv"&gt;across&lt;/span&gt; &lt;span class="nv"&gt;all&lt;/span&gt; &lt;span class="nv"&gt;models&lt;/span&gt;
    &lt;span class="nv"&gt;sub_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;sub_preds&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;clfs&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

    # &lt;span class="nv"&gt;Creating&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt; &lt;span class="nv"&gt;series&lt;/span&gt; &lt;span class="nv"&gt;from&lt;/span&gt; &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;predictions&lt;/span&gt;
    &lt;span class="nv"&gt;ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;pd&lt;/span&gt;.&lt;span class="nv"&gt;Series&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;sub_preds&lt;/span&gt;, &lt;span class="nv"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;test&lt;/span&gt;.&lt;span class="nv"&gt;index&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;ret&lt;/span&gt;.&lt;span class="nv"&gt;index&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;test&lt;/span&gt;.&lt;span class="nv"&gt;index&lt;/span&gt;.&lt;span class="nv"&gt;name&lt;/span&gt; 

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nv"&gt;ret&lt;/span&gt;

&lt;span class="nv"&gt;def&lt;/span&gt; &lt;span class="nv"&gt;predict_test_chunk&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;clfs&lt;/span&gt;, &lt;span class="nv"&gt;Test&lt;/span&gt;, &lt;span class="nv"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;tmp.csv&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;, &lt;span class="nv"&gt;chunks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:

    &lt;span class="nv"&gt;preds_sub&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;pd&lt;/span&gt;.&lt;span class="nv"&gt;DataFrame&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;

    &lt;span class="nv"&gt;num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;int&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7853253&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

    &lt;span class="nv"&gt;groups&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;Test&lt;/span&gt;.&lt;span class="nv"&gt;groupby&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;arange&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;Test&lt;/span&gt;&lt;span class="ss"&gt;))&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="nv"&gt;chunks&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

    &lt;span class="nv"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;idx&lt;/span&gt;, &lt;span class="nv"&gt;df&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;groups&lt;/span&gt;:

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nv"&gt;count&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;:
            &lt;span class="nv"&gt;print&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;Running on idx {} of {}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;.&lt;span class="nv"&gt;format&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;idx&lt;/span&gt;, &lt;span class="nv"&gt;num&lt;/span&gt;&lt;span class="ss"&gt;))&lt;/span&gt;
            &lt;span class="nv"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="nv"&gt;count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

        &lt;span class="nv"&gt;preds_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;predict_cross_validation&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;df&lt;/span&gt;, &lt;span class="nv"&gt;clfs&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
        &lt;span class="nv"&gt;preds_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;preds_df&lt;/span&gt;.&lt;span class="nv"&gt;to_frame&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;HasDetections&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

        &lt;span class="nv"&gt;preds_sub&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;pd&lt;/span&gt;.&lt;span class="nv"&gt;concat&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[&lt;span class="nv"&gt;preds_sub&lt;/span&gt;, &lt;span class="nv"&gt;preds_df&lt;/span&gt;], &lt;span class="nv"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

        &lt;span class="nv"&gt;del&lt;/span&gt; &lt;span class="nv"&gt;preds_df&lt;/span&gt;
        &lt;span class="nv"&gt;gc&lt;/span&gt;.&lt;span class="nv"&gt;collect&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nv"&gt;preds_sub&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Start Training&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clfs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;modeling_cross_validation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;Start&lt;/span&gt; &lt;span class="nv"&gt;Training&lt;/span&gt;
&lt;span class="nv"&gt;Training&lt;/span&gt; &lt;span class="k"&gt;until&lt;/span&gt; &lt;span class="nv"&gt;validation&lt;/span&gt; &lt;span class="nv"&gt;scores&lt;/span&gt; &lt;span class="nv"&gt;don&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;t improve for 50 rounds.&lt;/span&gt;
[&lt;span class="mi"&gt;100&lt;/span&gt;]    &lt;span class="nv"&gt;training&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;s binary_logloss: 0.626603 valid_1&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;s&lt;/span&gt; &lt;span class="nv"&gt;binary_logloss&lt;/span&gt;: &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;632894&lt;/span&gt;
[&lt;span class="mi"&gt;200&lt;/span&gt;]    &lt;span class="nv"&gt;training&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;s binary_logloss: 0.614641 valid_1&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;s&lt;/span&gt; &lt;span class="nv"&gt;binary_logloss&lt;/span&gt;: &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;624013&lt;/span&gt;
&lt;span class="nv"&gt;Did&lt;/span&gt; &lt;span class="nv"&gt;not&lt;/span&gt; &lt;span class="nv"&gt;meet&lt;/span&gt; &lt;span class="nv"&gt;early&lt;/span&gt; &lt;span class="nv"&gt;stopping&lt;/span&gt;. &lt;span class="nv"&gt;Best&lt;/span&gt; &lt;span class="nv"&gt;iteration&lt;/span&gt; &lt;span class="nv"&gt;is&lt;/span&gt;:
[&lt;span class="mi"&gt;200&lt;/span&gt;]    &lt;span class="nv"&gt;training&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;s binary_logloss: 0.614641 valid_1&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt;s&lt;/span&gt; &lt;span class="nv"&gt;binary_logloss&lt;/span&gt;: &lt;span class="mi"&gt;0&lt;/span&gt;.&lt;span class="mi"&gt;624013&lt;/span&gt;
&lt;span class="nv"&gt;Plot&lt;/span&gt; &lt;span class="nv"&gt;metrics&lt;/span&gt; &lt;span class="nv"&gt;during&lt;/span&gt; &lt;span class="nv"&gt;training&lt;/span&gt;...
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt; &lt;span class="n"&gt;feature&lt;/span&gt; &lt;span class="n"&gt;importances&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:image {"id":241} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/output_9_1.png){.wp-image-241}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:image {"id":242} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2019/01/output_9_3.png){.wp-image-242}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Start Predicting&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predict_test_chunk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clfs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;chunks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Start Submission&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df_sub&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sample_submission.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df_sub&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;HasDetections&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;HasDetections&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="k"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;df_sub&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mySubmission.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Done!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;</content><category term="Kaggle"></category></entry><entry><title>Model Capacity</title><link href="/model-capacity.html" rel="alternate"></link><published>2019-03-24T20:19:00+00:00</published><updated>2019-03-24T20:19:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-03-24:/model-capacity.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;While studying the book Deep Learning by Ian Goodfellow, I came across this concept of model capacity, and it was really intuitive in helping me understand the models representation of a given problem.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This ties to the concept of overfitting and underfitting&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Capacity&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Put simply, the capacity of the model …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;While studying the book Deep Learning by Ian Goodfellow, I came across this concept of model capacity, and it was really intuitive in helping me understand the models representation of a given problem.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This ties to the concept of overfitting and underfitting&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Capacity&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Put simply, the capacity of the model is the complexity of the model, or the ability to represent complex relationships.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A model with high capacity can represent very complex relationships, while a model with low capacity can represent not so complex relationships&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;How it ties to Overfitting and Underfitting&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Models perform best when their capacity approximately captures the complexity of the task they need to perform.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;If the task or data set has a very low complexity, but the model has a very high capacity, the model will overfit, as it will memorize and represent all the features in the training set that may not be in the testing set.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Conversely, if the task or data set is highly complex, but the capacity of the model is low, the model will underfit, as it cannot sufficiently represent the complex relationships and features in the training data&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Vapnik–Chervonenkis (&lt;strong&gt;VC&lt;/strong&gt;) &lt;strong&gt;dimension&lt;/strong&gt;&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The VC dimension is a measurement of a model's capacity, given that the task at hand is a binary classification problem.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The numerical value of a VC dimension tells us the largest set of data point the model can perfectly classify (or shatter). Thus, the larger the VC dimension, the more data points the model is able to perfectly classify, which also means that the model is inherently more complex.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The usage of a VC dimension usually just tells us how complex an algorithm is. A model with a higher VC dimension requires more data to properly train due to the higher complexity.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The take away here is that, overly complicated models are not always better as they can overfit. A simple decision tree or random forest can be perfect for data with low complexity.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A VC dimension is simply a quantification of how complex a model is. Higher VC = Able to separate more data points = more complex = requires more training data.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="Capacity"></category><category term="VC Dimension"></category></entry><entry><title>Counts Based Featurization</title><link href="/counts-based-featurization.html" rel="alternate"></link><published>2019-03-24T17:48:00+00:00</published><updated>2019-03-24T17:48:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-03-24:/counts-based-featurization.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;While doing the Microsoft Malware Classification challenge, I encountered a way of Feature representation called Count Based Features (CBF).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;CBF is good to use with very high cardinality features, and it transforms the high number of categories in the data to the number of it's occurrences. This representation is helpful …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;While doing the Microsoft Malware Classification challenge, I encountered a way of Feature representation called Count Based Features (CBF).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;CBF is good to use with very high cardinality features, and it transforms the high number of categories in the data to the number of it's occurrences. This representation is helpful because it extracts out a simple inherent feature of the data: count&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Below shows a simple example of how we get the CBF of a given feature&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:table --&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Label&lt;/strong&gt;   &lt;strong&gt;Feature1&lt;/strong&gt;
  0           A
  0           A
  1           A
  0           B
  1           B
  1           B
  1           B&lt;/p&gt;
&lt;hr&gt;
&lt;!-- /wp:table --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;CBF can be done in pandas in a single line&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;code&gt;Train.groupby([' Feature1 '])[' Feature1 '].transform('count')&lt;/code&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The output of this will give you&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:table --&gt;

&lt;p&gt;Label   &lt;strong&gt;Feature1&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;0       3
  0       3
  1       3
  0       4
  1       4
  1       4
  1       4&lt;/p&gt;
&lt;!-- /wp:table --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;As you can see, the categorical values are all converted their count values!  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="Count Feature"></category></entry><entry><title>LightGBM</title><link href="/lightgbm.html" rel="alternate"></link><published>2019-03-24T12:33:00+00:00</published><updated>2019-03-24T12:33:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-03-24:/lightgbm.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For some time, XGBoost was considered the Kaggle-Killer, being the winning model for most prediction problems. Recently Microsoft released their own gradient boosting framework called LightGBM, and it is way faster than XGB. In this post, I'm going to touch on the interesting portions of LightGBM.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;What is LightGBM?&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Similar …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For some time, XGBoost was considered the Kaggle-Killer, being the winning model for most prediction problems. Recently Microsoft released their own gradient boosting framework called LightGBM, and it is way faster than XGB. In this post, I'm going to touch on the interesting portions of LightGBM.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;What is LightGBM?&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Similar to XGBoost, LightGBM is a gradient boosted tree based algorithm. Unlike other gradient boosted trees which grows hroizontally, LightGBM grows vertically. LightGBM grows Leaf-wise, while others grow Level-wise.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":227} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/1-AZsSoXb8lc5N6mnhqX5JCg-1.png){.wp-image-227}

&lt;figcaption&gt;
LightGBM leaf-wise growth. This allows for deeper vertical growth

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:image {"id":228} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/1-whSa8rY4sgFQj1rEcWr8Ag-1.png){.wp-image-228}

&lt;figcaption&gt;
Other gradient boosted algortihms grow level wise, which results in longer horizontal growth

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Dealing with Non-Numeric Data&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The nice thing about LightGBM is that it can take in data as a whole, and it does not require inputs to be converted into numerical format! This means that if your data has a mix of numbers and strings, you can simply throw everything into the model to learn.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The one thing you have to do however, is to specify the string columns as &lt;code&gt;category&lt;/code&gt;. Below is a example of how we do it with Pandas Dataframe&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;MachineIdentifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;ProductName&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;EngineVersion&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;AppVersion&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;AvSigVersion&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;category&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;IsBeta&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;int8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;RtpStateBitfield&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;IsSxsPassiveMode&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;int8&amp;#39;&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;df_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;train.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nrows&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Or if you're creating new features, you have to recast the datatype of the new column to categories&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;feature&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;newFeatures&lt;/span&gt;:
    &lt;span class="nv"&gt;Train&lt;/span&gt;[&lt;span class="nv"&gt;feature&lt;/span&gt; ] &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;Train&lt;/span&gt;[&lt;span class="nv"&gt;feature&lt;/span&gt; ].&lt;span class="nv"&gt;astype&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;category&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Important Parameters to Tune&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;LightGBM has a huge array of parameters to tune, and I wont be listing them here. I will however be highlighting those I think are important, and has helped me increase my model predictions&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;max_depth&lt;/code&gt;: Defines how deep the tree grows&lt;/li&gt;
&lt;li&gt;&lt;code&gt;num_leaves&lt;/code&gt;: Defines the maximum number of leaves in a node&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_bin&lt;/code&gt;: Defines the maximum number of bins your feature will be bucketed in&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For a more comprehensive read, &lt;a href="https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html"&gt;click here!&lt;/a&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In this short post, we've very briefly covered about LightGBM, how it is different from other gradient boosted machines, and how to define categories for training.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;An important thing to know is that LightGBM is very sensitive to overfitting, and should not be used for small data sets &amp;lt;10,000 rows.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content></entry><entry><title>Feature Engineering</title><link href="/feature-engineering.html" rel="alternate"></link><published>2019-03-17T09:41:00+00:00</published><updated>2019-03-17T09:41:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-03-17:/feature-engineering.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Feature Engineering is one of the neglected portion of machine learning. Most topics revolve around Model Training (parameter tuning, cross validation). While that might be really important, feature engineering is equally important as well, but I can't seem to find good resources that talk about this. I suspect this is …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Feature Engineering is one of the neglected portion of machine learning. Most topics revolve around Model Training (parameter tuning, cross validation). While that might be really important, feature engineering is equally important as well, but I can't seem to find good resources that talk about this. I suspect this is because to perform feature engineering, you need expert knowledge of the data, and what it represents.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A typical workflow would look something like this&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Project Scoping / Data Collection&lt;/li&gt;
&lt;li&gt;Exploratory Analysis (EDA)&lt;/li&gt;
&lt;li&gt;Data Cleaning&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feature Engineering&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Model Training&lt;/li&gt;
&lt;li&gt;Project Delivery / Insights&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;What is not Feature Engineering&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:list --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data cleaning (Outlier detection, Missing values)&lt;/li&gt;
&lt;li&gt;Scaling and Normalization&lt;/li&gt;
&lt;li&gt;Feature Selection&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;I would classify these as data massaging, as you're just changing the data (except for Feature Selection). Feature Engineering is the creation of new data.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;What is Feature Engineering&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;There are a few ways to create new features from existing ones&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;Indicator Variables&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Indicator variables are new variables that help you isolate data. This new feature is discriminative and can help separate the data.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Examples:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Threshold: If you're studying data on alcohol consumption, you could create a new binary feature if the person is &lt;code&gt;&amp;gt;=21&lt;/code&gt; years old. The expert knowledge in this is knowing where your data came from, and what is the minimum age of drinking in that country/state&lt;/li&gt;
&lt;li&gt;Special Events: If you're studying sales, there could be seasons that have higher sales, such as &lt;code&gt;isChristmas&lt;/code&gt;, &lt;code&gt;isSinglesDay&lt;/code&gt; or &lt;code&gt;isBlackFriday&lt;/code&gt;. Expert knowledge is knowing what special events there are&lt;/li&gt;
&lt;li&gt;Groupings: You can create artificial groups for the data, for example in network traffic, you can group the, according to protocols or source. Expert knowledge is knowing how to interpret the data, and what grouping makes sense&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;Interaction of Features&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Features can interact with each other to create new variables. Interaction here means some mathematical operation between them.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Examples:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Sum of Features: If you're looking at sales of individual items, a new feature might be &lt;code&gt;overallSales&lt;/code&gt;, where you add the sales of each item together&lt;/li&gt;
&lt;li&gt;Product of Features: If you're looking at wages, and you have features like &lt;code&gt;hourlyRate&lt;/code&gt;, and &lt;code&gt;workingHours&lt;/code&gt;, you can create a new feature called &lt;code&gt;totalPay&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The expert knowledge in these areas are knowing how the features interact with each other to produce new features. However, from unfortunate experience, I've seen some feature interactions that makes absolutely no sense, but the model seems to think otherwise. An example I saw was a new feature created from the multiplication of &lt;code&gt;screenHorizontalSize&lt;/code&gt; and &lt;code&gt;totalRAM&lt;/code&gt; which makes absolutely no sense, but it gave a boost in prediction accuracy. Machine Learning really is still a black box.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;Feature Representation&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For some features, you can better represent them in other formats that give more information.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Examples:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Date to integer: When give a &lt;code&gt;datetime&lt;/code&gt; format string, it almost always makes sense to decompose it to it's integer components such as &lt;code&gt;day&lt;/code&gt;, &lt;code&gt;month&lt;/code&gt; and &lt;code&gt;year&lt;/code&gt;. More than that, you can create features such as &lt;code&gt;isWeekday&lt;/code&gt; or &lt;code&gt;isPeakHour&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Sparse classes to Other: In a categorical class, if some classes are hugely under-represented, they can be grouped together, and classified as &lt;code&gt;Others&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;External Data Augmentation&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Another way to create new features is to bring in new data such as Geolocation information. These external data can be used to add in new features, which in turn can interact, represent or isolate current features.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Indicator Features, Feature Interactions, Feature Representation, External Data Augmentation are all several way to engineer new features. This is different from data massaging.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Feature Engineering is extremely important in your Machine Learning workflow.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="Feature Engineering"></category></entry><entry><title>Microsoft Kaggle Challenge: Adversarial Validation</title><link href="/microsoft-kaggle-challenge-adversarial-validation.html" rel="alternate"></link><published>2019-03-10T19:45:00+00:00</published><updated>2019-03-10T19:45:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-03-10:/microsoft-kaggle-challenge-adversarial-validation.html</id><summary type="html">&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Overview&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;This was a concept I came across while doing a Kaggle challenge issued by Microsoft to predict if a computer would get hit by a malware or not.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This challenge was different from their previous one, where they wanted you to predict if the malware class of a given …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Overview&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;This was a concept I came across while doing a Kaggle challenge issued by Microsoft to predict if a computer would get hit by a malware or not.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This challenge was different from their previous one, where they wanted you to predict if the malware class of a given binary.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This challenge was really interesting, because we had a lot of given information about the target machine, the list of them are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;code&gt;MachineIdentifier, ProductName, EngineVersion, AppVersion, AvSigVersion, IsBeta, RtpStateBitfield, IsSxsPassiveMode, DefaultBrowsersIdentifier, AVProductStatesIdentifier, AVProductsInstalled, AVProductsEnabled, HasTpm, CountryIdentifier, CityIdentifier', OrganizationIdentifier', GeoNameIdentifier', LocaleEnglishNameIdentifier', Platform', Processor', OsVer', OsBuild', OsSuite', OsPlatformSubRelease', OsBuildLab', SkuEdition', IsProtected', AutoSampleOptIn', PuaMode', SMode', IeVerIdentifier', SmartScreen', Firewall', UacLuaenable', Census_MDC2FormFactor', Census_DeviceFamily', Census_OEMNameIdentifier', Census_OEMModelIdentifier', Census_ProcessorCoreCount', Census_ProcessorManufacturerIdentifier', Census_ProcessorModelIdentifier', Census_ProcessorClass', Census_PrimaryDiskTotalCapacity', Census_PrimaryDiskTypeName', Census_SystemVolumeTotalCapacity', Census_HasOpticalDiskDrive', Census_TotalPhysicalRAM', Census_ChassisTypeName', Census_InternalPrimaryDiagonalDisplaySizeInInches', Census_InternalPrimaryDisplayResolutionHorizontal', Census_InternalPrimaryDisplayResolutionVertical', Census_PowerPlatformRoleName', Census_InternalBatteryType', Census_InternalBatteryNumberOfCharges', Census_OSVersion', Census_OSArchitecture', Census_OSBranch', Census_OSBuildNumber', Census_OSBuildRevision', Census_OSEdition', Census_OSSkuName', Census_OSInstallTypeName', Census_OSInstallLanguageIdentifier', Census_OSUILocaleIdentifier', Census_OSWUAutoUpdateOptionsName', Census_IsPortableOperatingSystem', Census_GenuineStateName', Census_ActivationChannel', Census_IsFlightingInternal', Census_IsFlightsDisabled', Census_FlightRing', Census_ThresholdOptIn', Census_FirmwareManufacturerIdentifier', Census_FirmwareVersionIdentifier', Census_IsSecureBootEnabled', Census_IsWIMBootEnabled', Census_IsVirtualDevice', Census_IsTouchEnabled', Census_IsPenCapable, Census_IsAlwaysOnAlwaysConnectedCapable, Wdft_IsGamer, Wdft_RegionIdentifier&lt;/code&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Now that's pretty crazy, with things like Screen size, is the PC build a gaming PC or not, and is touch screen enabled. With such complex features, it'll be really challenging to pick out patterns to predict if the machine will get hit or not.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;But one important thing that stood out was how different the test set was compared to the training set. This can be shown in the following way:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Random sample 10k rows from the training data set and the testing data set&lt;/li&gt;
&lt;li&gt;Drop the initial label (in this case, &lt;code&gt;HasDetected&lt;/code&gt;), and put in your own label &lt;code&gt;IsTrain&lt;/code&gt; , and give the training data 1, and the test data 0&lt;/li&gt;
&lt;li&gt;Build a classifier to predict if a given row came from the training data set, or the testing data set&lt;/li&gt;
&lt;li&gt;If the model gives a high accuracy, it means that the training and testing data are really different.&lt;/li&gt;
&lt;li&gt;If the model gives an accuracy of about 50%, it means that the training and testing data are almost the same&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The model I built was LightGBM, and it gave me an incredible score of 93%! That means that the training and testing data set are incredibly different.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This gives us some problems if we use the standard solution of cross validation of the training data, since at the end, the testing data is so disparate.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Adversarial Validation&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In comes adversarial validation. The idea of this actually really simple.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Based on the model we built earlier to classify is a row belongs to training or testing, we use the same model to run the classification only on the training data set.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We then pick out the rows that the model identified wrongly as testing data. This means that those rows inside the training data set have features that are similar to the testing data set!&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;And so, instead of the conventional validation methods, we use these rows classified falsely as testing to be our validation data. This way, our model validates itself to data that is close the testing data, and there would not have a huge difference in performance when training the model, and testing it.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Downside&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;One of the downsides of doing this is that, once the testing data set changes again to something that is dissimilar to the current data, we the adversarial validation technique would perform poorly again.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This might mean that we need to retrain the adversarial data selection model, and pick out falsely classified rows, and retrain the prediction model all over again. And that sounds like a lot of work.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="Adversarial Validation"></category></entry><entry><title>Convolutional Neural Networks</title><link href="/convolutional-neural-networks.html" rel="alternate"></link><published>2019-02-17T16:11:00+00:00</published><updated>2019-02-17T16:11:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-02-17:/convolutional-neural-networks.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Convolutional Neural Networks (CNN) are neural networks that are mainly used for image recognition and image classification.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this post, we'll break down how a CNN works under the hoods.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Backgroud&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;If we used a traditional neural network without any of the prior convolution steps, the network would not scale …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Convolutional Neural Networks (CNN) are neural networks that are mainly used for image recognition and image classification.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this post, we'll break down how a CNN works under the hoods.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Backgroud&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;If we used a traditional neural network without any of the prior convolution steps, the network would not scale well at all.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;28 x 28 pixel of MNIST in a fully connected model gives use 784 input weights. Obviously, most pictures are a lot larger than 28 x 28. A 200 x 200 pixel picture would result in 120,000 input weights.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;To minimize the number of input parameters, we need produce lower representations of the image that captures the most amount of information.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;CNN was inspired the visual cortex, where in the human brain, parts of the visual cortex fired when detecting edges. Furthermore, studies have shown that the visual cortex works in layers; a given layer works on the features detected in the previous layer, from lines, to contours, to shapes, to entire objects.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Vector Representation of Images&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;As we know, all models only take in numerical inputs to perform their actions. Non-numerical data such as text, and in this case images, must first be converted to numerical vectors.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:media-text {"mediaId":190,"mediaType":"image"} --&gt;

&lt;div class="wp-block-media-text alignwide"&gt;

&lt;figure class="wp-block-media-text__media"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/1_zy1qfb9affzz66yxxoi2aw1.gif){.wp-image-190}
&lt;/figure&gt;
&lt;div class="wp-block-media-text__content"&gt;

&lt;!-- wp:paragraph --&gt;
&lt;/p&gt;
An image with multiple colors can be converted into a grayscale image, and each pixel is represented by its intensity from a range of 0-255.

&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

This gives us a resulting numerical vector representation of an image

&lt;p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;!-- /wp:media-text --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Convolution?&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Before we talk about convolutional neural networks, we need to understand what is the meaning of convolution first.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In mathematical terms, a convolution is the combination of two functions to produce a third function, which has the properties of the two combined functions.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The term &lt;em&gt;convolution&lt;/em&gt; refers to the resulting third function, as well as the process of computing the combination of two functions.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:media-text {"mediaId":192,"mediaType":"image"} --&gt;

&lt;div class="wp-block-media-text alignwide"&gt;

&lt;figure class="wp-block-media-text__media"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/convolution_of_box_signal_with_itself2.gif){.wp-image-192}
&lt;/figure&gt;
&lt;div class="wp-block-media-text__content"&gt;

&lt;!-- wp:paragraph --&gt;
&lt;/p&gt;
By sliding function *g(t*) onto *f(t)*, we produce a third function *(f\*g)(t)*. We say that *(f\*g)(t)* is the convolution of *f(t)* and *g(t)*

&lt;p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;!-- /wp:media-text --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Now we have a rough idea of what convolution is, we can go back to see how convolution works in the context of image processing.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Convolution of Images&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;To begin, we have converted the image to a &lt;em&gt;n x n&lt;/em&gt; matrix of numbers from 0-255 which indicates the intensity.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Next, we take a smaller matrix of size &lt;em&gt;m x m&lt;/em&gt;, where &lt;em&gt;m &amp;lt; n&lt;/em&gt;, and slide it over the original matrix. This smaller matrix is called a filter.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:media-text {"mediaId":196,"mediaType":"image","mediaWidth":23} --&gt;

&lt;div class="wp-block-media-text alignwide" style="grid-template-columns:23% auto;"&gt;

&lt;figure class="wp-block-media-text__media"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/screen-shot-2016-07-24-at-11-25-13-pm2.png){.wp-image-196}
&lt;/figure&gt;
&lt;div class="wp-block-media-text__content"&gt;

&lt;!-- wp:paragraph --&gt;
&lt;/p&gt;
Image that has been converted to a matrix of numbers. For simplicity, we'll just use 0 and 1.

&lt;p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;!-- /wp:media-text --&gt;

&lt;!-- wp:media-text {"mediaId":197,"mediaType":"image","mediaWidth":15} --&gt;

&lt;div class="wp-block-media-text alignwide" style="grid-template-columns:15% auto;"&gt;

&lt;figure class="wp-block-media-text__media"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/screen-shot-2016-07-24-at-11-25-24-pm1.png){.wp-image-197}
&lt;/figure&gt;
&lt;div class="wp-block-media-text__content"&gt;

&lt;!-- wp:paragraph --&gt;
&lt;/p&gt;
A smaller matrix, called a filter, that we'll use to slide over the original matrix

&lt;p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;!-- /wp:media-text --&gt;

&lt;!-- wp:media-text {"mediaId":198,"mediaType":"image"} --&gt;

&lt;div class="wp-block-media-text alignwide"&gt;

&lt;figure class="wp-block-media-text__media"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/convolution_schematic.gif){.wp-image-198}
&lt;/figure&gt;
&lt;div class="wp-block-media-text__content"&gt;

&lt;!-- wp:paragraph --&gt;
&lt;/p&gt;
As we slide the filter over the matrix, we do a matrix multiplication, and take the result of the multiplication for our convolution matrix.

&lt;p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;!-- /wp:media-text --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The convolution here can be seen as combining the original matrix and the filter to produce a third matrix, which is our convolved feature matrix.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The intuition behind this is that we are using the filter to extract features from the image. Different filter values will extract out different features from the image.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph {"align":"center"} --&gt;

&lt;p&gt;&lt;img alt="" class="wp-image-200" src="https://chanjinhao.files.wordpress.com/2018/12/screen-shot-2016-08-05-at-11-03-00-pm.png"&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We can also use multiple filters to produce multiple Convoluted feature maps, which is called "Depth"&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":204,"align":"center","width":374,"height":188} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/screen-shot-2016-08-10-at-3-42-35-am.png){.wp-image-204 width="374" height="188"}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;When building a CNN, the model learns the values of the filters on its own, while we have to specify other parameters like number of filters, filter size, stride and zero-padding.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For a given set of values, convolution (which is a set of filters) generates a new set of values. The depth of the new set of output corresponds to the number of filters, as each filter generates its own set of values.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Removing Negative values from Convolved Features&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;After we produce a Convolved feature map from the original image, we perform another operation called ReLU (Rectified Linear Unit) on each element.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;What ReLU does is that it replaces all negative values to 0.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Why we need to apply ReLU on a convolved feature map is because the Convolution step is a linear operation. To account for non-linearity, we need to introduce a nonlinear function such as ReLU.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The resulting feature map after applying ReLU is called a Rectified feature map.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":201} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/screen-shot-2016-08-07-at-6-18-19-pm.png){.wp-image-201}

&lt;figcaption&gt;
Convoluted feature map becomes a Rectified feature map, after ReLU is applied to each pixel.  
This process changes all negative values to a 0 value

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Dimensionality Reduction through Pooling&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;After we have extracted the Convoluted feature map, and passed it through our ReLU function to produce a Rectified feature map, we can reduce the feature map through a process called pooling.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;There are 3 types of pooling: Max pooling, Sum pooling and Average pooling. We'll talk about Max pooling, because it works better in practice, and once you understand Max pooling, Sum pooling and Average pooling works the same way.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In doing Max pooling, we define yet another window size &lt;em&gt;k x k&lt;/em&gt;, but in this case, we do not slide the window across the Rectified feature map. Instead, we divide the feature map up into the window size, and take the max value from it.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":203} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/screen-shot-2016-08-10-at-3-38-39-am.png){.wp-image-203}

&lt;figcaption&gt;
A Max pooling window size of 2x2.  
After we pass the Convolved feature map through ReLU, we get a Rectified feature map.  
We take the maximum value of the window size to get the reduced matrix.  

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;The Fully Connected Layer&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;After we have broken down the image through iterative process of Convolution, ReLU and pooling, we get a set of matrices to represent the important features of the original image.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We then line up each of the values of the pooled matrix into a single vector, and feed it into a fully connected neural network.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;When the neural network does it's learning via gradient descent or some other optimization algorithm, only the weights in the neural network and the values in the filter layer changes. The size of the filter and step size do not change.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Features at each Layer&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;We now have the 3 basic steps of a CNN: Convolution, ReLU and Pooling.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We can repeat this step numerous times to reduce the image, and extract out important features.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":205,"align":"center","width":606,"height":144} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/screen-shot-2016-08-08-at-2-26-09-am.png){.wp-image-205 width="606" height="144"}  
&lt;figcaption&gt;
Repeated Convolution + ReLU and Pooling to reduce the image and extract important features.
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The more layers we have, the more complicated features we can extract out from the image. At each layer, we reconstruct simple layers to form more complex layers.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:media-text {"mediaId":206,"mediaType":"image"} --&gt;

&lt;div class="wp-block-media-text alignwide"&gt;

&lt;figure class="wp-block-media-text__media"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/screen-shot-2016-08-10-at-12-58-30-pm.png){.wp-image-206}
&lt;/figure&gt;
&lt;div class="wp-block-media-text__content"&gt;

&lt;!-- wp:paragraph {"align":"left"} --&gt;
&lt;/p&gt;
In the first layer, we pick out simple features like edges and lines.

&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;


In the second layer, we're able to form parts of the face such as eyes and ears.

&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;


In the last layer, we can form the full face from all the layers

&lt;p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;!-- /wp:media-text --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In another example, we can visually see how the CNN breaks down an image using Convolution + ReLU and pooling to extract important features, and make a classification at the end.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":207,"align":"center","width":748,"height":423} --&gt;

&lt;div class="wp-block-image"&gt;

&lt;figure class="aligncenter is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/conv_all.png){.wp-image-207 width="748" height="423"}
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The intuition here is that we are making predictions here based on several features maps. If we have feature maps telling us there is two eyes, a nose and a mouth, we can make a prediction that it is a face.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;We've seen in this post how to do the following steps in a CNN&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Transform an image to a numerical vector&lt;/li&gt;
&lt;li&gt;Apply a filter to extract a Convoluted feature map&lt;/li&gt;
&lt;li&gt;Apply ReLU to transform negative values to 0&lt;/li&gt;
&lt;li&gt;Apply Pooling to get your Rectified feature map&lt;/li&gt;
&lt;li&gt;Repeat until extract important features&lt;/li&gt;
&lt;li&gt;Pass them into a fully connected layer to perform prediction&lt;/li&gt;
&lt;li&gt;Learning only changes the weights of the connected layer and the filter matrix values&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;</content></entry><entry><title>Model Optimizers Beyond Gradient Descent in Deep Learning</title><link href="/model-optimizers-beyond-gradient-descent-in-deep-learning.html" rel="alternate"></link><published>2019-02-10T09:52:00+00:00</published><updated>2019-02-10T09:52:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-02-10:/model-optimizers-beyond-gradient-descent-in-deep-learning.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this post, we're going to talk about the draw backs and constrains of a simple Gradient Descent algorithm when applied to Deep Learning models, and also talk about other optimization algorithms that aim to solve those problems.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;These problems mainly arise due to the complex error surface in Deep …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this post, we're going to talk about the draw backs and constrains of a simple Gradient Descent algorithm when applied to Deep Learning models, and also talk about other optimization algorithms that aim to solve those problems.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;These problems mainly arise due to the complex error surface in Deep Learning models, where Gradient Descent is unable to perform as well.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Challenges with Gradient Descent&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:heading {"level":4} --&gt;&lt;/p&gt;
&lt;h4&gt;Too many Local Minimas&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;One of the problems that Gradient Descent faces is having a the algorithm converge to a local minima, instead of the true global minima. Even in a simple 2 dimensional problem, we face the issue, which gets even worse when our problem scales up to higher dimensions. But here we'll see that the local minima problem is not a huge issue with Deep Learning models.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The first source that contributes to a local minima is &lt;strong&gt;model identifiability&lt;/strong&gt;. An identifiable model is a model that given an output, the weights or the structure of the network can be identified. In other words, there is a one-to-one mapping of parameters to out. If a model is non-identifiable, it means that for a given output, there exists more than one set of parameters that can produce it. &lt;strong&gt;A fully connected feed-forward neural network is non-identifiable&lt;/strong&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":181,"width":435,"height":388} --&gt;

&lt;figure class="wp-block-image is-resized"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/multi-layer_neural_network-vector2.png){.wp-image-181 width="435" height="388"}

&lt;figcaption&gt;
Different paths and connections in the neural network may give the same output. This give rise to the characteristics of being non-identifiable.

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Why this is so is because there exists a huge number of different permutations of neuron connections within the model that will produce the same output. A network with &lt;em&gt;n&lt;/em&gt; neurons has &lt;em&gt;n!&lt;/em&gt; possible parameter combinations.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;So why is model non-identifiability not an issue with Deep Learning models? That is because, even though the models themselves are non-identifiable, they all have the same behaviors. So given a group of non-identifiable models, they will all react the same way to the same inputs. And because of this property, there exists only a single local minima for a given non-identifiable model.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;Spurious Local Minima&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Another problem that a local minima can give us is being spurious. Spurious means giving false information about itself, and a spurious local minima means that the local minima incurring a higher loss function value than the true local minima. In a sense, the local minima is lying to us, and presents itself as the global minima.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;However, there has been many studies that shows that the local minima actually exhibits similar properties to the global minima, and hence, this too isn't a problem.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Wrong Directions in Gradient Descent&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:heading {"level":4} --&gt;&lt;/p&gt;
&lt;h4&gt;Non-Uniform and Changing Gradients&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The actual challenge to Gradient Descent as we shall see, is not the problem of local minima, but finding the right path for the algorithm to descend towards.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Intuitively, the gradient is supposed to descend towards the steepest direction, or the direction that brings the gradient value closer to zero. However, just by using this simple heuristic alone can be problematic on complex error surfaces (which is a common property of Deep Learning Models).&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A complex error surface has the properties of uneven gradients, and hence when we move from point to point, the gradient underneath our path may possibly change. This is opposed to a simple error surface that is circular, where the gradient is constant throughout a single direction. Having this changing gradient may result in our algorithm going towards the wrong direction, because it doesn't account for the changes that happens as we are moving.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Mathematically, we can quantify how much the gradient changes as we are moving by calculating the second derivative. This can be captured by calculating how much the gradient as w2 changes as we change the value of w1, and we store this value in a Hessian Matrix. And a Hessian Matrix that tells us the gradient changes as we move, is called an &lt;strong&gt;ill-conditioned&lt;/strong&gt; matrix.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Calculating this Hessian matrix turns out to be extremely expensive if we do it at each step, and so to tackle the problem of changing gradients, we factor in the &lt;strong&gt;momentum&lt;/strong&gt; parameter.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Momentum-Based Optimization&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Earlier, we stated that we may go into the wrong direction because we don't account for changing gradients, and also, if we decide to account for changing gradients using second derivatives, calculating a Hessian Matrix is extremely expensive.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The solution to this is instead of calculating the Hessian Matrix at every optimization step, we factor in the value of the previous gradient into the calculation of the current gradient. By taking into account of the previous gradient value to find the current gradient, the fluctuations of gradient value is drastically reduced.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This approach of remembering previous gradients is called &lt;strong&gt;Momentum&lt;/strong&gt;. This technique is analogous to taking a moving average of stock prices the market  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":178} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2018/12/movingaverage.gif){.wp-image-178}

&lt;figcaption&gt;
Fluctuations in the stock market price are reduced by looking at the averag

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We can thinking of the wildly fluctuating gradients at each point being represented by the green line, while the average is represented by the yellow line. Momentum based optimizers use the yellow line to calculate the change in gradient.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;To conclude this post, we have seen how there are problems applying simple gradient descent to complex error surfaces that are present in Deep Learning models.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Local minimas are not a problem, but changing gradients due to its complex surface are a problem. To try to factor in changing surfaces, we could calculate the Hessian Matrix, but that turns out to be extremely expensive.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;As a solution, we use Momentum based optimizers instead, which factors in previous gradient values to the calculation of the current gradient.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="Optimizers"></category></entry><entry><title>Gradient Descent</title><link href="/gradient-descent.html" rel="alternate"></link><published>2019-02-03T20:10:00+00:00</published><updated>2019-02-03T20:10:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-02-03:/gradient-descent.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A machine learning model consists of weights, and those weights, given a set of inputs, are used in the calculation process to produce a prediction. The prediction is then fed into a loss function, to calculate the the total error. Using this error, we feed it into an optimization algorithm …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A machine learning model consists of weights, and those weights, given a set of inputs, are used in the calculation process to produce a prediction. The prediction is then fed into a loss function, to calculate the the total error. Using this error, we feed it into an optimization algorithm, which goes back to the model and tweaks the weights.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This tweaking process is the learning, and how we do the tweaking is optimization algorithm.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;There are a few popular ways to perform weight tweaking and model optimization, that is, how do we decide how to tweak the weights. In this post, we're going to be talking about Gradient Descent.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Gradient Descent&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In our model, assume we have 2 weights, w1 and w2, for optimization.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We can plot all possible weights w1 and w2 can have, to all possible errors on a graph, and this will produce an bowl shape, where the bottom of the bowl corresponds to the lowest possible error.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":165} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2018/11/gradient_descent_method.png){.wp-image-165}

&lt;figcaption&gt;
The X and Y axis represents the values of w1 and w2.  
The Z axis represents the values of the error.

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;What we're tweaking here are the values of  w1 and w2. When we start shifting the values around the X and Y axis, the point on the bowl shape also shifts correspondingly. The goal here is to tweak the values of w1 and w2 such that the point on the bowl rests directly at the bottom, which has the lowest error value.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;So how do we know, at each point of time on the bowl shape, where do we move to? Here's where we calculate the gradient of that point of the bowl. At the bottom of the bowl, the gradient will be 0, because it'll be a horizontal surface. All other points on the bowl will have a non-zero gradient value.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":166} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2018/11/512px-gradient_descent-svg.png){.wp-image-166}

&lt;figcaption&gt;
Top down view of the bowl. As we shift w1 and w2 around, the point on the bowl shifts as well. We want the point to slowly traverse towards the center, where error is minimized.

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The heuristic we'll use is to a new point such that the new gradient value on the bowl is getting smaller. This is the idea of descending gradient, until it reaches a minimum.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;One hyper-parameter we can tune in our optimization algorithm is how fast the point moves. That is to say, at each descent step, how far away should the new point be. A big step might get you to the bottom faster, but you might end up overshooting, and be perpetually oscillating around the bottom, never reaching the end. A small step on the other hand will take a much longer time. This distance of each descent step is called the &lt;strong&gt;learning rate&lt;/strong&gt;.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In mathematical terms, Gradient Descent is the partial derivative of the error or loss function, with respect to the weights.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Stochastic Gradient Descent&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The process we described above is the vanilla way of doing Gradient Descent, and it's called &lt;strong&gt;Batch Gradient Descent&lt;/strong&gt;. This means that we take all the possible data points in a single batch, and compute the error surface, or the bowl.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In reality, the error surface isn't always so smooth in the shape of a bowl, but may consist several saddle points. Saddle points are points on the graph that are almost horizontal, but it's not the true minimum of the graph. This can lead to an issue of early stoppage, where our Gradient Descent thinks it has found the lowest point on the error surface.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":167} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2018/11/saddle_point-svg.png){.wp-image-167}

&lt;figcaption&gt;
The point stops at a saddle point, which has a zero gradient as well. Clearly that is not the lowest point on the graph, and our algorithm has prematurely halted.  

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;One of the solutions to this is &lt;strong&gt;Stochastic Gradient Descent&lt;/strong&gt;. In batch, we take all possible data points and plot the error surface. In Stochastic, we only use one single data point, and we estimate the error surface. The result is a dynamic error surface, which decreases the chance of us encountering a saddle point.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The downside to Stochastic Gradient Descent is that we're performing an estimation of the error surface based only on 1 point, which may not be an accurate representation of the error surface.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;And the obvious solution to this is called &lt;strong&gt;Mini-Batch Gradient Descent&lt;/strong&gt;, where instead of performing the error surface estimation based on one point, we use a group of points, or mini batches.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;To recap, in this post we've talked about one of the optimization algorithm, Gradient Descent. This algorithm tells you how to tweak your weights to minimize the loss function of your model.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;There are three models, each improving on the other:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list {"ordered":true} --&gt;

&lt;ol&gt;
&lt;li&gt;Batch Gradient Descent: Plots the error surface based on all points. This might lead to early convergence on saddle points.&lt;/li&gt;
&lt;li&gt;Stochastic Gradient Descent: Estimates and plots the error surface based on a single point. This leads to a poor estimation of the error surface.&lt;/li&gt;
&lt;li&gt;Mini-batch Gradient Descent: Uses small batches of the data set to perform the error surface estimation.&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- /wp:list --&gt;</content><category term="Gradient Descent"></category></entry><entry><title>Learning in Machine Learning</title><link href="/learning-in-machine-learning.html" rel="alternate"></link><published>2019-01-27T17:14:00+00:00</published><updated>2019-01-27T17:14:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-01-27:/learning-in-machine-learning.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;When we talk about machine learning, it's mostly a black box, where everything is nicely wrapped in easy to call library functions.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Scipy, Numpy, Scikit-learn help us abstract all the nitty gritty details underlying machine learning&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this post, we're going to see where exactly the learning takes place, and …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;When we talk about machine learning, it's mostly a black box, where everything is nicely wrapped in easy to call library functions.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Scipy, Numpy, Scikit-learn help us abstract all the nitty gritty details underlying machine learning&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this post, we're going to see where exactly the learning takes place, and what happens when you "train" a model.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;The Steps of Learning&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In every algorithm, the learning process follows this formula:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Predict -&amp;gt; Evaluate -&amp;gt; Tune -&amp;gt; Repeat&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;When we first throw in a bunch of features, the model initially makes blind &lt;strong&gt;Predictions&lt;/strong&gt; as to what the outcome is. Because it makes shots in the dark, the &lt;strong&gt;Evaluation&lt;/strong&gt; of the model is going to be very poor initially. The model then learns of its errors, and &lt;strong&gt;Tunes&lt;/strong&gt; its hyper-parameters to minimize the errors. After tuning, it &lt;strong&gt;Repeats&lt;/strong&gt; the process of prediction, and the cycle continues until a satisfactory Error value is obtained.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;When training the model, the learning process comes from telling the machine where it went wrong, or the Errors it has committed. The Error is derived from the difference of the model output and the desired outcome.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;The Error/Loss Functions&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;When the model makes a prediction, there is bound to be errors in the the desired outcome, and the actual outcome. The difference between the desired and actual outcome can be represented in various ways called Loss Functions.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Some way of calculating this Error, or Loss Function, are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Classification Accuracy&lt;/li&gt;
&lt;li&gt;Log Loss&lt;/li&gt;
&lt;li&gt;Confusion Matrix  &lt;/li&gt;
&lt;li&gt;Root Mean Square Error (RMSE)&lt;/li&gt;
&lt;li&gt;F1 Score&lt;/li&gt;
&lt;li&gt;Area Under Curve (AUC)&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;These Loss functions tell the model how badly it has done in its job of prediction, and to kindly go back and tune the way it performs its predictions.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;The Optimization Functions&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;To tune the way it performs predictions, the model uses Optimization Functions.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Using the Error value produced by either one of those loss functions, the model then tunes itself using Optimization Functions, which adjusts its hyper-parameters, to try to minimize those Error values.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;There are also several ways for the model to tune it hyper-parameters based on the Error value computed. I'll only be listing them, as going through each of them requires a post on its own:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Gradient Descent&lt;/li&gt;
&lt;li&gt;Momentum&lt;/li&gt;
&lt;li&gt;Adaptive Movement Estimation (Adam)&lt;/li&gt;
&lt;li&gt;Adagrad&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;These Optimization algorithms are optimizing, or minimizing, the Error value calculated previously.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Repeat&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;So you got your Loss function to tell you how badly you did, and the Optimization function for your model to tweak it's parameters. Now all you have to do is to keep repeating these steps, and your model is "Learning". But wait!  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Over/Under Fitting&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Is there such a thing as learning too much? In the context of machine learning, this scenario is entirely possible, where you model learns too much about the training data, which results in poor performance on unseen data.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This is analogous to a student studying for his final exam, and the way he does it is to memorize every single questions and answers from the past year papers, with little contextual understanding. Obviously when he takes the final exam, the questions will be different, and he will do very poorly.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In machine learning, overfitting is a problem when we have over-tuned the parameters in the model to a specific data set, resulting in poor performance in other data sets.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Some ways to overcome Overfitting are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Throw in more data (akin to studying more past year papers)  &lt;/li&gt;
&lt;li&gt;Cross validation during training&lt;/li&gt;
&lt;li&gt;Early stopping to stop learning too much&lt;/li&gt;
&lt;li&gt;Regularization that forces simplicity on your model  &lt;/li&gt;
&lt;li&gt;Ensemble to take the average of various models  &lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Underfitting on the other, is not as common of a problem as overfitting. Underfitting means that your model has not learnt much, and as a result it cant perform well. This is analogous to student studying too little for his final exams.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In Overfitting, your model is too complex. In Underfitting, your model is too simple.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:separator --&gt;

&lt;hr&gt;
&lt;!-- /wp:separator --&gt;

&lt;p&gt;&lt;/p&gt;
&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;So that's it! You've understood the abstracted underling principles of what happens when a machine "Learns", and the possibility of learning too much or too little.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For each prediction, we get an error value, and using this error value, we use optimization functions to change the way we perform our prediction.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;You've also seen some ways to prevent overfitting, which is a more common problem than underfitting.  &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="Machine learning"></category></entry><entry><title>Word2Vec</title><link href="/word2vec-and-skip-gram.html" rel="alternate"></link><published>2019-01-20T14:24:00+00:00</published><updated>2019-01-20T14:24:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-01-20:/word2vec-and-skip-gram.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In the field of machine learning, when we're dealing with text processing, we can't just read in the strings of the sentence to train our model. The model requires numerical vectors, and word embedding is a way to convert your sentences into these vectors.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;There are various word embedding techniques …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In the field of machine learning, when we're dealing with text processing, we can't just read in the strings of the sentence to train our model. The model requires numerical vectors, and word embedding is a way to convert your sentences into these vectors.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;There are various word embedding techniques for converting strings into vectors. Some of the common ones are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Bag of Words (BoW)&lt;/li&gt;
&lt;li&gt;TF-IDF&lt;/li&gt;
&lt;li&gt;Word2Vec&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;I've briefly touched on BoW and TF-IDF in my previous posts. In this post, we're going to be looking at Word2Vec.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Difference between BoW&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Word2Vec is different from BoW, as BoW produces a single value for each word, which is the count of the word occurrence in the corpus. Word2Vec on the other hand, produces a vector representation for each word (as the name implies, word to vector)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Having a numerical vector tied to a single word has more benefits, as compared to a single count number. Some of the features are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Cosine similarity between the vectors can indicate semantic similarity&lt;/li&gt;
&lt;li&gt;The vectors produced for each word are fixed length, resulting in a low dimensional output (As compared to BoW, which results in a high dimensional and sparse vector)&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;As a result, it's much easier to perform machine learning related task to the condense Word2Vec representations of the word.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Generating the vectors&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;There are two methods for generating the vectors in Word2Vec:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Skip-gram model&lt;/li&gt;
&lt;li&gt;Continuous Bag of Words (CBoW)&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;Skip-gram Model&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A Skip-gram is like N-gram, but instead of consecutive words, it skips around the given window.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In the example below, the windows size is 2, which is to say 2 words before, and 2 words after the target word. The Skip-gram model then picks out all combinations of word-pairs within this window, not only consecutive ones (like in N-grams)&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":135} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2018/11/training_data.png){.wp-image-135}

&lt;figcaption&gt;
http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In the skip-gram model, we're going to train a neural network  with a single hidden layer to perform the following task: Given an input word, output the probabilities of each word being "close" to the input word. This closeness is defined in a window:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We're going to throw all these word pairs in our one layer neural network, and train our model to identify nearby words for a given input word. So, the higher the frequency a pair of words occur together, the model learns this co-occurrence, and is able to give a higher probability that the word exists together.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;For example, in our training set, if we feed it with many instances of the word-pair ("Apple", "Orange"), because they happen to be in many sentences such as "Apples and oranges", our model picks up this co-occurrence and gives "Orange" a higher probability. On the other hand, word-pairs like ("Apple", "Day"), which could occur in a sentence, "An apple a day keeps the doctor away" occur less frequently, and model gives "Day" a lower probability.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The catch here however, is that we're going to use the weights trained in the hidden layer of the neural network as our product, instead of the output itself. We want to use the hidden layer of the trained model to give each word a vector representation&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The single hidden layer will have N number of neurons. In this example, we're going to assume N = 300, because 300 neurons was what Google used to train their Word2Vec model.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Our model will look something like this&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":139} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2018/11/presentation11.jpg){.wp-image-139}

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In the training phase, one hot encoding is used for the input and outputs. During the validation phase, the inputs is a one hot encoding, while the output is a probability for each word indicating their "closeness"&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Once the model is trained, we're interested only in the hidden layer. The weight matrix would be of the size (Number of words X Number of neurons), and this is actually the word vector we're looking for.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:image {"id":138} --&gt;

&lt;figure class="wp-block-image"&gt;
![](https://chanjinhao.files.wordpress.com/2018/11/weightmatrix1.jpg){.wp-image-138}

&lt;figcaption&gt;
Word Vector for each word, generated from the hidden layer  

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;!-- /wp:image --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The feature of this word vector generated from the weight matrix is that, for similar words, their vectors would be "close" to each other (Cosine distance). This is because of the way we used word-pairs to train the model.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Continuous Bag of Words&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A CBoW is just a Skip-gram reversed.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The input to a CBoW is a group of context words, and the output of the model tries to predict a single word that fits into the context of all the input words.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;CBoW represents the data differently&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Hi fred how was the pizza?&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;grams&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Hi fred how&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;fred how was&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;how was the&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;Skip&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;gram&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;skip&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;grams&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Hi fred how&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Hi fred was&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;fred how was&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;fred how the&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;or more intuitively, &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;CBOW&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="n"&gt;ate&lt;/span&gt; &lt;span class="n"&gt;_____&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; 
&lt;span class="n"&gt;Predict&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;given&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt; &lt;span class="k"&gt;case&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;food&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;Skip&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;gram&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;___&lt;/span&gt; &lt;span class="n"&gt;___&lt;/span&gt; &lt;span class="n"&gt;___&lt;/span&gt; &lt;span class="n"&gt;food&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Given&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="n"&gt;what&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;what&lt;/span&gt; &lt;span class="n"&gt;was&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt; &lt;span class="n"&gt;around&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt; &lt;span class="k"&gt;case&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="n"&gt;ate&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;</content><category term="Text processing"></category></entry><entry><title>Text Processing</title><link href="/text-processing.html" rel="alternate"></link><published>2019-01-13T18:41:00+00:00</published><updated>2019-01-13T18:41:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2019-01-13:/text-processing.html</id><summary type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this post, we're going to be exploring some typical methods for text processing for machine learning. When we're talking about machine learning with text, there are several areas of interest including &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Sentiment Analysis&lt;/li&gt;
&lt;li&gt;Question Answering&lt;/li&gt;
&lt;li&gt;Information Retrieval&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Before we do that, we must first understand that a machine learning …&lt;/p&gt;</summary><content type="html">&lt;!-- wp:paragraph --&gt;

&lt;p&gt;In this post, we're going to be exploring some typical methods for text processing for machine learning. When we're talking about machine learning with text, there are several areas of interest including &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Sentiment Analysis&lt;/li&gt;
&lt;li&gt;Question Answering&lt;/li&gt;
&lt;li&gt;Information Retrieval&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Before we do that, we must first understand that a machine learning model only takes in numerical values, or vectors, and not strings in the text. The problem now is how do we transform the collection of strings into vectors of numbers.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;There are several pre-processing steps, and we'll take a look at them below.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Tokenization&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Tokenization is splitting up the sentence into to words or phrases.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;There's Sentence Tokenizing, and Word Tokenizing, both of which are apparent in what they do. We'll mostly be using Word Tokenizing to split up a sentence in its constituent words. The example below uses NLTK's word_tokenize&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;code&gt;from nltk.tokenize import word_tokenize &amp;gt;&amp;gt;&amp;gt; string = "Hello! I am a sentence!" &amp;gt;&amp;gt;&amp;gt; word_tokenize(string ) ['Hello', '!', 'I', 'am', 'a', 'sentence', '!']&lt;/code&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Normalization&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Once the sentence has been broken up into it's words, we need to normalize it, so as to remove any unwanted meaning attached to features like capitalization. This process transforms the words, and picks out useful features.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;There are several common methods for normalization:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Lemmatization&lt;/li&gt;
&lt;li&gt;Stemming&lt;/li&gt;
&lt;li&gt;Capitalization&lt;/li&gt;
&lt;li&gt;Special Characters&lt;/li&gt;
&lt;li&gt;Stopwords&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Lemmatization and Stemming are pretty similar, where they both transform the words into their generalized forms. The difference is in how they change the word.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:table --&gt;

&lt;hr&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;         &lt;span class="n"&gt;Lemmatization&lt;/span&gt;   &lt;span class="n"&gt;Stemming&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Studying   Study           Study
  Studies    Study           Studi&lt;/p&gt;
&lt;hr&gt;
&lt;!-- /wp:table --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Stemming removes any suffixes, leaving behind it's inflected word. The outcome is not always desirable as you can see, cutting the -es from Studies. One common stemmer is the Porter stemmer, which reduces the words to its 'root' form.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Lemmatization on the other hand is smarter, and uses linguistics to reduce the word to it's base meaning. 'Studies' and 'Studying' both have the same base meaning of 'Study'. However, before you can apply lemmatization, you need to have a trained dictionary for that language to discover what is the base meaning. Luckily for us, the English language has many of such dictionaries.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Capitalization and Special Characters transformation is simply turning all the words into lowercase, and removing non-alphabet characters&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; sentence = sentence.lower() &amp;gt;&amp;gt;&amp;gt; sentence = re.sub('[^a-zA-Z]',' ',sentence)&lt;/code&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Stopwords removal is a method for removing common stopwords in a text. Stopwords carry little to no meaning to them, and are sentimentally agnostic, hence they should be removed so as not to generate too much noise in our matrix. A list of common stopwords can be found in &lt;a href="https://gist.github.com/sebleier/554280"&gt;NLTK's collection&lt;/a&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Transforming Tokens to Vectors&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;Bag of Words&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Once we have our collection of pre-processed tokens, we now need to transform them into features, or numeric vectors for us to fit into our model&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A Bag of Words (BoW) model is one way to quantize the text into numerical information. It is also called Text Vectorization, because we're converting a sentence into a numerical vector.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;BoW captures the counts of the words in a sentence &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;John&lt;/span&gt; &lt;span class="n"&gt;likes&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;watch&lt;/span&gt; &lt;span class="n"&gt;movies&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Mary&lt;/span&gt; &lt;span class="n"&gt;likes&lt;/span&gt; &lt;span class="n"&gt;movies&lt;/span&gt; &lt;span class="n"&gt;too&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;

&lt;span class="ss"&gt;&amp;quot;John&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;likes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;to&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;watch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;movies&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Mary&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;likes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;movies&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;too&amp;quot;&lt;/span&gt;

&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;John&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;likes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;to&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;watch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;movies&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Mary&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;too&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The downside of BoW model is that we lose word order, which is important when it comes to sentiment analysis. The ordering of the words in a sentence can produce very different meanings&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;"not all apples are bad"&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;"all apples are not bad"&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The former implies that not every single apple is bad, but there can be bad ones. The latter implies that every single apple is not bad, which means there are no bad ones.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Also, BoW counts are not normalized, which loses another feature of word importance. Words that occur very frequently such as stopwords hold little weight if they appear multiple times, and in every document. We want words that are rare, and occur less frequently. These words will have stronger features.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;TF-IDF&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;After we have the collection of words generated from BoW, we can count the frequency of the word, and the presence of the word in a given document. This  technique is called Term-Frequency - Inverse Document Frequency (TF-IDF) model.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;This is calculated by counting the number of times the word appears in all documents (TF), and the number of documents this word appears in (DF). We take the inverse of DF (IDF), because we don't want words that appear too frequently in all documents. &lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;A word with a high TF-IDF indicates a high term frequency, low document count. This highlights important issues in a document, but that are not shared across the whole corpus&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:heading {"level":4} --&gt;

&lt;h4&gt;N-Grams&lt;/h4&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;The BoW model grows linearly with each distinct vocabulary. With every new word added, the vector size increases by 1. This leads to an extremely spares and high dimension vector. To attempt to reduce the dimensions, we group words together into what we call N-grams, where N is the number of words in the group.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;N-grams are an improvement because it reduces the dimensionality of the vector, and it also captures context from the surrounding words.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Below shows a 2-gram representation of a sentence:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:code --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;John&lt;/span&gt; &lt;span class="n"&gt;likes&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;watch&lt;/span&gt; &lt;span class="n"&gt;movies&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Mary&lt;/span&gt; &lt;span class="n"&gt;likes&lt;/span&gt; &lt;span class="n"&gt;movies&lt;/span&gt; &lt;span class="n"&gt;too&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;

&lt;span class="ss"&gt;&amp;quot;John likes&amp;quot;&lt;/span&gt;
&lt;span class="ss"&gt;&amp;quot;likes to&amp;quot;&lt;/span&gt;
&lt;span class="ss"&gt;&amp;quot;to watch&amp;quot;&lt;/span&gt;
&lt;span class="ss"&gt;&amp;quot;watch movies&amp;quot;&lt;/span&gt;
&lt;span class="ss"&gt;&amp;quot;movies Mary&amp;quot;&lt;/span&gt;
&lt;span class="ss"&gt;&amp;quot;Mary likes&amp;quot;&lt;/span&gt;
&lt;span class="ss"&gt;&amp;quot;likes movies&amp;quot;&lt;/span&gt;
&lt;span class="ss"&gt;&amp;quot;movies too&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- /wp:code --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;We should be careful however, in choosing the appropriate value of N. If we generate too much N-grams (N is small), we end up generating too much noise.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;After the N-gram collection is generated, we can future refine the selection with the heuristic:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;

&lt;ul&gt;
&lt;li&gt;Remove high and low frequency n-grams&lt;/li&gt;
&lt;li&gt;High Frequency n-grams = Stop words&lt;/li&gt;
&lt;li&gt;Low Frequency n-grams = Rare words&lt;/li&gt;
&lt;li&gt;Keep medium frequency n-grams&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:heading {"level":3} --&gt;

&lt;h3&gt;Training a Model with the Vectors&lt;/h3&gt;
&lt;!-- /wp:heading --&gt;

&lt;!-- wp:paragraph --&gt;

&lt;p&gt;Once we have pre-processed the sentences to tokens, and vectorized them into numerical values, we can use those vectors to train our models to answer our questions.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;</content><category term="Text processing"></category></entry><entry><title>Kaggle Boiler Plate</title><link href="/kaggle-boiler-plate.html" rel="alternate"></link><published>2018-12-16T21:33:00+00:00</published><updated>2018-12-16T21:33:00+00:00</updated><author><name>jinhaochan</name></author><id>tag:None,2018-12-16:/kaggle-boiler-plate.html</id><summary type="html">&lt;p&gt;I've been playing around with Kaggle competitions for a while, and there are usually quite a few steps to perform.&lt;/p&gt;
&lt;p&gt;I've compiled a list of them below, in sequential order. These are by no means hard and fast rules, but simple heuristics to follow!&lt;/p&gt;
&lt;p&gt;I've added links here and there …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been playing around with Kaggle competitions for a while, and there are usually quite a few steps to perform.&lt;/p&gt;
&lt;p&gt;I've compiled a list of them below, in sequential order. These are by no means hard and fast rules, but simple heuristics to follow!&lt;/p&gt;
&lt;p&gt;I've added links here and there to guide you a long.&lt;/p&gt;
&lt;h1&gt;Importing your libraries&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;You're gonna have to import the usual &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;sklearn&lt;/code&gt; to do your dataframe manipulations, and machine learning stuff.&lt;/p&gt;
&lt;p&gt;Aside from those, you'll likely be importing other stuff that are relevant in transforming your data.&lt;/p&gt;
&lt;h1&gt;Reading in training data and testing data&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;When you first start the project, the first thing you want to do is to read in the data. It's usually named &lt;code&gt;train.csv&lt;/code&gt; and probably contains a few million lines.&lt;/p&gt;
&lt;p&gt;You most probably won't be able to read in all the data at once, so you're gonna have to read in just a few lines to get a preview.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;df_train = pd.read_csv("train.csv", nrows=1000000)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Of course when you're doing the actual training of the model, you're going to have to read in the whole thing!&lt;/p&gt;
&lt;p&gt;We also read in the test set, usually called &lt;code&gt;test.csv&lt;/code&gt;. The reason why we're reading in the test set, is so that when we perform feature creation and data massaging, we can do it both on the test and train data.&lt;/p&gt;
&lt;h1&gt;Visualizing data&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;You're going to want to visualize the data you've read in to analyze for any outliers, or obvious trends that can be helpful in feature creation.&lt;/p&gt;
&lt;p&gt;For univariate analysis, I usually apply barchart, or histogram, while for bivariate analysis, I'll apply a scatter plot.&lt;/p&gt;
&lt;h3&gt;Univariate Analysis&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/residentmario/univariate-plotting-with-pandas"&gt;https://www.kaggle.com/residentmario/univariate-plotting-with-pandas&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Bivariate Analysis&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/residentmario/bivariate-plotting-with-pandas"&gt;https://www.kaggle.com/residentmario/bivariate-plotting-with-pandas&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Cleaning data&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;After visualizing your data, you'll more or less know the upper or lower bounds, outliers, and whats considered to be normal.&lt;/p&gt;
&lt;p&gt;You must now remove those values that lie outside those normal ranges. There are a common data abnormalities which are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Outliers&lt;/li&gt;
&lt;li&gt;Missing values&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Some examples are: Taxi fares with negative values, coordinates that plot on the ocean, Null or NaN values, and many more.&lt;/p&gt;
&lt;p&gt;You want to be cleaning your data BEFORE training your model. If not, there will be unnecessary noise. You'll end up with a few lesser rows than your original training set.&lt;/p&gt;
&lt;p&gt;However, DO NOT CLEAN YOUR TESTING SET. The testing set is supposed to be untouched, aside from feature engineering.&lt;/p&gt;
&lt;h1&gt;Feature Engineering&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;After looking at your data, you will definitely need to engineer some features on your own. Doing this allows you to find features that correlated more strongly with the value you're predicting.&lt;/p&gt;
&lt;p&gt;We have 2 types of data type: Continuous and Discrete, and each of the data types have to be handled differently when doing feature engineering&lt;/p&gt;
&lt;p&gt;Continuous data runs in infinite ranges, while Discrete data are things that fall into categories.&lt;/p&gt;
&lt;p&gt;Example of Continuous data are prices, age and temperature. Strings are also considered Continuous data&lt;/p&gt;
&lt;p&gt;Examples of Discrete data are gender and types of cars.&lt;/p&gt;
&lt;h3&gt;Continuous Data Feature Engineering&lt;/h3&gt;
&lt;p&gt;For continuous data, we can bin the data into intervals.&lt;/p&gt;
&lt;p&gt;An example would be age group, where individual ages might be too scattered, but by grouping them in multiples of 5s or 10s, you might get a better representation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;text&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="mi"&gt;51&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;55&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;Group&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; 
&lt;span class="mi"&gt;56&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;Group&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; 
&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;65&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;Group&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; 
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;55&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;56&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;59&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;62&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For strings, we need to extract out relevant data that can be represented in numeric form. One example is parsing of the dates. In your original data, you're given a datetime string, which isn't helpful at all. You'll want to engineer features such as the day of the week, the hour, month, year, or even the seconds. These numerical features are much more helpful as compared to a string value.&lt;/p&gt;
&lt;h3&gt;Discrete Data Feature Engineering&lt;/h3&gt;
&lt;p&gt;For discrete data, the categories in the data can be one-hot-encoded. The reason why we do that is because when we change the categories to numeric values, we don't want to accidentally imply meaning and hierarchy between the numbers.&lt;/p&gt;
&lt;p&gt;For example if we have 5 different categories of cars, and we change them numerically to 0, 1, 2, 3, 4, the machine may end up learning that the 4th category is more important than the 0th category, based on the simple fact that 4 is greater than 0.&lt;/p&gt;
&lt;p&gt;So to prevent this problem of false importance, we use one-hot-encoding. The idea of one-hot-encoding can be visually represented as such&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="alignnone wp-image-170" height="220" src="https://chanjinhao.files.wordpress.com/2018/12/mtimfxh.png?w=300" width="560"&gt;&lt;/p&gt;
&lt;p&gt;This way, the categories are represented as 1s and 0s, which minimizes the possibility of learning false importance.&lt;/p&gt;
&lt;p&gt;We should take note that one-hot-encoding should be done on your train and test set combined. The reason why we want to do this is so we don't miss out data that is in the test and not in the train, vice-versa. If there is missing data, and we perform one-hot-encoding separately on the train and test, we will end up with missing columns, as one-hot-encoding does not generate them.&lt;/p&gt;
&lt;h3&gt;Feature Interaction&lt;/h3&gt;
&lt;p&gt;There is also Feature interaction, where two or more features are correlated or have interactions between each other. We can capture this interaction between two features by creating a new feature, which is a multiplication of these two correlated features.&lt;/p&gt;
&lt;h1&gt;Splitting of Data&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;Once you've cleaned your data and created your features, you can now start training your model! But before you do that, you first need to split your data in a train and test set. This is for performing a validation test to evaluate your model.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;X_train, X_test, y_train, y_test = train_test_split(df_train, y, test_size=0.2)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;code&gt;y&lt;/code&gt; is your target value to predict.&lt;/p&gt;
&lt;p&gt;The way you use these values are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;y_train&lt;/code&gt; for training the model&lt;/li&gt;
&lt;li&gt;Feed &lt;code&gt;X_test&lt;/code&gt; to your model&lt;/li&gt;
&lt;li&gt;Evaluate the output with &lt;code&gt;y_test&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Scaling Data&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;Because not all of your data will be in the same scale, we have to normalize them all to be of the same scale.&lt;/p&gt;
&lt;p&gt;For example, the scale for age can range from 0-90, while a pay range can go from 2000 - 10,000. This is bad for machine learning, because the model might attribute a hidden (but wrong) meaning to this difference in range.&lt;/p&gt;
&lt;p&gt;0 - 90 has a small range, while 2000 - 10,000 has a larger range.&lt;/p&gt;
&lt;p&gt;How we scale this is by using sklearn packages such as MinMax scaling.&lt;/p&gt;
&lt;p&gt;A potential problem to scaling is having data leakage, where we learn some attributes from the testing data set into the training dataset.&lt;/p&gt;
&lt;p&gt;How we overcome the problem of data leakage is to perform fit-transform on the training set, and only perform transform on your testing set&lt;/p&gt;
&lt;h1&gt;Training&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;You got to first identify if you're solving a classification or a regression problem, and a supervised or unsupervised problem.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sklearn&lt;/code&gt; provides a wide range of models for you to pick from.&lt;/p&gt;
&lt;p&gt;Models for Supervised learning in &lt;code&gt;sklearn&lt;/code&gt;: &lt;a href="http://scikit-learn.org/stable/supervised_learning.html"&gt;http://scikit-learn.org/stable/supervised_learning.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Models for Unsupervised learning in &lt;code&gt;sklearn&lt;/code&gt;: &lt;a href="http://scikit-learn.org/stable/unsupervised_learning.html"&gt;http://scikit-learn.org/stable/unsupervised_learning.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There's an overpowered model right now called XGBoost, but I highly recommend using it AFTER you've played around with other models. This is to allow you to have an understanding of how other models work, because XGBoost is definitely not a silver bullet.&lt;/p&gt;
&lt;p&gt;Installing XGBoost is a little bit tricky, because its an external library.&lt;/p&gt;
&lt;p&gt;If you're using Windows, these are the steps I followed: &lt;a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en"&gt;https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And the Linux version:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[code lang=&amp;quot;text&amp;quot;]&lt;/span&gt;
&lt;span class="na"&gt;$ pip install xgboost&lt;/span&gt;
&lt;span class="na"&gt;$ git clone https://github.com/dmlc/xgboost# cd xgboost-master&lt;/span&gt;
&lt;span class="na"&gt;$ make&lt;/span&gt;
&lt;span class="na"&gt;$ cd python-package/&lt;/span&gt;
&lt;span class="na"&gt;$ python setup.py install&lt;/span&gt;
&lt;span class="k"&gt;[/code]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can now &lt;code&gt;import xgboost&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;See how much easier it is on Linux.&lt;/p&gt;
&lt;h1&gt;Validation&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;Once your model is trained, you would need to validate the output with &lt;code&gt;y_test&lt;/code&gt;. &lt;code&gt;y_test&lt;/code&gt; contains the true values, while your model outputs a set of predicted values.&lt;/p&gt;
&lt;p&gt;Again, &lt;code&gt;sklearn&lt;/code&gt; provides a suite of tools for performing evaluation, depending on what model you were using: &lt;a href="http://scikit-learn.org/stable/modules/model_evaluation.html"&gt;http://scikit-learn.org/stable/modules/model_evaluation.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you get a bad score here, you'll want to revisit your feature engineering, or data cleaning again to see what you can do differently. Remember, more features != better model!&lt;/p&gt;
&lt;h1&gt;Prediction&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;Pump in the testing set into your model you trained, and get a set of output values. There'll be no evaluation on your side here. Evaluation will be done by Kaggle once you submit them. This is essentially your answer to their problem.&lt;/p&gt;
&lt;h1&gt;Writing to CSV&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;Kaggle usually provides a file called &lt;code&gt;sample_submission.csv&lt;/code&gt; to show you the format with the competition requires for submission.&lt;/p&gt;
&lt;p&gt;Transform your answers to fit into that model, then write the answers to CSV for submission&lt;/p&gt;
&lt;p&gt;&lt;code&gt;​​​​​df.to_csv("my_submission.csv", index=False)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;I notice that you'll want to include &lt;code&gt;index=False&lt;/code&gt; to exclude the row numbers in the dataframe&lt;/p&gt;
&lt;h1&gt;Submit Your Entry&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;That's it! Go on and submit your entry to Kaggle, and see how you rank against other Kagglers. Don't be disheartened if you didn't perform well, it takes a few iterations to improve your model.&lt;/p&gt;
&lt;p&gt;Also, don't be afraid to read up on other people's kernels to gain inspiration!&lt;/p&gt;
&lt;!-- wp:image --&gt;

&lt;figure class="wp-block-image"&gt;
&lt;img alt&gt;&lt;/img&gt;

&lt;/figure&gt;

&lt;!-- /wp:image --&gt;</content></entry></feed>