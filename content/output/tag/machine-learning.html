<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>A Pelican Blog - Machine learning</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="A Pelican Blog Atom Feed" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">A Pelican Blog </a></h1>
                <nav><ul>
                    <li><a href="/category/book-review.html">Book Review</a></li>
                    <li><a href="/category/data-science.html">Data Science</a></li>
                    <li><a href="/category/misc.html">misc</a></li>
                    <li><a href="/category/ramblings.html">Ramblings</a></li>
                    <li><a href="/category/review.html">Review</a></li>
                    <li><a href="/category/security.html">Security</a></li>
                    <li><a href="/category/software-engineering.html">Software Engineering</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/learning-in-machine-learning.html">Learning in Machine Learning</a></h1>
<footer class="post-info">
        <abbr class="published" title="2019-01-27T17:14:00+00:00">
                Published: Sun 27 January 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/machine-learning.html">Machine learning</a> </p>
</footer><!-- /.post-info --><p>When we talk about machine learning, it's mostly a black box, where everything is nicely wrapped in easy to call library functions.</p>
<p>Scipy, Numpy, Scikit-learn help us abstract all the nitty gritty details underlying machine learning</p>
<p>In this post, we're going to see where exactly the learning takes place, and what happens when you "train" a model.  </p>
<!-- wp:heading {"level":3} -->

<h3>The Steps of Learning</h3>
<hr>
<p>In every algorithm, the learning process follows this formula:</p>
<p>Predict -&gt; Evaluate -&gt; Tune -&gt; Repeat</p>
<p>When we first throw in a bunch of features, the model initially makes blind <strong>Predictions</strong> as to what the outcome is. Because it makes shots in the dark, the <strong>Evaluation</strong> of the model is going to be very poor initially. The model then learns of its errors, and <strong>Tunes</strong> its hyper-parameters to minimize the errors. After tuning, it <strong>Repeats</strong> the process of prediction, and the cycle continues until a satisfactory Error value is obtained.  </p>
<p>When training the model, the learning process comes from telling the machine where it went wrong, or the Errors it has committed. The Error is derived from the difference of the model output and the desired outcome.</p>
<!-- wp:heading {"level":3} -->

<h3>The Error/Loss Functions</h3>
<hr>
<p>When the model makes a prediction, there is bound to be errors in the the desired outcome, and the actual outcome. The difference between the desired and actual outcome can be represented in various ways called Loss Functions.  </p>
<p>Some way of calculating this Error, or Loss Function, are:</p>
<ul>
<li>Classification Accuracy</li>
<li>Log Loss</li>
<li>Confusion Matrix  </li>
<li>Root Mean Square Error (RMSE)</li>
<li>F1 Score</li>
<li>Area Under Curve (AUC)</li>
</ul>
<p>These Loss functions tell the model how badly it has done in its job of prediction, and to kindly go back and tune the way it performs its predictions.  </p>
<!-- wp:heading {"level":3} -->

<h3>The Optimization Functions</h3>
<hr>
<p>To tune the way it performs predictions, the model uses Optimization Functions.  </p>
<p>Using the Error value produced by either one of those loss functions, the model then tunes itself using Optimization Functions, which adjusts its hyper-parameters, to try to minimize those Error values.</p>
<p>There are also several ways for the model to tune it hyper-parameters based on the Error value computed. I'll only be listing them, as going through each of them requires a post on its own:</p>
<ul>
<li>Gradient Descent</li>
<li>Momentum</li>
<li>Adaptive Movement Estimation (Adam)</li>
<li>Adagrad</li>
</ul>
<p>These Optimization algorithms are optimizing, or minimizing, the Error value calculated previously.</p>
<!-- wp:heading {"level":3} -->

<h3>Repeat</h3>
<hr>
<p>So you got your Loss function to tell you how badly you did, and the Optimization function for your model to tweak it's parameters. Now all you have to do is to keep repeating these steps, and your model is "Learning". But wait!  </p>
<!-- wp:heading {"level":3} -->

<h3>Over/Under Fitting</h3>
<hr>
<p>Is there such a thing as learning too much? In the context of machine learning, this scenario is entirely possible, where you model learns too much about the training data, which results in poor performance on unseen data.</p>
<p>This is analogous to a student studying for his final exam, and the way he does it is to memorize every single questions and answers from the past year papers, with little contextual understanding. Obviously when he takes the final exam, the questions will be different, and he will do very poorly.  </p>
<p>In machine learning, overfitting is a problem when we have over-tuned the parameters in the model to a specific data set, resulting in poor performance in other data sets.</p>
<p>Some ways to overcome Overfitting are:</p>
<ul>
<li>Throw in more data (akin to studying more past year papers)  </li>
<li>Cross validation during training</li>
<li>Early stopping to stop learning too much</li>
<li>Regularization that forces simplicity on your model  </li>
<li>Ensemble to take the average of various models  </li>
</ul>
<p>Underfitting on the other, is not as common of a problem as overfitting. Underfitting means that your model has not learnt much, and as a result it cant perform well. This is analogous to student studying too little for his final exams.</p>
<p>In Overfitting, your model is too complex. In Underfitting, your model is too simple.</p>
<!-- wp:heading {"level":3} -->

<h3>Conclusion</h3>
<hr>
<p>So that's it! You've understood the abstracted underling principles of what happens when a machine "Learns", and the possibility of learning too much or too little.</p>
<p>For each prediction, we get an error value, and using this error value, we use optimization functions to change the way we perform our prediction.  </p>
<p>You've also seen some ways to prevent overfitting, which is a more common problem than underfitting.  </p>                </article>
            </aside><!-- /#featured -->
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>