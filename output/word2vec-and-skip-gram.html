
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/font-awesome.min.css">







<meta name="author" content="jinhaochan" />
<meta name="description" content="In the field of machine learning, when we&#39;re dealing with text processing, we can&#39;t just read in the strings of the sentence to train our model. The model requires numerical vectors, and word embedding is a way to convert your sentences into these vectors. There are various word embedding techniques …" />
<meta name="keywords" content="Text processing">

<meta property="og:site_name" content="glob"/>
<meta property="og:title" content="Word2Vec"/>
<meta property="og:description" content="In the field of machine learning, when we&#39;re dealing with text processing, we can&#39;t just read in the strings of the sentence to train our model. The model requires numerical vectors, and word embedding is a way to convert your sentences into these vectors. There are various word embedding techniques …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/word2vec-and-skip-gram.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-01-20 14:24:00+08:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="/author/jinhaochan.html">
<meta property="article:section" content="Data Science"/>
<meta property="article:tag" content="Text processing"/>
<meta property="og:image" content="">

  <title>glob &ndash; Word2Vec</title>

</head>
<body>
  <aside>
    <div>
      <a href="">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>


      <nav>
        <ul class="list">

          <li><a href="/">All</a></li>
          <li><a href="/category/data-science.html">Data Science</a></li>
          <li><a href="/category/security.html">Cyber Security</a></li>
          <li><a href="/category/software-engineering.html">Software Engineering</a></li>
          <li><a href="/category/review.html">Book Reviews</a></li>
          <li><a href="/category/ramblings.html">Ramblings</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/jinhao-hao-chan-162630120/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-github" href="https://www.github.com/jinhaochan" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="word2vec-and-skip-gram">Word2Vec</h1>
    <p>
          Posted on Sun 20 January 2019 in <a href="/category/data-science.html">Data Science</a>


    </p>
  </header>


  <div>
    <!-- wp:paragraph -->

<p>In the field of machine learning, when we're dealing with text processing, we can't just read in the strings of the sentence to train our model. The model requires numerical vectors, and word embedding is a way to convert your sentences into these vectors.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>There are various word embedding techniques for converting strings into vectors. Some of the common ones are:</p>
<!-- /wp:paragraph -->

<!-- wp:list -->

<ul>
<li>Bag of Words (BoW)</li>
<li>TF-IDF</li>
<li>Word2Vec</li>
</ul>
<!-- /wp:list -->

<!-- wp:paragraph -->

<p>I've briefly touched on BoW and TF-IDF in my previous posts. In this post, we're going to be looking at Word2Vec.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>Difference between BoW</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->

<p>Word2Vec is different from BoW, as BoW produces a single value for each word, which is the count of the word occurrence in the corpus. Word2Vec on the other hand, produces a vector representation for each word (as the name implies, word to vector)</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>Having a numerical vector tied to a single word has more benefits, as compared to a single count number. Some of the features are:</p>
<!-- /wp:paragraph -->

<!-- wp:list -->

<ul>
<li>Cosine similarity between the vectors can indicate semantic similarity</li>
<li>The vectors produced for each word are fixed length, resulting in a low dimensional output (As compared to BoW, which results in a high dimensional and sparse vector)</li>
</ul>
<!-- /wp:list -->

<!-- wp:paragraph -->

<p>As a result, it's much easier to perform machine learning related task to the condense Word2Vec representations of the word.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>Generating the vectors</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->

<p>There are two methods for generating the vectors in Word2Vec:</p>
<!-- /wp:paragraph -->

<!-- wp:list -->

<ul>
<li>Skip-gram model</li>
<li>Continuous Bag of Words (CBoW)</li>
</ul>
<!-- /wp:list -->

<!-- wp:heading {"level":4} -->

<h4>Skip-gram Model</h4>
<!-- /wp:heading -->

<!-- wp:paragraph -->

<p>A Skip-gram is like N-gram, but instead of consecutive words, it skips around the given window.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>In the example below, the windows size is 2, which is to say 2 words before, and 2 words after the target word. The Skip-gram model then picks out all combinations of word-pairs within this window, not only consecutive ones (like in N-grams)</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":135} -->

<p><img alt="placeholder" class="wp-image-135" src="/media/2018/11/training_data.png">  </p>
<figcaption>
http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

</figcaption>

<!-- /wp:image -->

<!-- wp:paragraph -->

<p>In the skip-gram model, we're going to train a neural network  with a single hidden layer to perform the following task: Given an input word, output the probabilities of each word being "close" to the input word. This closeness is defined in a window:</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>We're going to throw all these word pairs in our one layer neural network, and train our model to identify nearby words for a given input word. So, the higher the frequency a pair of words occur together, the model learns this co-occurrence, and is able to give a higher probability that the word exists together.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>For example, in our training set, if we feed it with many instances of the word-pair ("Apple", "Orange"), because they happen to be in many sentences such as "Apples and oranges", our model picks up this co-occurrence and gives "Orange" a higher probability. On the other hand, word-pairs like ("Apple", "Day"), which could occur in a sentence, "An apple a day keeps the doctor away" occur less frequently, and model gives "Day" a lower probability.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>The catch here however, is that we're going to use the weights trained in the hidden layer of the neural network as our product, instead of the output itself. We want to use the hidden layer of the trained model to give each word a vector representation</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>The single hidden layer will have N number of neurons. In this example, we're going to assume N = 300, because 300 neurons was what Google used to train their Word2Vec model.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>Our model will look something like this</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":139} -->

<p><img alt="placeholder" class="wp-image-139" src="/media/2018/11/presentation11.jpg"></p>
<!-- /wp:image -->

<!-- wp:paragraph -->

<p>In the training phase, one hot encoding is used for the input and outputs. During the validation phase, the inputs is a one hot encoding, while the output is a probability for each word indicating their "closeness"</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>Once the model is trained, we're interested only in the hidden layer. The weight matrix would be of the size (Number of words X Number of neurons), and this is actually the word vector we're looking for.</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":138} -->

<p><img alt="placeholder" class="wp-image-138" src="/media/2018/11/weightmatrix1.jpg">  </p>
<figcaption>
Word Vector for each word, generated from the hidden layer

</figcaption>

<!-- /wp:image -->

<!-- wp:paragraph -->

<p>The feature of this word vector generated from the weight matrix is that, for similar words, their vectors would be "close" to each other (Cosine distance). This is because of the way we used word-pairs to train the model.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>Continuous Bag of Words</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->

<p>A CBoW is just a Skip-gram reversed.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>The input to a CBoW is a group of context words, and the output of the model tries to predict a single word that fits into the context of all the input words.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>CBoW represents the data differently</p>
<!-- /wp:paragraph -->

<!-- wp:code -->

<div class="highlight"><pre><span></span><span class="ss">&quot;Hi fred how was the pizza?&quot;</span>

<span class="n">CBOW</span><span class="p">:</span> <span class="mi">3</span><span class="o">-</span><span class="n">grams</span> <span class="err">{</span><span class="ss">&quot;Hi fred how&quot;</span><span class="p">,</span> <span class="ss">&quot;fred how was&quot;</span><span class="p">,</span> <span class="ss">&quot;how was the&quot;</span><span class="p">,</span> <span class="p">...</span><span class="err">}</span>

<span class="n">Skip</span><span class="o">-</span><span class="n">gram</span> <span class="mi">1</span><span class="o">-</span><span class="n">skip</span> <span class="mi">3</span><span class="o">-</span><span class="n">grams</span><span class="p">:</span> <span class="err">{</span><span class="ss">&quot;Hi fred how&quot;</span><span class="p">,</span> <span class="ss">&quot;Hi fred was&quot;</span><span class="p">,</span> <span class="ss">&quot;fred how was&quot;</span><span class="p">,</span> <span class="ss">&quot;fred how the&quot;</span><span class="p">,</span> <span class="p">...</span><span class="err">}</span>
</pre></div>


<!-- /wp:code -->

<!-- wp:paragraph -->

<p>or more intuitively, </p>
<!-- /wp:paragraph -->

<!-- wp:code -->

<div class="highlight"><pre><span></span><span class="n">CBOW</span><span class="o">:</span> <span class="n">The</span> <span class="n">cat</span> <span class="n">ate</span> <span class="n">_____</span><span class="o">.</span> 
<span class="n">Predict</span> <span class="n">the</span> <span class="n">target</span> <span class="n">word</span><span class="o">,</span> <span class="n">given</span> <span class="n">the</span> <span class="n">context</span><span class="o">.</span> <span class="n">In</span> <span class="k">this</span> <span class="k">case</span><span class="o">,</span> <span class="n">it</span><span class="err">’</span><span class="n">s</span> <span class="err">“</span><span class="n">food</span><span class="err">”</span><span class="o">.</span>

<span class="n">Skip</span><span class="o">-</span><span class="n">gram</span><span class="o">:</span> <span class="n">___</span> <span class="n">___</span> <span class="n">___</span> <span class="n">food</span><span class="o">.</span>
<span class="n">Given</span> <span class="n">the</span> <span class="n">target</span> <span class="n">what</span><span class="o">,</span> <span class="n">what</span> <span class="n">was</span> <span class="n">the</span> <span class="n">context</span> <span class="n">around</span> <span class="n">it</span><span class="o">?</span> <span class="n">In</span> <span class="k">this</span> <span class="k">case</span><span class="o">,</span> <span class="n">it</span><span class="err">’</span><span class="n">s</span> <span class="err">“</span><span class="n">The</span> <span class="n">cat</span> <span class="n">ate</span><span class="err">”</span>
</pre></div>


<!-- /wp:code -->
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/text-processing.html">Text processing</a>
    </p>
  </div>





</article>

    <footer>
<p>&copy;  </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " glob ",
  "url" : "",
  "image": "",
  "description": ""
}
</script>

</body>
</html>