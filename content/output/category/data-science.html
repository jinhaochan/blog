<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>A Pelican Blog - Data Science</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="A Pelican Blog Atom Feed" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">A Pelican Blog </a></h1>
                <nav><ul>
                    <li><a href="/category/book-review.html">Book Review</a></li>
                    <li class="active"><a href="/category/data-science.html">Data Science</a></li>
                    <li><a href="/category/misc.html">misc</a></li>
                    <li><a href="/category/ramblings.html">Ramblings</a></li>
                    <li><a href="/category/review.html">Review</a></li>
                    <li><a href="/category/security.html">Security</a></li>
                    <li><a href="/category/software-engineering.html">Software Engineering</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/lstm.html">LSTM</a></h1>
<footer class="post-info">
        <abbr class="published" title="2019-06-30T15:33:00+00:00">
                Published: Sun 30 June 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/exploding-gradients.html">Exploding Gradients</a> <a href="/tag/lstm.html">LSTM</a> <a href="/tag/vanishing-gradients.html">Vanishing Gradients</a> </p>
</footer><!-- /.post-info --><!-- wp:paragraph -->

<p>In the previous post, we talked about RNN, and how performing Backpropagation through time (BPTT) on an unrolled RNN with many time steps can lead to the problems of vanishing / exploding gradients, and difficulties in learning long term dependencies.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>In this post, we're going to look at a the LSTM (Long Short Term Memory) model that is a variant of an RNN, but is designed specifically to combat the 2 issues.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>LSTM Structure</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>Lets visually inspect the difference between a normal RNN cell and an LSTM cell.</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":362} -->

<figure class="wp-block-image">
![]({attach}media/2019/03/lstm3-simplernn.png){.wp-image-362}

<figcaption>
An unrolled RNN cell

</figcaption>
</figure>

<!-- /wp:image -->

<!-- wp:image {"id":356} -->

<figure class="wp-block-image">
![]({attach}media/2019/03/lstm3-chain.png){.wp-image-356}

<figcaption>
An unrolled LSTM cell

</figcaption>
</figure>

<!-- /wp:image -->

<!-- wp:paragraph -->

<p>The A on each of the cells represent the Activation in the cell. In the RNN, this can either be a Sigmoid, tanh, ReLU, or other activation functions. In the LSTM however, its a combination of 3 Sigmoids and a tanh function.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>The biggest difference, aside from the more complex internal structure of the LSTM, is that it has two connecting data pipelines from cell to cell.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>Cell State</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>The top line of an LSTM cell represents the cell state</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":357,"align":"center"} -->

<div class="wp-block-image">

<figure class="aligncenter">
![]({attach}media/2019/03/lstm3-c-line.png){.wp-image-357}
</figure>

</div>

<!-- /wp:image -->

<!-- wp:paragraph -->

<p>The LSTM has the ability to modify the cell state by removing information (through the multiplicative forget gate), or adding information (through the additive input gate)</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>This data flow from cell to cell is modified by two operators: The multiplication operator, and the addition operator denoted by the two pink nodes</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>LSTM Gates</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>The LSTM has 3 gates in the cell:</p>
<!-- /wp:paragraph -->

<!-- wp:list {"ordered":true} -->

<ol>
<li>Forget Gate</li>
<li>Input gate</li>
<li>Output Gate</li>
</ol>
<!-- /wp:list -->

<!-- wp:paragraph -->

<p>These gates modify the data that is flowing in and out of the LSTM cell</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":4} -->

<h4>The Common Sigmoid Layer</h4>
<!-- /wp:heading -->

<!-- wp:paragraph -->

<p>In all the 3 gates, there exists the common Sigmoid layer. This layer outputs a value from 0 to 1 for each state in the cell state.</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":365,"align":"center","width":87,"height":106} -->

<div class="wp-block-image">

<figure class="aligncenter is-resized">
![]({attach}media/2019/03/lstm3-gate.png){.wp-image-365 width="87" height="106"}  
<figcaption>
The common Sigmoid layer in all 3 gates
</figcaption>
</figure>

</div>

<!-- /wp:image -->

<!-- wp:paragraph -->

<p>The Sigmoid layer outputs a value from 0 to 1. This value corresponds to a value in the cell state, and this would mean different things for different gates.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>In the Forget gate, 0 would mean forget the value in the cell state, and 1 would mean remember the value entirely.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>In the Input gate, 0 would mean do not update this value at all, and 1 would mean update the value entirely.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>In the Output gate, 0 would mean do not output this cell state value, and 1 would mean output this value entirely.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":4} -->

<h4>Forget Gate</h4>
<!-- /wp:heading -->

<!-- wp:paragraph -->

<p>The first gate is the forget gate. This gate decides what information to discard from the cell state.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>This gate has the Sigmoid activation function. It takes in the previous time step's output, and the current time step input.</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":359} -->

<figure class="wp-block-image">
![]({attach}media/2019/03/lstm3-focus-f.png){.wp-image-359}

</figure>

<!-- /wp:image -->

<!-- wp:paragraph -->

<p>The two inputs are concatenated, and passed through the Sigmoid layer.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>Recall that a Sigmoid activation function outputs a value from 0 to 1. A value of 0 means completely forget this input value, while 1 means to remember the value entirely.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":4} -->

<h4>Input Gate</h4>
<!-- /wp:heading -->

<!-- wp:paragraph -->

<p>The next gate is the input gate, which is a combination of both the Sigmoid and tanh activation function. This gate decides what new information to add to the cell state.</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":360} -->

<figure class="wp-block-image">
![]({attach}media/2019/03/lstm3-focus-i.png){.wp-image-360}

</figure>

<!-- /wp:image -->

<!-- wp:paragraph -->

<p>There are two steps in the input gate phase</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>In the first step, the output of the previous time step and the input of the current time step are concatenated together, and passed into a Sigmoid. This layer decides which cell state values to update. (0 means do not update, 1 means update entirely)</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>The inputs are also passed into a tanh activation function, which tells the model what to update the cells states with.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>The multiplicative combination of these two outputs tells us which cell state to update (from the sigmoid layer), and what to update it with (tanh layer)</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":4} -->

<h4>Output Gate</h4>
<!-- /wp:heading -->

<!-- wp:paragraph -->

<p>The last gate, output gate, decides what value the cell would output. The output is derived from multiplying the outputs from the Sigmoid layer and tanh layer</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":361} -->

<figure class="wp-block-image">
![]({attach}media/2019/03/lstm3-focus-o.png){.wp-image-361}

</figure>

<!-- /wp:image -->

<!-- wp:paragraph -->

<p>The Sigmoid layer takes in the previous and current time step values, and outputs values 0 to 1 for each value in the cell. A value of 0 means do not output this cell state value at all, and 1 means to output the entire value.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>The tanh layer takes in the current cell state, which scales the values to be from -1 to 1. This value is then multiplied by the output of the Sigmoid layer to get the final output value.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>Modification of the Cell State</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>The cell states in each LSTM cell are modified either by the forget gate, or the input gate.</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":364} -->

<figure class="wp-block-image">
![]({attach}media/2019/03/lstm3-focus-c-2.png){.wp-image-364}

</figure>

<!-- /wp:image -->

<!-- wp:paragraph -->

<p>The forget gate outputs values 0-1, and is multiplied by the cell state. Cell states multiplied by 0 will be completely forgotten, while those multiplied by value &gt; 0 will be remembered by varying degrees.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>The input gate then updates each value in the cell state by a candidate amount (from the tanh layer), scaled by a factor (decided from the Sigmoid layer)</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>Conclusion</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>In each LSTM cell, there contains a cell state.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>Forget gate decides what to drop from the cell state</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>Input gate creates candidate values to update the cell state</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>Output gate decides what values to output from the cell state</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>Final Notes</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>I usually end with the conclusion, but all the information above was all the technical aspects of an LSTM. Here are some further questions relating to LSTM.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>Q: What are you actually training when you do your Backpropagation through time on an LSTM?</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>A: Recall that the gates have to control what to forget, input and output from each LSTM cell. What exactly to forget, input and output are the variables being trained.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>Q: So... how does it combat vanishing/exploding gradients?</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>Gradients explode when their values are greater than 1, and vanish when their values are lesser than 1, and are backpropagated for too large a time step.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>The key to LSTM preventing the vanexplgrad (I just made that up) is cell state updating step. Below shows the formula for updating the cell</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":369,"align":"center"} -->

<div class="wp-block-image">

<figure class="aligncenter">
![]({attach}media/2019/03/untitled-1.png){.wp-image-369}  
<figcaption>
Calculating the current cell state using values from the previous cell state
</figcaption>
</figure>

</div>

<!-- /wp:image -->

<!-- wp:list -->

<ul>
<li><code>c(t)</code> is the current cell state to compute</li>
<li><code>i</code> is the input gate that decides which cell state to update, <code>g</code> is the actual value changes to be made to the cell state. These two are multiplied together to get final cell state changes.</li>
<li><code>f</code> is the forget gate, and <code>c(t-1)</code> is the previous cell state. These two are multiplied to drop values from the previous cell state.</li>
</ul>
<!-- /wp:list -->

<!-- wp:paragraph -->

<p>When performing backpropagation, we find the derivative w.r.t the error. This gives us the formula</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":368,"align":"center"} -->

<div class="wp-block-image">

<figure class="aligncenter">
![]({attach}media/2019/03/2.png){.wp-image-368}  
<figcaption>
Derivative of w.r.t the error
</figcaption>
</figure>

</div>

<!-- /wp:image -->

<!-- wp:paragraph -->

<p>This formula does not have any multiplicative element in it, and so when BPTT occurs, a <em>linear carousel</em> occurs, thus preventing vanexplgrad.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<!-- /wp:paragraph -->                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/rnn-and-vanishing-exploding-gradients.html" rel="bookmark"
                           title="Permalink to RNN and Vanishing/Exploding Gradients">RNN and Vanishing/Exploding Gradients</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-06-23T16:23:00+00:00">
                Published: Sun 23 June 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/rnn.html">RNN</a> <a href="/tag/vanishing-gradients.html">Vanishing Gradients</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>In this post, we're going to be looking at:</p>
<!-- /wp:paragraph -->

<!-- wp:list {"ordered":true} -->

<ol>
<li>Recurrent Neural Networks (RNN)</li>
<li>Weight updates in an RNN</li>
<li>Unrolling an RNN</li>
<li>Vanishing/Exploding Gradient Problem</li>
</ol>
<!-- /wp:list -->

<!-- wp:heading {"level":3} -->

<h3>Recurrent Neural Networks</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>A Recurrent Neural Network (RNN) is a variant of neural networks, where in each neuron, the outputs cycle back to themselves, hence …</p>
                <a class="readmore" href="/rnn-and-vanishing-exploding-gradients.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/k-means-clustering.html" rel="bookmark"
                           title="Permalink to K-Means Clustering">K-Means Clustering</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-06-16T21:18:00+00:00">
                Published: Sun 16 June 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/k-means-clustering.html">K means clustering</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>K-Means Clustering is an unsupervised learning algorithm. It works by grouping similar data points together to try to find underlying patterns.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>The number of groups are pre-defined by the user as K.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>How the Algorithm works</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>Before the iterative update starts, a random selection of centroid locations are picked on …</p>
                <a class="readmore" href="/k-means-clustering.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/random-forests.html" rel="bookmark"
                           title="Permalink to Random Forests">Random Forests</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-06-09T20:43:00+00:00">
                Published: Sun 09 June 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/decision-tree.html">Decision Tree</a> <a href="/tag/random-forest.html">Random Forest</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>A random forest is an ensemble approach of combining multiple decision trees. Ensembling and Decision Trees, we first need to explain what these two things are</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>Decision Trees</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>Decision Trees try to encode and separate the data into if-else rules. It breaks the data down into smaller and smaller subsets …</p>
                <a class="readmore" href="/random-forests.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/gan.html" rel="bookmark"
                           title="Permalink to GAN?">GAN?</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-05-26T13:13:00+00:00">
                Published: Sun 26 May 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>

</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>A Generative Adversarial Network (GAN) is a collection of two neural network models: A Discriminator, and a Generator. The goals of the two models are opposing to each other</p>
<!-- /wp:paragraph -->

<!-- wp:list {"ordered":true} -->

<ol>
<li>Discriminator: Given a set of features, we try to predict the label</li>
<li>Generator: Given a label, we try to predict the …</li></ol>
                <a class="readmore" href="/gan.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/visualizing-neural-networks.html" rel="bookmark"
                           title="Permalink to Visualizing Neural Networks">Visualizing Neural Networks</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-05-19T09:31:00+00:00">
                Published: Sun 19 May 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>

</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>Neural Networks have always been sort of a black box when it comes to it's implementation, and how it produces good results. I came across some material that shows visually, how the neural networks morph the problem space so that they are separable.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>Simple Data</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>Here's a sample graph that …</p>
                <a class="readmore" href="/visualizing-neural-networks.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/tips-for-kaggling.html" rel="bookmark"
                           title="Permalink to Tips for Kaggling">Tips for Kaggling</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-05-12T11:31:00+00:00">
                Published: Sun 12 May 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/kaggle.html">Kaggle</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>I've been doing Kaggle competitions for awhile (although with not much success), and I've learning quite a few things along the way. One of which is how to properly approach the problem, and iterate through it to climb the LB (leader board).</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>Setting the baseline</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>The first thing I would …</p>
                <a class="readmore" href="/tips-for-kaggling.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/say-no-to-overfitting.html" rel="bookmark"
                           title="Permalink to Say NO to Overfitting!">Say NO to Overfitting!</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-05-05T13:58:00+00:00">
                Published: Sun 05 May 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/overfitting.html">Overfitting</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>Just some experience I've encountered while working on a very small data set of 1703 training samples, and 1705 testing samples.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>One way to combat overfitting is to use cross validation. While doing so, it's important for you not to just look at the final validation score, but also observe …</p>
                <a class="readmore" href="/say-no-to-overfitting.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/what-is-maximum-likelihood-estimation.html" rel="bookmark"
                           title="Permalink to What is Maximum Likelihood Estimation?">What is Maximum Likelihood Estimation?</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-04-28T09:44:00+00:00">
                Published: Sun 28 April 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>

</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>In machine learning, we often perform what we call <strong>parameter estimation</strong>, which are the weights that are assigned to each feature of the input data.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>For example, in a simple linear model, we use the equation <code>y=mx + c</code> , and <code>m</code> and <code>c</code> are your parameters to be estimated. For …</p>
                <a class="readmore" href="/what-is-maximum-likelihood-estimation.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/activation-functions.html" rel="bookmark"
                           title="Permalink to Activation Functions">Activation Functions</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-04-14T18:07:00+00:00">
                Published: Sun 14 April 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/activation-functions.html">Activation Functions</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>The structure of a deep learning model consists mainly of nodes, and connections between them. Most of the time, every single node is connected to every other node in the next layer, which we call a Dense layer.</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":260,"align":"center"} -->

<div class="wp-block-image">

<figure class="aligncenter">
![]({attach}media/2019/01/2.png){.wp-image-260}  
<figcaption>
Nodes in a fully connected …</figcaption></figure></div>
                <a class="readmore" href="/activation-functions.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/the-interpretation-of-roc-and-auc.html" rel="bookmark"
                           title="Permalink to The Interpretation of ROC and AUC">The Interpretation of ROC and AUC</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-04-14T12:52:00+00:00">
                Published: Sun 14 April 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>

</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>The ROC curve and it's AUC is a common metric for evaluation the performance of a model. In this post, we dig deeper to find out how to interpret the results, and what corrective actions to take to improve it.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>What is it?</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>The ROC curve, or Receiver Operating Characteristic …</p>
                <a class="readmore" href="/the-interpretation-of-roc-and-auc.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regularization.html" rel="bookmark"
                           title="Permalink to Regularization">Regularization</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-04-07T09:49:00+00:00">
                Published: Sun 07 April 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/drop-out.html">drop out</a> <a href="/tag/regularization.html">regularization</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>One of the major problems in training a model in machine learning is overfitting. Especially when your model gets more and more complex, it starts to memorize the patterns in the training data. This makes it perform poorly on unseen data, which has new patterns.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>Overfitting is the result of …</p>
                <a class="readmore" href="/regularization.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/microsoft-kaggle-competition.html" rel="bookmark"
                           title="Permalink to Microsoft Kaggle Competition">Microsoft Kaggle Competition</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-03-31T09:38:00+00:00">
                Published: Sun 31 March 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/kaggle.html">Kaggle</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>This is the write up for my solution for the Microsoft Malware Prediction</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p><a href="https://www.kaggle.com/c/microsoft-malware-prediction">https://www.kaggle.com/c/microsoft-malware-prediction</a></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>I got pretty high up the leader board, but it was nothing that I was proud of, because:</p>
<!-- /wp:paragraph -->

<!-- wp:list {"ordered":true} -->

<ol>
<li>I grossly overfitted my model</li>
<li>The final result was a blend of another …</li></ol>
                <a class="readmore" href="/microsoft-kaggle-competition.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/model-capacity.html" rel="bookmark"
                           title="Permalink to Model Capacity">Model Capacity</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-03-24T20:19:00+00:00">
                Published: Sun 24 March 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/capacity.html">Capacity</a> <a href="/tag/vc-dimension.html">VC Dimension</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>While studying the book Deep Learning by Ian Goodfellow, I came across this concept of model capacity, and it was really intuitive in helping me understand the models representation of a given problem.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>This ties to the concept of overfitting and underfitting</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>Capacity</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>Put simply, the capacity of the model …</p>
                <a class="readmore" href="/model-capacity.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/counts-based-featurization.html" rel="bookmark"
                           title="Permalink to Counts Based Featurization">Counts Based Featurization</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-03-24T17:48:00+00:00">
                Published: Sun 24 March 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/count-feature.html">Count Feature</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>While doing the Microsoft Malware Classification challenge, I encountered a way of Feature representation called Count Based Features (CBF).</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>CBF is good to use with very high cardinality features, and it transforms the high number of categories in the data to the number of it's occurrences. This representation is helpful …</p>
                <a class="readmore" href="/counts-based-featurization.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/lightgbm.html" rel="bookmark"
                           title="Permalink to LightGBM">LightGBM</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-03-24T12:33:00+00:00">
                Published: Sun 24 March 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>

</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>For some time, XGBoost was considered the Kaggle-Killer, being the winning model for most prediction problems. Recently Microsoft released their own gradient boosting framework called LightGBM, and it is way faster than XGB. In this post, I'm going to touch on the interesting portions of LightGBM.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>What is LightGBM?</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>Similar …</p>
                <a class="readmore" href="/lightgbm.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/feature-engineering.html" rel="bookmark"
                           title="Permalink to Feature Engineering">Feature Engineering</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-03-17T09:41:00+00:00">
                Published: Sun 17 March 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/feature-engineering.html">Feature Engineering</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>Feature Engineering is one of the neglected portion of machine learning. Most topics revolve around Model Training (parameter tuning, cross validation). While that might be really important, feature engineering is equally important as well, but I can't seem to find good resources that talk about this. I suspect this is …</p>
                <a class="readmore" href="/feature-engineering.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/microsoft-kaggle-challenge-adversarial-validation.html" rel="bookmark"
                           title="Permalink to Microsoft Kaggle Challenge: Adversarial Validation">Microsoft Kaggle Challenge: Adversarial Validation</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-03-10T19:45:00+00:00">
                Published: Sun 10 March 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/adversarial-validation.html">Adversarial Validation</a> </p>
</footer><!-- /.post-info -->                <!-- wp:heading {"level":3} -->

<h3>Overview</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>This was a concept I came across while doing a Kaggle challenge issued by Microsoft to predict if a computer would get hit by a malware or not.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>This challenge was different from their previous one, where they wanted you to predict if the malware class of a given …</p>
                <a class="readmore" href="/microsoft-kaggle-challenge-adversarial-validation.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/convolutional-neural-networks.html" rel="bookmark"
                           title="Permalink to Convolutional Neural Networks">Convolutional Neural Networks</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-02-17T16:11:00+00:00">
                Published: Sun 17 February 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>

</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>Convolutional Neural Networks (CNN) are neural networks that are mainly used for image recognition and image classification.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>In this post, we'll break down how a CNN works under the hoods.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->

<h3>Backgroud</h3>
<!-- /wp:heading -->

<!-- wp:separator -->

<hr>
<!-- /wp:separator -->

<p></p>
<!-- wp:paragraph --></p>
<p>If we used a traditional neural network without any of the prior convolution steps, the network would not scale …</p>
                <a class="readmore" href="/convolutional-neural-networks.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/model-optimizers-beyond-gradient-descent-in-deep-learning.html" rel="bookmark"
                           title="Permalink to Model Optimizers Beyond Gradient Descent in Deep Learning">Model Optimizers Beyond Gradient Descent in Deep Learning</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-02-10T09:52:00+00:00">
                Published: Sun 10 February 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/optimizers.html">Optimizers</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>In this post, we're going to talk about the draw backs and constrains of a simple Gradient Descent algorithm when applied to Deep Learning models, and also talk about other optimization algorithms that aim to solve those problems.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>These problems mainly arise due to the complex error surface in Deep …</p>
                <a class="readmore" href="/model-optimizers-beyond-gradient-descent-in-deep-learning.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/gradient-descent.html" rel="bookmark"
                           title="Permalink to Gradient Descent">Gradient Descent</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-02-03T20:10:00+00:00">
                Published: Sun 03 February 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/gradient-descent.html">Gradient Descent</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>A machine learning model consists of weights, and those weights, given a set of inputs, are used in the calculation process to produce a prediction. The prediction is then fed into a loss function, to calculate the the total error. Using this error, we feed it into an optimization algorithm …</p>
                <a class="readmore" href="/gradient-descent.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/learning-in-machine-learning.html" rel="bookmark"
                           title="Permalink to Learning in Machine Learning">Learning in Machine Learning</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-01-27T17:14:00+00:00">
                Published: Sun 27 January 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/machine-learning.html">Machine learning</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>When we talk about machine learning, it's mostly a black box, where everything is nicely wrapped in easy to call library functions.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>Scipy, Numpy, Scikit-learn help us abstract all the nitty gritty details underlying machine learning</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>In this post, we're going to see where exactly the learning takes place, and …</p>
                <a class="readmore" href="/learning-in-machine-learning.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/word2vec-and-skip-gram.html" rel="bookmark"
                           title="Permalink to Word2Vec">Word2Vec</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-01-20T14:24:00+00:00">
                Published: Sun 20 January 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/text-processing.html">Text processing</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>In the field of machine learning, when we're dealing with text processing, we can't just read in the strings of the sentence to train our model. The model requires numerical vectors, and word embedding is a way to convert your sentences into these vectors.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->

<p>There are various word embedding techniques …</p>
                <a class="readmore" href="/word2vec-and-skip-gram.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/text-processing.html" rel="bookmark"
                           title="Permalink to Text Processing">Text Processing</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2019-01-13T18:41:00+00:00">
                Published: Sun 13 January 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="/tag/text-processing.html">Text processing</a> </p>
</footer><!-- /.post-info -->                <!-- wp:paragraph -->

<p>In this post, we're going to be exploring some typical methods for text processing for machine learning. When we're talking about machine learning with text, there are several areas of interest including </p>
<!-- /wp:paragraph -->

<!-- wp:list -->

<ul>
<li>Sentiment Analysis</li>
<li>Question Answering</li>
<li>Information Retrieval</li>
</ul>
<!-- /wp:list -->

<!-- wp:paragraph -->

<p>Before we do that, we must first understand that a machine learning …</p>
                <a class="readmore" href="/text-processing.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/kaggle-boiler-plate.html" rel="bookmark"
                           title="Permalink to Kaggle Boiler Plate">Kaggle Boiler Plate</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2018-12-16T21:33:00+00:00">
                Published: Sun 16 December 2018
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/jinhaochan.html">jinhaochan</a>
        </address>
<p>In <a href="/category/data-science.html">Data Science</a>.</p>

</footer><!-- /.post-info -->                <p>I've been playing around with Kaggle competitions for a while, and there are usually quite a few steps to perform.</p>
<p>I've compiled a list of them below, in sequential order. These are by no means hard and fast rules, but simple heuristics to follow!</p>
<p>I've added links here and there …</p>
                <a class="readmore" href="/kaggle-boiler-plate.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>